{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "files=os.listdir(\"CK+48\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fear', 'contempt', 'happy', 'surprise', 'anger', 'disgust', 'sadness']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CK+48/fear/S506_004_00000036.png   0\n",
      "CK+48/fear/S055_006_00000006.png   0\n",
      "CK+48/fear/S125_006_00000022.png   0\n",
      "CK+48/fear/S062_001_00000015.png   0\n",
      "CK+48/fear/S054_002_00000014.png   0\n",
      "CK+48/fear/S074_001_00000018.png   0\n",
      "CK+48/fear/S050_001_00000017.png   0\n",
      "CK+48/fear/S132_003_00000022.png   0\n",
      "CK+48/fear/S506_004_00000037.png   0\n",
      "CK+48/fear/S065_002_00000022.png   0\n",
      "CK+48/fear/S050_001_00000015.png   0\n",
      "CK+48/fear/S102_003_00000016.png   0\n",
      "CK+48/fear/S125_006_00000021.png   0\n",
      "CK+48/fear/S068_004_00000010.png   0\n",
      "CK+48/fear/S084_002_00000022.png   0\n",
      "CK+48/fear/S062_001_00000017.png   0\n",
      "CK+48/fear/S504_004_00000014.png   0\n",
      "CK+48/fear/S999_003_00000053.png   0\n",
      "CK+48/fear/S068_004_00000009.png   0\n",
      "CK+48/fear/S501_004_00000055.png   0\n",
      "CK+48/fear/S032_004_00000012.png   0\n",
      "CK+48/fear/S054_002_00000013.png   0\n",
      "CK+48/fear/S091_001_00000014.png   0\n",
      "CK+48/fear/S504_004_00000013.png   0\n",
      "CK+48/fear/S502_004_00000050.png   0\n",
      "CK+48/fear/S050_001_00000016.png   0\n",
      "CK+48/fear/S074_001_00000019.png   0\n",
      "CK+48/fear/S501_004_00000054.png   0\n",
      "CK+48/fear/S119_003_00000022.png   0\n",
      "CK+48/fear/S091_001_00000013.png   0\n",
      "CK+48/fear/S046_003_00000015.png   0\n",
      "CK+48/fear/S124_003_00000010.png   0\n",
      "CK+48/fear/S125_006_00000020.png   0\n",
      "CK+48/fear/S504_004_00000015.png   0\n",
      "CK+48/fear/S065_002_00000021.png   0\n",
      "CK+48/fear/S046_003_00000014.png   0\n",
      "CK+48/fear/S068_004_00000008.png   0\n",
      "CK+48/fear/S138_001_00000011.png   0\n",
      "CK+48/fear/S065_002_00000020.png   0\n",
      "CK+48/fear/S091_001_00000015.png   0\n",
      "CK+48/fear/S502_004_00000052.png   0\n",
      "CK+48/fear/S124_003_00000011.png   0\n",
      "CK+48/fear/S011_003_00000013.png   0\n",
      "CK+48/fear/S084_002_00000023.png   0\n",
      "CK+48/fear/S999_003_00000054.png   0\n",
      "CK+48/fear/S074_001_00000020.png   0\n",
      "CK+48/fear/S119_003_00000023.png   0\n",
      "CK+48/fear/S062_001_00000016.png   0\n",
      "CK+48/fear/S059_002_00000015.png   0\n",
      "CK+48/fear/S119_003_00000024.png   0\n",
      "CK+48/fear/S055_006_00000007.png   0\n",
      "CK+48/fear/S138_001_00000012.png   0\n",
      "CK+48/fear/S059_002_00000017.png   0\n",
      "CK+48/fear/S102_003_00000014.png   0\n",
      "CK+48/fear/S011_003_00000014.png   0\n",
      "CK+48/fear/S054_002_00000015.png   0\n",
      "CK+48/fear/S084_002_00000021.png   0\n",
      "CK+48/fear/S011_003_00000012.png   0\n",
      "CK+48/fear/S055_006_00000008.png   0\n",
      "CK+48/fear/S138_001_00000010.png   0\n",
      "CK+48/fear/S117_003_00000013.png   0\n",
      "CK+48/fear/S117_003_00000012.png   0\n",
      "CK+48/fear/S124_003_00000009.png   0\n",
      "CK+48/fear/S032_004_00000014.png   0\n",
      "CK+48/fear/S032_004_00000013.png   0\n",
      "CK+48/fear/S501_004_00000056.png   0\n",
      "CK+48/fear/S059_002_00000016.png   0\n",
      "CK+48/fear/S132_003_00000023.png   0\n",
      "CK+48/fear/S046_003_00000016.png   0\n",
      "CK+48/fear/S506_004_00000038.png   0\n",
      "CK+48/fear/S117_003_00000014.png   0\n",
      "CK+48/fear/S999_003_00000055.png   0\n",
      "CK+48/fear/S502_004_00000051.png   0\n",
      "CK+48/fear/S132_003_00000021.png   0\n",
      "CK+48/fear/S102_003_00000015.png   0\n",
      "CK+48/contempt/S156_002_00000019.png   1\n",
      "CK+48/contempt/S505_002_00000019.png   1\n",
      "CK+48/contempt/S147_002_00000012.png   1\n",
      "CK+48/contempt/S139_002_00000013.png   1\n",
      "CK+48/contempt/S157_002_00000010.png   1\n",
      "CK+48/contempt/S158_002_00000009.png   1\n",
      "CK+48/contempt/S504_002_00000007.png   1\n",
      "CK+48/contempt/S149_002_00000011.png   1\n",
      "CK+48/contempt/S139_002_00000011.png   1\n",
      "CK+48/contempt/S505_002_00000021.png   1\n",
      "CK+48/contempt/S158_002_00000010.png   1\n",
      "CK+48/contempt/S505_002_00000020.png   1\n",
      "CK+48/contempt/S149_002_00000013.png   1\n",
      "CK+48/contempt/S503_002_00000008.png   1\n",
      "CK+48/contempt/S148_002_00000015.png   1\n",
      "CK+48/contempt/S155_002_00000012.png   1\n",
      "CK+48/contempt/S154_002_00000012.png   1\n",
      "CK+48/contempt/S138_008_00000009.png   1\n",
      "CK+48/contempt/S148_002_00000013.png   1\n",
      "CK+48/contempt/S160_006_00000008.png   1\n",
      "CK+48/contempt/S160_006_00000009.png   1\n",
      "CK+48/contempt/S158_002_00000011.png   1\n",
      "CK+48/contempt/S155_002_00000011.png   1\n",
      "CK+48/contempt/S147_002_00000013.png   1\n",
      "CK+48/contempt/S139_002_00000012.png   1\n",
      "CK+48/contempt/S154_002_00000011.png   1\n",
      "CK+48/contempt/S157_002_00000009.png   1\n",
      "CK+48/contempt/S151_002_00000028.png   1\n",
      "CK+48/contempt/S503_002_00000006.png   1\n",
      "CK+48/contempt/S895_002_00000005.png   1\n",
      "CK+48/contempt/S157_002_00000011.png   1\n",
      "CK+48/contempt/S155_002_00000010.png   1\n",
      "CK+48/contempt/S151_002_00000029.png   1\n",
      "CK+48/contempt/S895_002_00000006.png   1\n",
      "CK+48/contempt/S503_002_00000007.png   1\n",
      "CK+48/contempt/S138_008_00000007.png   1\n",
      "CK+48/contempt/S154_002_00000013.png   1\n",
      "CK+48/contempt/S504_002_00000009.png   1\n",
      "CK+48/contempt/S156_002_00000021.png   1\n",
      "CK+48/contempt/S149_002_00000012.png   1\n",
      "CK+48/contempt/S156_002_00000020.png   1\n",
      "CK+48/contempt/S502_002_00000008.png   1\n",
      "CK+48/contempt/S151_002_00000027.png   1\n",
      "CK+48/contempt/S148_002_00000014.png   1\n",
      "CK+48/contempt/S895_002_00000007.png   1\n",
      "CK+48/contempt/S502_002_00000009.png   1\n",
      "CK+48/contempt/S506_002_00000007.png   1\n",
      "CK+48/contempt/S502_002_00000007.png   1\n",
      "CK+48/contempt/S504_002_00000008.png   1\n",
      "CK+48/contempt/S160_006_00000010.png   1\n",
      "CK+48/contempt/S147_002_00000011.png   1\n",
      "CK+48/contempt/S506_002_00000008.png   1\n",
      "CK+48/contempt/S138_008_00000008.png   1\n",
      "CK+48/contempt/S506_002_00000009.png   1\n",
      "CK+48/happy/S037_006_00000019.png   2\n",
      "CK+48/happy/S070_003_00000016.png   2\n",
      "CK+48/happy/S116_007_00000015.png   2\n",
      "CK+48/happy/S137_011_00000020.png   2\n",
      "CK+48/happy/S093_004_00000015.png   2\n",
      "CK+48/happy/S026_006_00000011.png   2\n",
      "CK+48/happy/S134_004_00000015.png   2\n",
      "CK+48/happy/S067_005_00000022.png   2\n",
      "CK+48/happy/S125_005_00000013.png   2\n",
      "CK+48/happy/S079_004_00000025.png   2\n",
      "CK+48/happy/S026_006_00000013.png   2\n",
      "CK+48/happy/S011_006_00000013.png   2\n",
      "CK+48/happy/S128_011_00000015.png   2\n",
      "CK+48/happy/S097_006_00000019.png   2\n",
      "CK+48/happy/S109_006_00000014.png   2\n",
      "CK+48/happy/S071_005_00000020.png   2\n",
      "CK+48/happy/S130_013_00000014.png   2\n",
      "CK+48/happy/S109_006_00000013.png   2\n",
      "CK+48/happy/S135_012_00000019.png   2\n",
      "CK+48/happy/S129_012_00000009.png   2\n",
      "CK+48/happy/S091_003_00000019.png   2\n",
      "CK+48/happy/S100_006_00000016.png   2\n",
      "CK+48/happy/S044_003_00000012.png   2\n",
      "CK+48/happy/S014_005_00000016.png   2\n",
      "CK+48/happy/S074_005_00000041.png   2\n",
      "CK+48/happy/S063_002_00000021.png   2\n",
      "CK+48/happy/S055_005_00000043.png   2\n",
      "CK+48/happy/S136_006_00000018.png   2\n",
      "CK+48/happy/S068_002_00000013.png   2\n",
      "CK+48/happy/S075_006_00000024.png   2\n",
      "CK+48/happy/S011_006_00000011.png   2\n",
      "CK+48/happy/S106_006_00000009.png   2\n",
      "CK+48/happy/S060_002_00000026.png   2\n",
      "CK+48/happy/S078_004_00000027.png   2\n",
      "CK+48/happy/S076_006_00000018.png   2\n",
      "CK+48/happy/S085_002_00000013.png   2\n",
      "CK+48/happy/S125_005_00000011.png   2\n",
      "CK+48/happy/S062_004_00000023.png   2\n",
      "CK+48/happy/S083_003_00000018.png   2\n",
      "CK+48/happy/S131_006_00000021.png   2\n",
      "CK+48/happy/S098_004_00000013.png   2\n",
      "CK+48/happy/S067_005_00000020.png   2\n",
      "CK+48/happy/S108_008_00000011.png   2\n",
      "CK+48/happy/S086_002_00000014.png   2\n",
      "CK+48/happy/S050_006_00000021.png   2\n",
      "CK+48/happy/S055_005_00000044.png   2\n",
      "CK+48/happy/S062_004_00000024.png   2\n",
      "CK+48/happy/S094_004_00000011.png   2\n",
      "CK+48/happy/S128_011_00000014.png   2\n",
      "CK+48/happy/S042_006_00000015.png   2\n",
      "CK+48/happy/S053_004_00000023.png   2\n",
      "CK+48/happy/S116_007_00000017.png   2\n",
      "CK+48/happy/S072_006_00000021.png   2\n",
      "CK+48/happy/S079_004_00000024.png   2\n",
      "CK+48/happy/S131_006_00000022.png   2\n",
      "CK+48/happy/S044_003_00000013.png   2\n",
      "CK+48/happy/S138_005_00000016.png   2\n",
      "CK+48/happy/S137_011_00000018.png   2\n",
      "CK+48/happy/S083_003_00000019.png   2\n",
      "CK+48/happy/S127_004_00000016.png   2\n",
      "CK+48/happy/S099_004_00000013.png   2\n",
      "CK+48/happy/S034_005_00000008.png   2\n",
      "CK+48/happy/S067_005_00000021.png   2\n",
      "CK+48/happy/S133_010_00000013.png   2\n",
      "CK+48/happy/S065_004_00000026.png   2\n",
      "CK+48/happy/S066_003_00000012.png   2\n",
      "CK+48/happy/S115_008_00000016.png   2\n",
      "CK+48/happy/S037_006_00000021.png   2\n",
      "CK+48/happy/S070_003_00000015.png   2\n",
      "CK+48/happy/S127_004_00000014.png   2\n",
      "CK+48/happy/S136_006_00000020.png   2\n",
      "CK+48/happy/S095_007_00000019.png   2\n",
      "CK+48/happy/S053_004_00000024.png   2\n",
      "CK+48/happy/S042_006_00000017.png   2\n",
      "CK+48/happy/S068_002_00000014.png   2\n",
      "CK+48/happy/S065_004_00000027.png   2\n",
      "CK+48/happy/S097_006_00000017.png   2\n",
      "CK+48/happy/S095_007_00000020.png   2\n",
      "CK+48/happy/S034_005_00000009.png   2\n",
      "CK+48/happy/S108_008_00000012.png   2\n",
      "CK+48/happy/S069_004_00000017.png   2\n",
      "CK+48/happy/S085_002_00000014.png   2\n",
      "CK+48/happy/S075_006_00000025.png   2\n",
      "CK+48/happy/S063_002_00000023.png   2\n",
      "CK+48/happy/S115_008_00000017.png   2\n",
      "CK+48/happy/S106_006_00000011.png   2\n",
      "CK+48/happy/S071_005_00000019.png   2\n",
      "CK+48/happy/S010_006_00000015.png   2\n",
      "CK+48/happy/S085_002_00000012.png   2\n",
      "CK+48/happy/S092_004_00000023.png   2\n",
      "CK+48/happy/S093_004_00000016.png   2\n",
      "CK+48/happy/S096_004_00000011.png   2\n",
      "CK+48/happy/S010_006_00000013.png   2\n",
      "CK+48/happy/S092_004_00000022.png   2\n",
      "CK+48/happy/S087_005_00000011.png   2\n",
      "CK+48/happy/S050_006_00000022.png   2\n",
      "CK+48/happy/S130_013_00000013.png   2\n",
      "CK+48/happy/S137_011_00000019.png   2\n",
      "CK+48/happy/S092_004_00000024.png   2\n",
      "CK+48/happy/S138_005_00000014.png   2\n",
      "CK+48/happy/S136_006_00000019.png   2\n",
      "CK+48/happy/S129_012_00000010.png   2\n",
      "CK+48/happy/S050_006_00000023.png   2\n",
      "CK+48/happy/S037_006_00000020.png   2\n",
      "CK+48/happy/S091_003_00000020.png   2\n",
      "CK+48/happy/S042_006_00000016.png   2\n",
      "CK+48/happy/S100_006_00000014.png   2\n",
      "CK+48/happy/S091_003_00000021.png   2\n",
      "CK+48/happy/S138_005_00000015.png   2\n",
      "CK+48/happy/S124_007_00000024.png   2\n",
      "CK+48/happy/S114_006_00000022.png   2\n",
      "CK+48/happy/S133_010_00000012.png   2\n",
      "CK+48/happy/S071_005_00000021.png   2\n",
      "CK+48/happy/S079_004_00000026.png   2\n",
      "CK+48/happy/S114_006_00000023.png   2\n",
      "CK+48/happy/S132_006_00000021.png   2\n",
      "CK+48/happy/S115_008_00000015.png   2\n",
      "CK+48/happy/S096_004_00000009.png   2\n",
      "CK+48/happy/S032_006_00000016.png   2\n",
      "CK+48/happy/S074_005_00000043.png   2\n",
      "CK+48/happy/S095_007_00000021.png   2\n",
      "CK+48/happy/S014_005_00000017.png   2\n",
      "CK+48/happy/S052_004_00000031.png   2\n",
      "CK+48/happy/S135_012_00000018.png   2\n",
      "CK+48/happy/S056_004_00000019.png   2\n",
      "CK+48/happy/S052_004_00000032.png   2\n",
      "CK+48/happy/S064_003_00000023.png   2\n",
      "CK+48/happy/S057_006_00000031.png   2\n",
      "CK+48/happy/S096_004_00000010.png   2\n",
      "CK+48/happy/S076_006_00000017.png   2\n",
      "CK+48/happy/S094_004_00000012.png   2\n",
      "CK+48/happy/S124_007_00000023.png   2\n",
      "CK+48/happy/S061_002_00000015.png   2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CK+48/happy/S114_006_00000021.png   2\n",
      "CK+48/happy/S089_002_00000021.png   2\n",
      "CK+48/happy/S076_006_00000019.png   2\n",
      "CK+48/happy/S035_006_00000017.png   2\n",
      "CK+48/happy/S052_004_00000033.png   2\n",
      "CK+48/happy/S069_004_00000015.png   2\n",
      "CK+48/happy/S053_004_00000022.png   2\n",
      "CK+48/happy/S075_006_00000023.png   2\n",
      "CK+48/happy/S094_004_00000010.png   2\n",
      "CK+48/happy/S072_006_00000020.png   2\n",
      "CK+48/happy/S074_005_00000042.png   2\n",
      "CK+48/happy/S099_004_00000015.png   2\n",
      "CK+48/happy/S099_004_00000014.png   2\n",
      "CK+48/happy/S078_004_00000026.png   2\n",
      "CK+48/happy/S044_003_00000014.png   2\n",
      "CK+48/happy/S026_006_00000012.png   2\n",
      "CK+48/happy/S127_004_00000015.png   2\n",
      "CK+48/happy/S132_006_00000023.png   2\n",
      "CK+48/happy/S133_010_00000014.png   2\n",
      "CK+48/happy/S086_002_00000015.png   2\n",
      "CK+48/happy/S087_005_00000010.png   2\n",
      "CK+48/happy/S098_004_00000014.png   2\n",
      "CK+48/happy/S109_006_00000015.png   2\n",
      "CK+48/happy/S116_007_00000016.png   2\n",
      "CK+48/happy/S083_003_00000017.png   2\n",
      "CK+48/happy/S057_006_00000033.png   2\n",
      "CK+48/happy/S065_004_00000028.png   2\n",
      "CK+48/happy/S093_004_00000014.png   2\n",
      "CK+48/happy/S089_002_00000019.png   2\n",
      "CK+48/happy/S066_003_00000010.png   2\n",
      "CK+48/happy/S011_006_00000012.png   2\n",
      "CK+48/happy/S057_006_00000032.png   2\n",
      "CK+48/happy/S124_007_00000022.png   2\n",
      "CK+48/happy/S134_004_00000014.png   2\n",
      "CK+48/happy/S056_004_00000018.png   2\n",
      "CK+48/happy/S055_005_00000045.png   2\n",
      "CK+48/happy/S032_006_00000015.png   2\n",
      "CK+48/happy/S063_002_00000022.png   2\n",
      "CK+48/happy/S072_006_00000022.png   2\n",
      "CK+48/happy/S125_005_00000012.png   2\n",
      "CK+48/happy/S132_006_00000022.png   2\n",
      "CK+48/happy/S064_003_00000024.png   2\n",
      "CK+48/happy/S010_006_00000014.png   2\n",
      "CK+48/happy/S128_011_00000016.png   2\n",
      "CK+48/happy/S066_003_00000011.png   2\n",
      "CK+48/happy/S106_006_00000010.png   2\n",
      "CK+48/happy/S070_003_00000017.png   2\n",
      "CK+48/happy/S064_003_00000025.png   2\n",
      "CK+48/happy/S014_005_00000015.png   2\n",
      "CK+48/happy/S069_004_00000016.png   2\n",
      "CK+48/happy/S061_002_00000013.png   2\n",
      "CK+48/happy/S130_013_00000015.png   2\n",
      "CK+48/happy/S129_012_00000011.png   2\n",
      "CK+48/happy/S032_006_00000014.png   2\n",
      "CK+48/happy/S056_004_00000020.png   2\n",
      "CK+48/happy/S035_006_00000018.png   2\n",
      "CK+48/happy/S061_002_00000014.png   2\n",
      "CK+48/happy/S068_002_00000015.png   2\n",
      "CK+48/happy/S034_005_00000010.png   2\n",
      "CK+48/happy/S135_012_00000020.png   2\n",
      "CK+48/happy/S134_004_00000013.png   2\n",
      "CK+48/happy/S060_002_00000024.png   2\n",
      "CK+48/happy/S062_004_00000022.png   2\n",
      "CK+48/happy/S078_004_00000025.png   2\n",
      "CK+48/happy/S089_002_00000020.png   2\n",
      "CK+48/happy/S131_006_00000020.png   2\n",
      "CK+48/happy/S098_004_00000015.png   2\n",
      "CK+48/happy/S108_008_00000013.png   2\n",
      "CK+48/happy/S100_006_00000015.png   2\n",
      "CK+48/happy/S087_005_00000012.png   2\n",
      "CK+48/happy/S035_006_00000016.png   2\n",
      "CK+48/happy/S097_006_00000018.png   2\n",
      "CK+48/happy/S086_002_00000013.png   2\n",
      "CK+48/happy/S060_002_00000025.png   2\n",
      "CK+48/surprise/S056_003_00000009.png   4\n",
      "CK+48/surprise/S076_001_00000017.png   4\n",
      "CK+48/surprise/S074_002_00000014.png   4\n",
      "CK+48/surprise/S011_001_00000016.png   4\n",
      "CK+48/surprise/S111_001_00000013.png   4\n",
      "CK+48/surprise/S066_002_00000021.png   4\n",
      "CK+48/surprise/S095_001_00000016.png   4\n",
      "CK+48/surprise/S136_001_00000017.png   4\n",
      "CK+48/surprise/S061_001_00000012.png   4\n",
      "CK+48/surprise/S097_001_00000019.png   4\n",
      "CK+48/surprise/S077_001_00000027.png   4\n",
      "CK+48/surprise/S063_001_00000013.png   4\n",
      "CK+48/surprise/S011_001_00000015.png   4\n",
      "CK+48/surprise/S026_001_00000013.png   4\n",
      "CK+48/surprise/S090_002_00000009.png   4\n",
      "CK+48/surprise/S074_002_00000016.png   4\n",
      "CK+48/surprise/S122_001_00000012.png   4\n",
      "CK+48/surprise/S032_001_00000021.png   4\n",
      "CK+48/surprise/S059_001_00000016.png   4\n",
      "CK+48/surprise/S042_001_00000019.png   4\n",
      "CK+48/surprise/S066_002_00000022.png   4\n",
      "CK+48/surprise/S125_007_00000008.png   4\n",
      "CK+48/surprise/S113_001_00000010.png   4\n",
      "CK+48/surprise/S136_001_00000019.png   4\n",
      "CK+48/surprise/S078_001_00000033.png   4\n",
      "CK+48/surprise/S056_003_00000010.png   4\n",
      "CK+48/surprise/S115_001_00000006.png   4\n",
      "CK+48/surprise/S079_001_00000010.png   4\n",
      "CK+48/surprise/S101_002_00000018.png   4\n",
      "CK+48/surprise/S068_003_00000012.png   4\n",
      "CK+48/surprise/S087_001_00000009.png   4\n",
      "CK+48/surprise/S046_002_00000006.png   4\n",
      "CK+48/surprise/S052_001_00000014.png   4\n",
      "CK+48/surprise/S063_001_00000011.png   4\n",
      "CK+48/surprise/S084_001_00000008.png   4\n",
      "CK+48/surprise/S082_001_00000014.png   4\n",
      "CK+48/surprise/S138_004_00000012.png   4\n",
      "CK+48/surprise/S137_001_00000013.png   4\n",
      "CK+48/surprise/S058_001_00000020.png   4\n",
      "CK+48/surprise/S044_001_00000024.png   4\n",
      "CK+48/surprise/S114_001_00000016.png   4\n",
      "CK+48/surprise/S050_002_00000018.png   4\n",
      "CK+48/surprise/S035_001_00000013.png   4\n",
      "CK+48/surprise/S011_001_00000014.png   4\n",
      "CK+48/surprise/S052_001_00000015.png   4\n",
      "CK+48/surprise/S095_001_00000014.png   4\n",
      "CK+48/surprise/S063_001_00000012.png   4\n",
      "CK+48/surprise/S114_001_00000018.png   4\n",
      "CK+48/surprise/S062_002_00000015.png   4\n",
      "CK+48/surprise/S065_003_00000022.png   4\n",
      "CK+48/surprise/S032_001_00000022.png   4\n",
      "CK+48/surprise/S131_001_00000016.png   4\n",
      "CK+48/surprise/S055_001_00000012.png   4\n",
      "CK+48/surprise/S037_001_00000019.png   4\n",
      "CK+48/surprise/S116_001_00000012.png   4\n",
      "CK+48/surprise/S114_001_00000017.png   4\n",
      "CK+48/surprise/S052_001_00000013.png   4\n",
      "CK+48/surprise/S081_001_00000017.png   4\n",
      "CK+48/surprise/S022_001_00000030.png   4\n",
      "CK+48/surprise/S034_001_00000027.png   4\n",
      "CK+48/surprise/S138_004_00000011.png   4\n",
      "CK+48/surprise/S082_001_00000015.png   4\n",
      "CK+48/surprise/S119_001_00000011.png   4\n",
      "CK+48/surprise/S085_003_00000013.png   4\n",
      "CK+48/surprise/S088_001_00000015.png   4\n",
      "CK+48/surprise/S051_002_00000018.png   4\n",
      "CK+48/surprise/S088_001_00000016.png   4\n",
      "CK+48/surprise/S135_001_00000039.png   4\n",
      "CK+48/surprise/S053_001_00000021.png   4\n",
      "CK+48/surprise/S076_001_00000016.png   4\n",
      "CK+48/surprise/S075_002_00000014.png   4\n",
      "CK+48/surprise/S060_003_00000017.png   4\n",
      "CK+48/surprise/S096_001_00000007.png   4\n",
      "CK+48/surprise/S026_001_00000015.png   4\n",
      "CK+48/surprise/S102_002_00000016.png   4\n",
      "CK+48/surprise/S097_001_00000021.png   4\n",
      "CK+48/surprise/S014_001_00000027.png   4\n",
      "CK+48/surprise/S101_002_00000019.png   4\n",
      "CK+48/surprise/S092_001_00000017.png   4\n",
      "CK+48/surprise/S067_002_00000013.png   4\n",
      "CK+48/surprise/S053_001_00000023.png   4\n",
      "CK+48/surprise/S138_004_00000013.png   4\n",
      "CK+48/surprise/S117_001_00000014.png   4\n",
      "CK+48/surprise/S124_001_00000012.png   4\n",
      "CK+48/surprise/S022_001_00000028.png   4\n",
      "CK+48/surprise/S055_001_00000010.png   4\n",
      "CK+48/surprise/S107_001_00000009.png   4\n",
      "CK+48/surprise/S054_003_00000007.png   4\n",
      "CK+48/surprise/S129_002_00000010.png   4\n",
      "CK+48/surprise/S133_009_00000005.png   4\n",
      "CK+48/surprise/S059_001_00000018.png   4\n",
      "CK+48/surprise/S057_001_00000019.png   4\n",
      "CK+48/surprise/S087_001_00000011.png   4\n",
      "CK+48/surprise/S069_002_00000014.png   4\n",
      "CK+48/surprise/S097_001_00000020.png   4\n",
      "CK+48/surprise/S100_002_00000014.png   4\n",
      "CK+48/surprise/S117_001_00000013.png   4\n",
      "CK+48/surprise/S058_001_00000018.png   4\n",
      "CK+48/surprise/S095_001_00000015.png   4\n",
      "CK+48/surprise/S080_001_00000016.png   4\n",
      "CK+48/surprise/S133_009_00000006.png   4\n",
      "CK+48/surprise/S034_001_00000029.png   4\n",
      "CK+48/surprise/S090_002_00000010.png   4\n",
      "CK+48/surprise/S137_001_00000014.png   4\n",
      "CK+48/surprise/S094_001_00000009.png   4\n",
      "CK+48/surprise/S110_001_00000013.png   4\n",
      "CK+48/surprise/S073_001_00000011.png   4\n",
      "CK+48/surprise/S086_001_00000018.png   4\n",
      "CK+48/surprise/S130_001_00000018.png   4\n",
      "CK+48/surprise/S084_001_00000010.png   4\n",
      "CK+48/surprise/S064_001_00000012.png   4\n",
      "CK+48/surprise/S051_002_00000019.png   4\n",
      "CK+48/surprise/S067_002_00000014.png   4\n",
      "CK+48/surprise/S099_001_00000012.png   4\n",
      "CK+48/surprise/S056_003_00000008.png   4\n",
      "CK+48/surprise/S116_001_00000013.png   4\n",
      "CK+48/surprise/S137_001_00000012.png   4\n",
      "CK+48/surprise/S102_002_00000018.png   4\n",
      "CK+48/surprise/S055_001_00000011.png   4\n",
      "CK+48/surprise/S080_001_00000017.png   4\n",
      "CK+48/surprise/S085_003_00000012.png   4\n",
      "CK+48/surprise/S132_008_00000008.png   4\n",
      "CK+48/surprise/S035_001_00000015.png   4\n",
      "CK+48/surprise/S132_008_00000010.png   4\n",
      "CK+48/surprise/S064_001_00000010.png   4\n",
      "CK+48/surprise/S060_003_00000016.png   4\n",
      "CK+48/surprise/S126_004_00000011.png   4\n",
      "CK+48/surprise/S061_001_00000011.png   4\n",
      "CK+48/surprise/S037_001_00000020.png   4\n",
      "CK+48/surprise/S022_001_00000029.png   4\n",
      "CK+48/surprise/S111_001_00000014.png   4\n",
      "CK+48/surprise/S044_001_00000023.png   4\n",
      "CK+48/surprise/S100_002_00000013.png   4\n",
      "CK+48/surprise/S136_001_00000018.png   4\n",
      "CK+48/surprise/S066_002_00000020.png   4\n",
      "CK+48/surprise/S077_001_00000026.png   4\n",
      "CK+48/surprise/S044_001_00000022.png   4\n",
      "CK+48/surprise/S135_001_00000037.png   4\n",
      "CK+48/surprise/S111_001_00000012.png   4\n",
      "CK+48/surprise/S117_001_00000012.png   4\n",
      "CK+48/surprise/S089_001_00000016.png   4\n",
      "CK+48/surprise/S046_002_00000004.png   4\n",
      "CK+48/surprise/S073_001_00000012.png   4\n",
      "CK+48/surprise/S059_001_00000017.png   4\n",
      "CK+48/surprise/S060_003_00000018.png   4\n",
      "CK+48/surprise/S054_003_00000006.png   4\n",
      "CK+48/surprise/S090_002_00000011.png   4\n",
      "CK+48/surprise/S110_001_00000012.png   4\n",
      "CK+48/surprise/S088_001_00000017.png   4\n",
      "CK+48/surprise/S107_001_00000008.png   4\n",
      "CK+48/surprise/S074_002_00000015.png   4\n",
      "CK+48/surprise/S026_001_00000014.png   4\n",
      "CK+48/surprise/S010_002_00000013.png   4\n",
      "CK+48/surprise/S122_001_00000010.png   4\n",
      "CK+48/surprise/S064_001_00000011.png   4\n",
      "CK+48/surprise/S079_001_00000011.png   4\n",
      "CK+48/surprise/S058_001_00000019.png   4\n",
      "CK+48/surprise/S082_001_00000013.png   4\n",
      "CK+48/surprise/S067_002_00000012.png   4\n",
      "CK+48/surprise/S062_002_00000014.png   4\n",
      "CK+48/surprise/S046_002_00000005.png   4\n",
      "CK+48/surprise/S122_001_00000011.png   4\n",
      "CK+48/surprise/S054_003_00000005.png   4\n",
      "CK+48/surprise/S125_007_00000009.png   4\n",
      "CK+48/surprise/S131_001_00000015.png   4\n",
      "CK+48/surprise/S096_001_00000006.png   4\n",
      "CK+48/surprise/S086_001_00000017.png   4\n",
      "CK+48/surprise/S125_007_00000007.png   4\n",
      "CK+48/surprise/S085_003_00000011.png   4\n",
      "CK+48/surprise/S075_002_00000012.png   4\n",
      "CK+48/surprise/S035_001_00000014.png   4\n",
      "CK+48/surprise/S037_001_00000018.png   4\n",
      "CK+48/surprise/S010_002_00000014.png   4\n",
      "CK+48/surprise/S100_002_00000015.png   4\n",
      "CK+48/surprise/S087_001_00000010.png   4\n",
      "CK+48/surprise/S070_002_00000014.png   4\n",
      "CK+48/surprise/S065_003_00000021.png   4\n",
      "CK+48/surprise/S081_001_00000018.png   4\n",
      "CK+48/surprise/S057_001_00000017.png   4\n",
      "CK+48/surprise/S071_001_00000011.png   4\n",
      "CK+48/surprise/S062_002_00000016.png   4\n",
      "CK+48/surprise/S014_001_00000028.png   4\n",
      "CK+48/surprise/S089_001_00000015.png   4\n",
      "CK+48/surprise/S077_001_00000028.png   4\n",
      "CK+48/surprise/S092_001_00000016.png   4\n",
      "CK+48/surprise/S115_001_00000008.png   4\n",
      "CK+48/surprise/S075_002_00000013.png   4\n",
      "CK+48/surprise/S081_001_00000019.png   4\n",
      "CK+48/surprise/S057_001_00000018.png   4\n",
      "CK+48/surprise/S110_001_00000011.png   4\n",
      "CK+48/surprise/S078_001_00000032.png   4\n",
      "CK+48/surprise/S135_001_00000038.png   4\n",
      "CK+48/surprise/S094_001_00000010.png   4\n",
      "CK+48/surprise/S133_009_00000004.png   4\n",
      "CK+48/surprise/S124_001_00000013.png   4\n",
      "CK+48/surprise/S119_001_00000010.png   4\n",
      "CK+48/surprise/S102_002_00000017.png   4\n",
      "CK+48/surprise/S061_001_00000010.png   4\n",
      "CK+48/surprise/S068_003_00000013.png   4\n",
      "CK+48/surprise/S086_001_00000019.png   4\n",
      "CK+48/surprise/S107_001_00000010.png   4\n",
      "CK+48/surprise/S116_001_00000014.png   4\n",
      "CK+48/surprise/S071_001_00000012.png   4\n",
      "CK+48/surprise/S124_001_00000014.png   4\n",
      "CK+48/surprise/S050_002_00000017.png   4\n",
      "CK+48/surprise/S076_001_00000015.png   4\n",
      "CK+48/surprise/S092_001_00000015.png   4\n",
      "CK+48/surprise/S127_001_00000016.png   4\n",
      "CK+48/surprise/S115_001_00000007.png   4\n",
      "CK+48/surprise/S032_001_00000020.png   4\n",
      "CK+48/surprise/S099_001_00000013.png   4\n",
      "CK+48/surprise/S053_001_00000022.png   4\n",
      "CK+48/surprise/S073_001_00000013.png   4\n",
      "CK+48/surprise/S042_001_00000017.png   4\n",
      "CK+48/surprise/S071_001_00000013.png   4\n",
      "CK+48/surprise/S069_002_00000013.png   4\n",
      "CK+48/surprise/S070_002_00000015.png   4\n",
      "CK+48/surprise/S094_001_00000008.png   4\n",
      "CK+48/surprise/S127_001_00000017.png   4\n",
      "CK+48/surprise/S130_001_00000016.png   4\n",
      "CK+48/surprise/S084_001_00000009.png   4\n",
      "CK+48/surprise/S096_001_00000005.png   4\n",
      "CK+48/surprise/S089_001_00000014.png   4\n",
      "CK+48/surprise/S068_003_00000014.png   4\n",
      "CK+48/surprise/S130_001_00000017.png   4\n",
      "CK+48/surprise/S050_002_00000016.png   4\n",
      "CK+48/surprise/S010_002_00000012.png   4\n",
      "CK+48/surprise/S065_003_00000020.png   4\n",
      "CK+48/surprise/S119_001_00000009.png   4\n",
      "CK+48/surprise/S069_002_00000012.png   4\n",
      "CK+48/surprise/S042_001_00000018.png   4\n",
      "CK+48/surprise/S113_001_00000011.png   4\n",
      "CK+48/surprise/S113_001_00000012.png   4\n",
      "CK+48/surprise/S099_001_00000014.png   4\n",
      "CK+48/surprise/S079_001_00000012.png   4\n",
      "CK+48/surprise/S129_002_00000011.png   4\n",
      "CK+48/surprise/S078_001_00000031.png   4\n",
      "CK+48/surprise/S126_004_00000010.png   4\n",
      "CK+48/surprise/S080_001_00000018.png   4\n",
      "CK+48/surprise/S034_001_00000028.png   4\n",
      "CK+48/surprise/S051_002_00000017.png   4\n",
      "CK+48/surprise/S070_002_00000016.png   4\n",
      "CK+48/surprise/S126_004_00000012.png   4\n",
      "CK+48/surprise/S132_008_00000009.png   4\n",
      "CK+48/surprise/S127_001_00000015.png   4\n",
      "CK+48/surprise/S131_001_00000014.png   4\n",
      "CK+48/surprise/S014_001_00000029.png   4\n",
      "CK+48/surprise/S101_002_00000017.png   4\n",
      "CK+48/surprise/S129_002_00000009.png   4\n",
      "CK+48/anger/S136_005_00000010.png   3\n",
      "CK+48/anger/S042_004_00000019.png   3\n",
      "CK+48/anger/S134_003_00000010.png   3\n",
      "CK+48/anger/S034_003_00000026.png   3\n",
      "CK+48/anger/S504_001_00000020.png   3\n",
      "CK+48/anger/S130_007_00000018.png   3\n",
      "CK+48/anger/S087_007_00000015.png   3\n",
      "CK+48/anger/S067_004_00000023.png   3\n",
      "CK+48/anger/S050_004_00000021.png   3\n",
      "CK+48/anger/S011_004_00000020.png   3\n",
      "CK+48/anger/S028_001_00000022.png   3\n",
      "CK+48/anger/S126_008_00000027.png   3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CK+48/anger/S999_001_00000016.png   3\n",
      "CK+48/anger/S072_005_00000019.png   3\n",
      "CK+48/anger/S113_008_00000022.png   3\n",
      "CK+48/anger/S037_003_00000021.png   3\n",
      "CK+48/anger/S010_004_00000019.png   3\n",
      "CK+48/anger/S075_008_00000011.png   3\n",
      "CK+48/anger/S055_004_00000028.png   3\n",
      "CK+48/anger/S028_001_00000024.png   3\n",
      "CK+48/anger/S042_004_00000020.png   3\n",
      "CK+48/anger/S037_003_00000020.png   3\n",
      "CK+48/anger/S032_003_00000017.png   3\n",
      "CK+48/anger/S032_003_00000015.png   3\n",
      "CK+48/anger/S506_001_00000039.png   3\n",
      "CK+48/anger/S999_001_00000017.png   3\n",
      "CK+48/anger/S042_004_00000018.png   3\n",
      "CK+48/anger/S055_004_00000027.png   3\n",
      "CK+48/anger/S087_007_00000016.png   3\n",
      "CK+48/anger/S045_005_00000029.png   3\n",
      "CK+48/anger/S087_007_00000014.png   3\n",
      "CK+48/anger/S090_007_00000014.png   3\n",
      "CK+48/anger/S029_001_00000019.png   3\n",
      "CK+48/anger/S112_005_00000015.png   3\n",
      "CK+48/anger/S029_001_00000017.png   3\n",
      "CK+48/anger/S022_005_00000031.png   3\n",
      "CK+48/anger/S090_007_00000012.png   3\n",
      "CK+48/anger/S089_003_00000036.png   3\n",
      "CK+48/anger/S999_001_00000018.png   3\n",
      "CK+48/anger/S501_001_00000065.png   3\n",
      "CK+48/anger/S109_003_00000016.png   3\n",
      "CK+48/anger/S090_007_00000013.png   3\n",
      "CK+48/anger/S117_006_00000008.png   3\n",
      "CK+48/anger/S092_003_00000013.png   3\n",
      "CK+48/anger/S029_001_00000018.png   3\n",
      "CK+48/anger/S026_003_00000015.png   3\n",
      "CK+48/anger/S058_005_00000009.png   3\n",
      "CK+48/anger/S010_004_00000018.png   3\n",
      "CK+48/anger/S050_004_00000020.png   3\n",
      "CK+48/anger/S502_001_00000015.png   3\n",
      "CK+48/anger/S022_005_00000030.png   3\n",
      "CK+48/anger/S045_005_00000028.png   3\n",
      "CK+48/anger/S133_003_00000047.png   3\n",
      "CK+48/anger/S503_001_00000071.png   3\n",
      "CK+48/anger/S136_005_00000009.png   3\n",
      "CK+48/anger/S127_010_00000017.png   3\n",
      "CK+48/anger/S111_006_00000009.png   3\n",
      "CK+48/anger/S022_005_00000032.png   3\n",
      "CK+48/anger/S111_006_00000008.png   3\n",
      "CK+48/anger/S113_008_00000023.png   3\n",
      "CK+48/anger/S506_001_00000040.png   3\n",
      "CK+48/anger/S503_001_00000069.png   3\n",
      "CK+48/anger/S092_003_00000014.png   3\n",
      "CK+48/anger/S100_005_00000022.png   3\n",
      "CK+48/anger/S089_003_00000034.png   3\n",
      "CK+48/anger/S092_003_00000012.png   3\n",
      "CK+48/anger/S129_006_00000008.png   3\n",
      "CK+48/anger/S117_006_00000010.png   3\n",
      "CK+48/anger/S089_003_00000035.png   3\n",
      "CK+48/anger/S058_005_00000008.png   3\n",
      "CK+48/anger/S119_008_00000017.png   3\n",
      "CK+48/anger/S126_008_00000028.png   3\n",
      "CK+48/anger/S066_005_00000011.png   3\n",
      "CK+48/anger/S109_003_00000015.png   3\n",
      "CK+48/anger/S119_008_00000016.png   3\n",
      "CK+48/anger/S075_008_00000010.png   3\n",
      "CK+48/anger/S126_008_00000029.png   3\n",
      "CK+48/anger/S045_005_00000030.png   3\n",
      "CK+48/anger/S130_007_00000019.png   3\n",
      "CK+48/anger/S119_008_00000018.png   3\n",
      "CK+48/anger/S055_004_00000026.png   3\n",
      "CK+48/anger/S026_003_00000013.png   3\n",
      "CK+48/anger/S082_005_00000017.png   3\n",
      "CK+48/anger/S112_005_00000016.png   3\n",
      "CK+48/anger/S072_005_00000017.png   3\n",
      "CK+48/anger/S503_001_00000070.png   3\n",
      "CK+48/anger/S111_006_00000010.png   3\n",
      "CK+48/anger/S066_005_00000010.png   3\n",
      "CK+48/anger/S067_004_00000021.png   3\n",
      "CK+48/anger/S100_005_00000021.png   3\n",
      "CK+48/anger/S501_001_00000067.png   3\n",
      "CK+48/anger/S504_001_00000021.png   3\n",
      "CK+48/anger/S011_004_00000019.png   3\n",
      "CK+48/anger/S071_004_00000028.png   3\n",
      "CK+48/anger/S134_003_00000009.png   3\n",
      "CK+48/anger/S130_007_00000020.png   3\n",
      "CK+48/anger/S011_004_00000021.png   3\n",
      "CK+48/anger/S133_003_00000046.png   3\n",
      "CK+48/anger/S502_001_00000014.png   3\n",
      "CK+48/anger/S034_003_00000027.png   3\n",
      "CK+48/anger/S066_005_00000009.png   3\n",
      "CK+48/anger/S037_003_00000022.png   3\n",
      "CK+48/anger/S075_008_00000012.png   3\n",
      "CK+48/anger/S502_001_00000016.png   3\n",
      "CK+48/anger/S129_006_00000009.png   3\n",
      "CK+48/anger/S082_005_00000016.png   3\n",
      "CK+48/anger/S014_003_00000030.png   3\n",
      "CK+48/anger/S071_004_00000027.png   3\n",
      "CK+48/anger/S010_004_00000017.png   3\n",
      "CK+48/anger/S014_003_00000028.png   3\n",
      "CK+48/anger/S072_005_00000018.png   3\n",
      "CK+48/anger/S026_003_00000014.png   3\n",
      "CK+48/anger/S028_001_00000023.png   3\n",
      "CK+48/anger/S100_005_00000023.png   3\n",
      "CK+48/anger/S136_005_00000008.png   3\n",
      "CK+48/anger/S506_001_00000038.png   3\n",
      "CK+48/anger/S133_003_00000045.png   3\n",
      "CK+48/anger/S058_005_00000010.png   3\n",
      "CK+48/anger/S117_006_00000009.png   3\n",
      "CK+48/anger/S082_005_00000015.png   3\n",
      "CK+48/anger/S067_004_00000022.png   3\n",
      "CK+48/anger/S127_010_00000018.png   3\n",
      "CK+48/anger/S501_001_00000066.png   3\n",
      "CK+48/anger/S504_001_00000022.png   3\n",
      "CK+48/anger/S112_005_00000017.png   3\n",
      "CK+48/anger/S032_003_00000016.png   3\n",
      "CK+48/anger/S129_006_00000010.png   3\n",
      "CK+48/anger/S109_003_00000017.png   3\n",
      "CK+48/anger/S034_003_00000025.png   3\n",
      "CK+48/anger/S113_008_00000021.png   3\n",
      "CK+48/anger/S050_004_00000019.png   3\n",
      "CK+48/anger/S014_003_00000029.png   3\n",
      "CK+48/anger/S134_003_00000011.png   3\n",
      "CK+48/anger/S071_004_00000026.png   3\n",
      "CK+48/anger/S127_010_00000016.png   3\n",
      "CK+48/disgust/S088_004_00000020.png   5\n",
      "CK+48/disgust/S128_004_00000012.png   5\n",
      "CK+48/disgust/S081_008_00000009.png   5\n",
      "CK+48/disgust/S090_006_00000010.png   5\n",
      "CK+48/disgust/S070_005_00000016.png   5\n",
      "CK+48/disgust/S111_007_00000014.png   5\n",
      "CK+48/disgust/S106_004_00000006.png   5\n",
      "CK+48/disgust/S085_004_00000016.png   5\n",
      "CK+48/disgust/S082_007_00000010.png   5\n",
      "CK+48/disgust/S131_010_00000018.png   5\n",
      "CK+48/disgust/S051_003_00000018.png   5\n",
      "CK+48/disgust/S061_004_00000020.png   5\n",
      "CK+48/disgust/S077_006_00000014.png   5\n",
      "CK+48/disgust/S088_004_00000018.png   5\n",
      "CK+48/disgust/S134_008_00000012.png   5\n",
      "CK+48/disgust/S068_005_00000021.png   5\n",
      "CK+48/disgust/S067_006_00000009.png   5\n",
      "CK+48/disgust/S069_003_00000011.png   5\n",
      "CK+48/disgust/S077_006_00000013.png   5\n",
      "CK+48/disgust/S075_005_00000010.png   5\n",
      "CK+48/disgust/S130_012_00000009.png   5\n",
      "CK+48/disgust/S124_006_00000011.png   5\n",
      "CK+48/disgust/S060_005_00000020.png   5\n",
      "CK+48/disgust/S055_003_00000009.png   5\n",
      "CK+48/disgust/S108_006_00000018.png   5\n",
      "CK+48/disgust/S054_004_00000024.png   5\n",
      "CK+48/disgust/S074_004_00000017.png   5\n",
      "CK+48/disgust/S054_004_00000022.png   5\n",
      "CK+48/disgust/S108_006_00000020.png   5\n",
      "CK+48/disgust/S107_005_00000010.png   5\n",
      "CK+48/disgust/S065_005_00000008.png   5\n",
      "CK+48/disgust/S124_006_00000009.png   5\n",
      "CK+48/disgust/S062_005_00000029.png   5\n",
      "CK+48/disgust/S052_006_00000012.png   5\n",
      "CK+48/disgust/S005_001_00000009.png   5\n",
      "CK+48/disgust/S105_008_00000010.png   5\n",
      "CK+48/disgust/S035_005_00000019.png   5\n",
      "CK+48/disgust/S005_001_00000011.png   5\n",
      "CK+48/disgust/S134_008_00000013.png   5\n",
      "CK+48/disgust/S109_005_00000014.png   5\n",
      "CK+48/disgust/S074_004_00000016.png   5\n",
      "CK+48/disgust/S035_005_00000018.png   5\n",
      "CK+48/disgust/S065_005_00000007.png   5\n",
      "CK+48/disgust/S076_005_00000012.png   5\n",
      "CK+48/disgust/S032_005_00000016.png   5\n",
      "CK+48/disgust/S129_011_00000016.png   5\n",
      "CK+48/disgust/S125_008_00000009.png   5\n",
      "CK+48/disgust/S074_004_00000018.png   5\n",
      "CK+48/disgust/S087_004_00000011.png   5\n",
      "CK+48/disgust/S022_006_00000016.png   5\n",
      "CK+48/disgust/S054_004_00000023.png   5\n",
      "CK+48/disgust/S068_005_00000020.png   5\n",
      "CK+48/disgust/S102_009_00000013.png   5\n",
      "CK+48/disgust/S116_006_00000007.png   5\n",
      "CK+48/disgust/S106_004_00000008.png   5\n",
      "CK+48/disgust/S057_003_00000015.png   5\n",
      "CK+48/disgust/S134_008_00000011.png   5\n",
      "CK+48/disgust/S088_004_00000019.png   5\n",
      "CK+48/disgust/S056_002_00000008.png   5\n",
      "CK+48/disgust/S060_005_00000021.png   5\n",
      "CK+48/disgust/S069_003_00000010.png   5\n",
      "CK+48/disgust/S073_006_00000013.png   5\n",
      "CK+48/disgust/S045_004_00000015.png   5\n",
      "CK+48/disgust/S116_006_00000005.png   5\n",
      "CK+48/disgust/S045_004_00000014.png   5\n",
      "CK+48/disgust/S071_006_00000014.png   5\n",
      "CK+48/disgust/S105_008_00000009.png   5\n",
      "CK+48/disgust/S087_004_00000012.png   5\n",
      "CK+48/disgust/S132_005_00000014.png   5\n",
      "CK+48/disgust/S079_002_00000010.png   5\n",
      "CK+48/disgust/S067_006_00000011.png   5\n",
      "CK+48/disgust/S056_002_00000009.png   5\n",
      "CK+48/disgust/S078_007_00000013.png   5\n",
      "CK+48/disgust/S124_006_00000010.png   5\n",
      "CK+48/disgust/S058_006_00000016.png   5\n",
      "CK+48/disgust/S105_008_00000008.png   5\n",
      "CK+48/disgust/S044_006_00000018.png   5\n",
      "CK+48/disgust/S085_004_00000017.png   5\n",
      "CK+48/disgust/S068_005_00000019.png   5\n",
      "CK+48/disgust/S097_004_00000028.png   5\n",
      "CK+48/disgust/S045_004_00000013.png   5\n",
      "CK+48/disgust/S132_005_00000016.png   5\n",
      "CK+48/disgust/S011_005_00000020.png   5\n",
      "CK+48/disgust/S057_003_00000013.png   5\n",
      "CK+48/disgust/S095_006_00000012.png   5\n",
      "CK+48/disgust/S096_003_00000011.png   5\n",
      "CK+48/disgust/S128_004_00000013.png   5\n",
      "CK+48/disgust/S070_005_00000015.png   5\n",
      "CK+48/disgust/S076_005_00000010.png   5\n",
      "CK+48/disgust/S032_005_00000014.png   5\n",
      "CK+48/disgust/S046_004_00000015.png   5\n",
      "CK+48/disgust/S109_005_00000013.png   5\n",
      "CK+48/disgust/S099_007_00000010.png   5\n",
      "CK+48/disgust/S058_006_00000018.png   5\n",
      "CK+48/disgust/S067_006_00000010.png   5\n",
      "CK+48/disgust/S079_002_00000011.png   5\n",
      "CK+48/disgust/S125_008_00000008.png   5\n",
      "CK+48/disgust/S111_007_00000013.png   5\n",
      "CK+48/disgust/S095_006_00000013.png   5\n",
      "CK+48/disgust/S087_004_00000010.png   5\n",
      "CK+48/disgust/S046_004_00000016.png   5\n",
      "CK+48/disgust/S081_008_00000010.png   5\n",
      "CK+48/disgust/S116_006_00000006.png   5\n",
      "CK+48/disgust/S062_005_00000028.png   5\n",
      "CK+48/disgust/S125_008_00000010.png   5\n",
      "CK+48/disgust/S062_005_00000027.png   5\n",
      "CK+48/disgust/S090_006_00000011.png   5\n",
      "CK+48/disgust/S044_006_00000017.png   5\n",
      "CK+48/disgust/S102_009_00000014.png   5\n",
      "CK+48/disgust/S051_003_00000017.png   5\n",
      "CK+48/disgust/S128_004_00000011.png   5\n",
      "CK+48/disgust/S080_008_00000008.png   5\n",
      "CK+48/disgust/S102_009_00000015.png   5\n",
      "CK+48/disgust/S130_012_00000011.png   5\n",
      "CK+48/disgust/S057_003_00000014.png   5\n",
      "CK+48/disgust/S097_004_00000029.png   5\n",
      "CK+48/disgust/S130_012_00000010.png   5\n",
      "CK+48/disgust/S055_003_00000008.png   5\n",
      "CK+48/disgust/S098_003_00000011.png   5\n",
      "CK+48/disgust/S129_011_00000018.png   5\n",
      "CK+48/disgust/S131_010_00000016.png   5\n",
      "CK+48/disgust/S131_010_00000017.png   5\n",
      "CK+48/disgust/S044_006_00000019.png   5\n",
      "CK+48/disgust/S065_005_00000006.png   5\n",
      "CK+48/disgust/S056_002_00000010.png   5\n",
      "CK+48/disgust/S052_006_00000013.png   5\n",
      "CK+48/disgust/S071_006_00000012.png   5\n",
      "CK+48/disgust/S107_005_00000009.png   5\n",
      "CK+48/disgust/S061_004_00000021.png   5\n",
      "CK+48/disgust/S061_004_00000022.png   5\n",
      "CK+48/disgust/S032_005_00000015.png   5\n",
      "CK+48/disgust/S022_006_00000017.png   5\n",
      "CK+48/disgust/S078_007_00000012.png   5\n",
      "CK+48/disgust/S073_006_00000012.png   5\n",
      "CK+48/disgust/S022_006_00000015.png   5\n",
      "CK+48/disgust/S077_006_00000012.png   5\n",
      "CK+48/disgust/S099_007_00000011.png   5\n",
      "CK+48/disgust/S082_007_00000008.png   5\n",
      "CK+48/disgust/S081_008_00000011.png   5\n",
      "CK+48/disgust/S011_005_00000018.png   5\n",
      "CK+48/disgust/S107_005_00000011.png   5\n",
      "CK+48/disgust/S108_006_00000019.png   5\n",
      "CK+48/disgust/S082_007_00000009.png   5\n",
      "CK+48/disgust/S073_006_00000014.png   5\n",
      "CK+48/disgust/S070_005_00000014.png   5\n",
      "CK+48/disgust/S106_004_00000007.png   5\n",
      "CK+48/disgust/S080_008_00000009.png   5\n",
      "CK+48/disgust/S069_003_00000009.png   5\n",
      "CK+48/disgust/S071_006_00000013.png   5\n",
      "CK+48/disgust/S046_004_00000017.png   5\n",
      "CK+48/disgust/S109_005_00000012.png   5\n",
      "CK+48/disgust/S085_004_00000015.png   5\n",
      "CK+48/disgust/S035_005_00000017.png   5\n",
      "CK+48/disgust/S052_006_00000011.png   5\n",
      "CK+48/disgust/S055_003_00000007.png   5\n",
      "CK+48/disgust/S096_003_00000010.png   5\n",
      "CK+48/disgust/S051_003_00000016.png   5\n",
      "CK+48/disgust/S111_007_00000012.png   5\n",
      "CK+48/disgust/S005_001_00000010.png   5\n",
      "CK+48/disgust/S129_011_00000017.png   5\n",
      "CK+48/disgust/S099_007_00000012.png   5\n",
      "CK+48/disgust/S076_005_00000011.png   5\n",
      "CK+48/disgust/S098_003_00000013.png   5\n",
      "CK+48/disgust/S060_005_00000019.png   5\n",
      "CK+48/disgust/S132_005_00000015.png   5\n",
      "CK+48/disgust/S058_006_00000017.png   5\n",
      "CK+48/disgust/S090_006_00000009.png   5\n",
      "CK+48/disgust/S079_002_00000012.png   5\n",
      "CK+48/disgust/S097_004_00000030.png   5\n",
      "CK+48/disgust/S096_003_00000012.png   5\n",
      "CK+48/disgust/S078_007_00000011.png   5\n",
      "CK+48/disgust/S075_005_00000012.png   5\n",
      "CK+48/disgust/S075_005_00000011.png   5\n",
      "CK+48/disgust/S011_005_00000019.png   5\n",
      "CK+48/disgust/S098_003_00000012.png   5\n",
      "CK+48/disgust/S080_008_00000007.png   5\n",
      "CK+48/disgust/S095_006_00000011.png   5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CK+48/sadness/S064_004_00000014.png   6\n",
      "CK+48/sadness/S081_002_00000024.png   6\n",
      "CK+48/sadness/S108_005_00000021.png   6\n",
      "CK+48/sadness/S066_004_00000009.png   6\n",
      "CK+48/sadness/S130_009_00000019.png   6\n",
      "CK+48/sadness/S046_001_00000024.png   6\n",
      "CK+48/sadness/S113_003_00000015.png   6\n",
      "CK+48/sadness/S138_007_00000011.png   6\n",
      "CK+48/sadness/S080_005_00000011.png   6\n",
      "CK+48/sadness/S138_007_00000010.png   6\n",
      "CK+48/sadness/S130_009_00000017.png   6\n",
      "CK+48/sadness/S138_007_00000009.png   6\n",
      "CK+48/sadness/S505_006_00000017.png   6\n",
      "CK+48/sadness/S506_006_00000041.png   6\n",
      "CK+48/sadness/S066_004_00000010.png   6\n",
      "CK+48/sadness/S125_001_00000012.png   6\n",
      "CK+48/sadness/S026_002_00000015.png   6\n",
      "CK+48/sadness/S014_002_00000014.png   6\n",
      "CK+48/sadness/S503_006_00000018.png   6\n",
      "CK+48/sadness/S095_010_00000012.png   6\n",
      "CK+48/sadness/S080_005_00000012.png   6\n",
      "CK+48/sadness/S106_002_00000016.png   6\n",
      "CK+48/sadness/S130_009_00000018.png   6\n",
      "CK+48/sadness/S504_006_00000017.png   6\n",
      "CK+48/sadness/S011_002_00000020.png   6\n",
      "CK+48/sadness/S071_002_00000018.png   6\n",
      "CK+48/sadness/S132_002_00000018.png   6\n",
      "CK+48/sadness/S132_002_00000017.png   6\n",
      "CK+48/sadness/S026_002_00000014.png   6\n",
      "CK+48/sadness/S504_006_00000016.png   6\n",
      "CK+48/sadness/S136_003_00000014.png   6\n",
      "CK+48/sadness/S064_004_00000013.png   6\n",
      "CK+48/sadness/S506_006_00000040.png   6\n",
      "CK+48/sadness/S115_004_00000017.png   6\n",
      "CK+48/sadness/S115_004_00000016.png   6\n",
      "CK+48/sadness/S136_003_00000012.png   6\n",
      "CK+48/sadness/S108_005_00000020.png   6\n",
      "CK+48/sadness/S137_005_00000026.png   6\n",
      "CK+48/sadness/S503_006_00000019.png   6\n",
      "CK+48/sadness/S113_003_00000013.png   6\n",
      "CK+48/sadness/S093_001_00000018.png   6\n",
      "CK+48/sadness/S132_002_00000016.png   6\n",
      "CK+48/sadness/S014_002_00000015.png   6\n",
      "CK+48/sadness/S046_001_00000025.png   6\n",
      "CK+48/sadness/S095_010_00000013.png   6\n",
      "CK+48/sadness/S014_002_00000016.png   6\n",
      "CK+48/sadness/S071_002_00000019.png   6\n",
      "CK+48/sadness/S504_006_00000018.png   6\n",
      "CK+48/sadness/S506_006_00000042.png   6\n",
      "CK+48/sadness/S042_002_00000015.png   6\n",
      "CK+48/sadness/S501_006_00000039.png   6\n",
      "CK+48/sadness/S066_004_00000008.png   6\n",
      "CK+48/sadness/S026_002_00000016.png   6\n",
      "CK+48/sadness/S125_001_00000014.png   6\n",
      "CK+48/sadness/S113_003_00000014.png   6\n",
      "CK+48/sadness/S042_002_00000014.png   6\n",
      "CK+48/sadness/S131_003_00000022.png   6\n",
      "CK+48/sadness/S131_003_00000024.png   6\n",
      "CK+48/sadness/S115_004_00000015.png   6\n",
      "CK+48/sadness/S501_006_00000040.png   6\n",
      "CK+48/sadness/S137_005_00000027.png   6\n",
      "CK+48/sadness/S011_002_00000022.png   6\n",
      "CK+48/sadness/S501_006_00000041.png   6\n",
      "CK+48/sadness/S046_001_00000023.png   6\n",
      "CK+48/sadness/S106_002_00000015.png   6\n",
      "CK+48/sadness/S095_010_00000014.png   6\n",
      "CK+48/sadness/S131_003_00000023.png   6\n",
      "CK+48/sadness/S042_002_00000016.png   6\n",
      "CK+48/sadness/S136_003_00000013.png   6\n",
      "CK+48/sadness/S093_001_00000019.png   6\n",
      "CK+48/sadness/S125_001_00000013.png   6\n",
      "CK+48/sadness/S093_001_00000020.png   6\n",
      "CK+48/sadness/S080_005_00000013.png   6\n",
      "CK+48/sadness/S081_002_00000023.png   6\n",
      "CK+48/sadness/S137_005_00000025.png   6\n",
      "CK+48/sadness/S503_006_00000020.png   6\n",
      "CK+48/sadness/S505_006_00000019.png   6\n",
      "CK+48/sadness/S064_004_00000012.png   6\n",
      "CK+48/sadness/S071_002_00000020.png   6\n",
      "CK+48/sadness/S505_006_00000018.png   6\n",
      "CK+48/sadness/S081_002_00000022.png   6\n",
      "CK+48/sadness/S106_002_00000014.png   6\n",
      "CK+48/sadness/S108_005_00000022.png   6\n",
      "CK+48/sadness/S011_002_00000021.png   6\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import math\n",
    "\n",
    "Exp=['fear', 'contempt', 'happy', 'anger', 'surprise', 'disgust', 'sadness']\n",
    "i=0\n",
    "last=[]\n",
    "images=[]\n",
    "labels=[]\n",
    "for fle in files:\n",
    "    idx=Exp.index(fle)\n",
    "    label=idx\n",
    "  \n",
    "    total=\"CK+48\"+'/'+fle\n",
    "    files_exp= os.listdir(total)  \n",
    "    for fle_2 in files_exp:\n",
    "        file_main=total+'/'+fle_2\n",
    "        print(file_main+\"   \"+str(label))\n",
    "        image= cv2.imread(file_main)    \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image= cv2.resize(image,(48,48))\n",
    "        images.append(image)\n",
    "        labels.append(label)\n",
    "        i+=1\n",
    "    last.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_classes=7\n",
    "\n",
    "#making array\n",
    "images_f=np.array(images)\n",
    "labels_f=np.array(labels)\n",
    "\n",
    "#normalization of image\n",
    "images_f_2=images_f/255\n",
    "\n",
    "#label encoding for labels in  numbers instead of strings\n",
    "labels_encoded=tf.keras.utils.to_categorical(labels_f,num_classes=num_of_classes)\n",
    "\n",
    "#final train test split\n",
    "X_train, X_test, Y_train, Y_test= train_test_split(images_f_2, labels_encoded,test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(735, 48, 48, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten,BatchNormalization\n",
    "from tensorflow.keras.layers import Dense, MaxPooling2D,Conv2D\n",
    "from tensorflow.keras.layers import Input,Activation,Add\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def Convolution(input_tensor,filters):\n",
    "    \n",
    "    x = Conv2D(filters=filters,kernel_size=(3, 3),padding = 'same',strides=(1, 1),kernel_regularizer=l2(0.001))(input_tensor)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x= Activation('relu')(x)\n",
    "\n",
    "    return x\n",
    "def model(input_shape):\n",
    "    inputs = Input((input_shape))\n",
    "\n",
    "    conv_1= Convolution(inputs,32)\n",
    "    maxp_1 = MaxPooling2D(pool_size = (2,2)) (conv_1)\n",
    "    conv_2 = Convolution(maxp_1,64)\n",
    "    maxp_2 = MaxPooling2D(pool_size = (2, 2)) (conv_2)\n",
    "    conv_3 = Convolution(maxp_2,128)\n",
    "    maxp_3 = MaxPooling2D(pool_size = (2, 2)) (conv_3)\n",
    "    conv_4 = Convolution(maxp_3,256)\n",
    "    maxp_4 = MaxPooling2D(pool_size = (2, 2)) (conv_4)\n",
    "    flatten= Flatten() (maxp_4)\n",
    "    dense_1= Dense(128,activation='relu')(flatten)\n",
    "    drop_1=Dropout(0.2)(dense_1)\n",
    "    output= Dense(7,activation=\"sigmoid\")(drop_1)\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[output])\n",
    "\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"Adam\",\n",
    "    metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model=model(input_shape = (48,48,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 48, 48, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 48, 48, 32)        896       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 12, 12, 128)       73856     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 6, 6, 256)         295168    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               295040    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7)                 903       \n",
      "=================================================================\n",
      "Total params: 684,359\n",
      "Trainable params: 684,359\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "fle_s='Emotion_detection.h5'\n",
    "checkpointer = ModelCheckpoint(fle_s, monitor='loss',verbose=1,save_best_only=True,save_weights_only=False, mode='auto',save_freq='epoch')\n",
    "callback_list=[checkpointer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 2.0648 - accuracy: 0.2503\n",
      "Epoch 00001: loss improved from inf to 2.06484, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 5s 220ms/step - loss: 2.0648 - accuracy: 0.2503 - val_loss: 2.0128 - val_accuracy: 0.2439\n",
      "Epoch 2/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.9505 - accuracy: 0.2571\n",
      "Epoch 00002: loss improved from 2.06484 to 1.95048, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 5s 237ms/step - loss: 1.9505 - accuracy: 0.2571 - val_loss: 1.9157 - val_accuracy: 0.2439\n",
      "Epoch 3/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.8855 - accuracy: 0.2585\n",
      "Epoch 00003: loss improved from 1.95048 to 1.88548, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 5s 237ms/step - loss: 1.8855 - accuracy: 0.2585 - val_loss: 1.8912 - val_accuracy: 0.2439\n",
      "Epoch 4/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.7822 - accuracy: 0.3156\n",
      "Epoch 00004: loss improved from 1.88548 to 1.78219, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 5s 227ms/step - loss: 1.7822 - accuracy: 0.3156 - val_loss: 1.6340 - val_accuracy: 0.4390\n",
      "Epoch 5/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4778 - accuracy: 0.4707\n",
      "Epoch 00005: loss improved from 1.78219 to 1.47780, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 5s 238ms/step - loss: 1.4778 - accuracy: 0.4707 - val_loss: 1.3565 - val_accuracy: 0.5325\n",
      "Epoch 6/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.1665 - accuracy: 0.5755\n",
      "Epoch 00006: loss improved from 1.47780 to 1.16651, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 5s 229ms/step - loss: 1.1665 - accuracy: 0.5755 - val_loss: 1.0460 - val_accuracy: 0.6585\n",
      "Epoch 7/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.8912 - accuracy: 0.6925\n",
      "Epoch 00007: loss improved from 1.16651 to 0.89122, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 5s 229ms/step - loss: 0.8912 - accuracy: 0.6925 - val_loss: 0.7874 - val_accuracy: 0.7764\n",
      "Epoch 8/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.7559 - accuracy: 0.7619\n",
      "Epoch 00008: loss improved from 0.89122 to 0.75589, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 5s 227ms/step - loss: 0.7559 - accuracy: 0.7619 - val_loss: 0.7797 - val_accuracy: 0.7114\n",
      "Epoch 9/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6755 - accuracy: 0.7918\n",
      "Epoch 00009: loss improved from 0.75589 to 0.67547, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 5s 234ms/step - loss: 0.6755 - accuracy: 0.7918 - val_loss: 0.6481 - val_accuracy: 0.8618\n",
      "Epoch 10/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.5700 - accuracy: 0.8190\n",
      "Epoch 00010: loss improved from 0.67547 to 0.57000, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 5s 231ms/step - loss: 0.5700 - accuracy: 0.8190 - val_loss: 0.6041 - val_accuracy: 0.8415\n",
      "Epoch 11/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.4805 - accuracy: 0.8721\n",
      "Epoch 00011: loss improved from 0.57000 to 0.48046, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 5s 233ms/step - loss: 0.4805 - accuracy: 0.8721 - val_loss: 0.5010 - val_accuracy: 0.8699\n",
      "Epoch 12/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.4450 - accuracy: 0.8844\n",
      "Epoch 00012: loss improved from 0.48046 to 0.44502, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 6s 263ms/step - loss: 0.4450 - accuracy: 0.8844 - val_loss: 0.4366 - val_accuracy: 0.9187\n",
      "Epoch 13/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.3961 - accuracy: 0.8980\n",
      "Epoch 00013: loss improved from 0.44502 to 0.39615, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 5s 232ms/step - loss: 0.3961 - accuracy: 0.8980 - val_loss: 0.3996 - val_accuracy: 0.9024\n",
      "Epoch 14/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.3921 - accuracy: 0.9020\n",
      "Epoch 00014: loss improved from 0.39615 to 0.39207, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 6s 244ms/step - loss: 0.3921 - accuracy: 0.9020 - val_loss: 0.4685 - val_accuracy: 0.9187\n",
      "Epoch 15/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.3183 - accuracy: 0.9415\n",
      "Epoch 00015: loss improved from 0.39207 to 0.31830, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 6s 243ms/step - loss: 0.3183 - accuracy: 0.9415 - val_loss: 0.2956 - val_accuracy: 0.9553\n",
      "Epoch 16/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.3365 - accuracy: 0.9347\n",
      "Epoch 00016: loss did not improve from 0.31830\n",
      "23/23 [==============================] - 5s 224ms/step - loss: 0.3365 - accuracy: 0.9347 - val_loss: 0.5041 - val_accuracy: 0.8659\n",
      "Epoch 17/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.3288 - accuracy: 0.9333\n",
      "Epoch 00017: loss did not improve from 0.31830\n",
      "23/23 [==============================] - 4s 195ms/step - loss: 0.3288 - accuracy: 0.9333 - val_loss: 0.3779 - val_accuracy: 0.9268\n",
      "Epoch 18/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.2691 - accuracy: 0.9619\n",
      "Epoch 00018: loss improved from 0.31830 to 0.26912, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 5s 204ms/step - loss: 0.2691 - accuracy: 0.9619 - val_loss: 0.2777 - val_accuracy: 0.9593\n",
      "Epoch 19/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.2234 - accuracy: 0.9728\n",
      "Epoch 00019: loss improved from 0.26912 to 0.22343, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 5s 233ms/step - loss: 0.2234 - accuracy: 0.9728 - val_loss: 0.2639 - val_accuracy: 0.9593\n",
      "Epoch 20/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.2140 - accuracy: 0.9755\n",
      "Epoch 00020: loss improved from 0.22343 to 0.21405, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 5s 229ms/step - loss: 0.2140 - accuracy: 0.9755 - val_loss: 0.2817 - val_accuracy: 0.9553\n",
      "Epoch 21/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.2132 - accuracy: 0.9741\n",
      "Epoch 00021: loss improved from 0.21405 to 0.21319, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 5s 232ms/step - loss: 0.2132 - accuracy: 0.9741 - val_loss: 0.2484 - val_accuracy: 0.9675\n",
      "Epoch 22/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.2205 - accuracy: 0.9714\n",
      "Epoch 00022: loss did not improve from 0.21319\n",
      "23/23 [==============================] - 5s 223ms/step - loss: 0.2205 - accuracy: 0.9714 - val_loss: 0.2885 - val_accuracy: 0.9472\n",
      "Epoch 23/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.2155 - accuracy: 0.9714\n",
      "Epoch 00023: loss did not improve from 0.21319\n",
      "23/23 [==============================] - 4s 195ms/step - loss: 0.2155 - accuracy: 0.9714 - val_loss: 0.2344 - val_accuracy: 0.9675\n",
      "Epoch 24/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.2047 - accuracy: 0.9823\n",
      "Epoch 00024: loss improved from 0.21319 to 0.20471, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 5s 224ms/step - loss: 0.2047 - accuracy: 0.9823 - val_loss: 0.2341 - val_accuracy: 0.9756\n",
      "Epoch 25/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1905 - accuracy: 0.9837\n",
      "Epoch 00025: loss improved from 0.20471 to 0.19045, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 5s 229ms/step - loss: 0.1905 - accuracy: 0.9837 - val_loss: 0.1973 - val_accuracy: 0.9756\n",
      "Epoch 26/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1746 - accuracy: 0.9850\n",
      "Epoch 00026: loss improved from 0.19045 to 0.17463, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 5s 228ms/step - loss: 0.1746 - accuracy: 0.9850 - val_loss: 0.2085 - val_accuracy: 0.9837\n",
      "Epoch 27/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1780 - accuracy: 0.9878\n",
      "Epoch 00027: loss did not improve from 0.17463\n",
      "23/23 [==============================] - 5s 223ms/step - loss: 0.1780 - accuracy: 0.9878 - val_loss: 0.1822 - val_accuracy: 0.9756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1971 - accuracy: 0.9701\n",
      "Epoch 00028: loss did not improve from 0.17463\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.1971 - accuracy: 0.9701 - val_loss: 0.2657 - val_accuracy: 0.9553\n",
      "Epoch 29/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1868 - accuracy: 0.9755\n",
      "Epoch 00029: loss did not improve from 0.17463\n",
      "23/23 [==============================] - 5s 196ms/step - loss: 0.1868 - accuracy: 0.9755 - val_loss: 0.2643 - val_accuracy: 0.9593\n",
      "Epoch 30/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1909 - accuracy: 0.9782\n",
      "Epoch 00030: loss did not improve from 0.17463\n",
      "23/23 [==============================] - 4s 184ms/step - loss: 0.1909 - accuracy: 0.9782 - val_loss: 0.2421 - val_accuracy: 0.9593\n",
      "Epoch 31/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.2270 - accuracy: 0.9728\n",
      "Epoch 00031: loss did not improve from 0.17463\n",
      "23/23 [==============================] - 4s 187ms/step - loss: 0.2270 - accuracy: 0.9728 - val_loss: 0.2224 - val_accuracy: 0.9715\n",
      "Epoch 32/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1660 - accuracy: 0.9891\n",
      "Epoch 00032: loss improved from 0.17463 to 0.16600, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 5s 213ms/step - loss: 0.1660 - accuracy: 0.9891 - val_loss: 0.1993 - val_accuracy: 0.9797\n",
      "Epoch 33/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1695 - accuracy: 0.9918\n",
      "Epoch 00033: loss did not improve from 0.16600\n",
      "23/23 [==============================] - 5s 232ms/step - loss: 0.1695 - accuracy: 0.9918 - val_loss: 0.1833 - val_accuracy: 0.9837\n",
      "Epoch 34/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1473 - accuracy: 0.9959\n",
      "Epoch 00034: loss improved from 0.16600 to 0.14727, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 5s 224ms/step - loss: 0.1473 - accuracy: 0.9959 - val_loss: 0.1695 - val_accuracy: 0.9837\n",
      "Epoch 35/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1383 - accuracy: 0.9932\n",
      "Epoch 00035: loss improved from 0.14727 to 0.13830, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 6s 251ms/step - loss: 0.1383 - accuracy: 0.9932 - val_loss: 0.1922 - val_accuracy: 0.9797\n",
      "Epoch 36/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1726 - accuracy: 0.9837\n",
      "Epoch 00036: loss did not improve from 0.13830\n",
      "23/23 [==============================] - 5s 231ms/step - loss: 0.1726 - accuracy: 0.9837 - val_loss: 0.1663 - val_accuracy: 0.9878\n",
      "Epoch 37/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1839 - accuracy: 0.9755\n",
      "Epoch 00037: loss did not improve from 0.13830\n",
      "23/23 [==============================] - 5s 199ms/step - loss: 0.1839 - accuracy: 0.9755 - val_loss: 0.2661 - val_accuracy: 0.9431\n",
      "Epoch 38/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1475 - accuracy: 0.9932\n",
      "Epoch 00038: loss did not improve from 0.13830\n",
      "23/23 [==============================] - 4s 193ms/step - loss: 0.1475 - accuracy: 0.9932 - val_loss: 0.1638 - val_accuracy: 0.9837\n",
      "Epoch 39/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1344 - accuracy: 0.9986\n",
      "Epoch 00039: loss improved from 0.13830 to 0.13445, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.1344 - accuracy: 0.9986 - val_loss: 0.1646 - val_accuracy: 0.9878\n",
      "Epoch 40/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1334 - accuracy: 0.9959\n",
      "Epoch 00040: loss improved from 0.13445 to 0.13341, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 6s 247ms/step - loss: 0.1334 - accuracy: 0.9959 - val_loss: 0.1779 - val_accuracy: 0.9837\n",
      "Epoch 41/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1321 - accuracy: 0.9918\n",
      "Epoch 00041: loss improved from 0.13341 to 0.13214, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 5s 237ms/step - loss: 0.1321 - accuracy: 0.9918 - val_loss: 0.1551 - val_accuracy: 0.9919\n",
      "Epoch 42/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1279 - accuracy: 0.9959\n",
      "Epoch 00042: loss improved from 0.13214 to 0.12791, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 6s 261ms/step - loss: 0.1279 - accuracy: 0.9959 - val_loss: 0.1561 - val_accuracy: 0.9878\n",
      "Epoch 43/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1253 - accuracy: 0.9959\n",
      "Epoch 00043: loss improved from 0.12791 to 0.12534, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 6s 274ms/step - loss: 0.1253 - accuracy: 0.9959 - val_loss: 0.1415 - val_accuracy: 0.9919\n",
      "Epoch 44/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1185 - accuracy: 0.9959\n",
      "Epoch 00044: loss improved from 0.12534 to 0.11848, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 6s 256ms/step - loss: 0.1185 - accuracy: 0.9959 - val_loss: 0.1401 - val_accuracy: 0.9837\n",
      "Epoch 45/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1404 - accuracy: 0.9864\n",
      "Epoch 00045: loss did not improve from 0.11848\n",
      "23/23 [==============================] - 5s 237ms/step - loss: 0.1404 - accuracy: 0.9864 - val_loss: 0.1762 - val_accuracy: 0.9837\n",
      "Epoch 46/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1443 - accuracy: 0.9905\n",
      "Epoch 00046: loss did not improve from 0.11848\n",
      "23/23 [==============================] - 5s 233ms/step - loss: 0.1443 - accuracy: 0.9905 - val_loss: 0.1925 - val_accuracy: 0.9715\n",
      "Epoch 47/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1400 - accuracy: 0.9864\n",
      "Epoch 00047: loss did not improve from 0.11848\n",
      "23/23 [==============================] - 5s 209ms/step - loss: 0.1400 - accuracy: 0.9864 - val_loss: 0.1607 - val_accuracy: 0.9837\n",
      "Epoch 48/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1506 - accuracy: 0.9864\n",
      "Epoch 00048: loss did not improve from 0.11848\n",
      "23/23 [==============================] - 5s 198ms/step - loss: 0.1506 - accuracy: 0.9864 - val_loss: 0.1545 - val_accuracy: 0.9878\n",
      "Epoch 49/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1345 - accuracy: 0.9878\n",
      "Epoch 00049: loss did not improve from 0.11848\n",
      "23/23 [==============================] - 5s 212ms/step - loss: 0.1345 - accuracy: 0.9878 - val_loss: 0.1930 - val_accuracy: 0.9756\n",
      "Epoch 50/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1491 - accuracy: 0.9878\n",
      "Epoch 00050: loss did not improve from 0.11848\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.1491 - accuracy: 0.9878 - val_loss: 0.1688 - val_accuracy: 0.9797\n",
      "Epoch 51/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1305 - accuracy: 0.9891\n",
      "Epoch 00051: loss did not improve from 0.11848\n",
      "23/23 [==============================] - 4s 192ms/step - loss: 0.1305 - accuracy: 0.9891 - val_loss: 0.1696 - val_accuracy: 0.9837\n",
      "Epoch 52/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1723 - accuracy: 0.9837\n",
      "Epoch 00052: loss did not improve from 0.11848\n",
      "23/23 [==============================] - 4s 190ms/step - loss: 0.1723 - accuracy: 0.9837 - val_loss: 0.1933 - val_accuracy: 0.9715\n",
      "Epoch 53/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1278 - accuracy: 0.9905\n",
      "Epoch 00053: loss did not improve from 0.11848\n",
      "23/23 [==============================] - 4s 190ms/step - loss: 0.1278 - accuracy: 0.9905 - val_loss: 0.1603 - val_accuracy: 0.9797\n",
      "Epoch 54/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1267 - accuracy: 0.9946\n",
      "Epoch 00054: loss did not improve from 0.11848\n",
      "23/23 [==============================] - 4s 191ms/step - loss: 0.1267 - accuracy: 0.9946 - val_loss: 0.1413 - val_accuracy: 0.9797\n",
      "Epoch 55/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1307 - accuracy: 0.9878\n",
      "Epoch 00055: loss did not improve from 0.11848\n",
      "23/23 [==============================] - 4s 187ms/step - loss: 0.1307 - accuracy: 0.9878 - val_loss: 0.1509 - val_accuracy: 0.9878\n",
      "Epoch 56/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1112 - accuracy: 0.9973\n",
      "Epoch 00056: loss improved from 0.11848 to 0.11119, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 4s 192ms/step - loss: 0.1112 - accuracy: 0.9973 - val_loss: 0.1352 - val_accuracy: 0.9878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1142 - accuracy: 0.9973\n",
      "Epoch 00057: loss did not improve from 0.11119\n",
      "23/23 [==============================] - 5s 229ms/step - loss: 0.1142 - accuracy: 0.9973 - val_loss: 0.1423 - val_accuracy: 0.9878\n",
      "Epoch 58/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1015 - accuracy: 1.0000\n",
      "Epoch 00058: loss improved from 0.11119 to 0.10152, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 4s 189ms/step - loss: 0.1015 - accuracy: 1.0000 - val_loss: 0.1269 - val_accuracy: 0.9919\n",
      "Epoch 59/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1136 - accuracy: 0.9946\n",
      "Epoch 00059: loss did not improve from 0.10152\n",
      "23/23 [==============================] - 5s 220ms/step - loss: 0.1136 - accuracy: 0.9946 - val_loss: 0.1793 - val_accuracy: 0.9675\n",
      "Epoch 60/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1314 - accuracy: 0.9891\n",
      "Epoch 00060: loss did not improve from 0.10152\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.1314 - accuracy: 0.9891 - val_loss: 0.1646 - val_accuracy: 0.9837\n",
      "Epoch 61/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1319 - accuracy: 0.9891\n",
      "Epoch 00061: loss did not improve from 0.10152\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.1319 - accuracy: 0.9891 - val_loss: 0.1829 - val_accuracy: 0.9634\n",
      "Epoch 62/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1180 - accuracy: 0.9918\n",
      "Epoch 00062: loss did not improve from 0.10152\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.1180 - accuracy: 0.9918 - val_loss: 0.1647 - val_accuracy: 0.9837\n",
      "Epoch 63/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1119 - accuracy: 0.9946\n",
      "Epoch 00063: loss did not improve from 0.10152\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.1119 - accuracy: 0.9946 - val_loss: 0.1324 - val_accuracy: 0.9797\n",
      "Epoch 64/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1040 - accuracy: 0.9973\n",
      "Epoch 00064: loss did not improve from 0.10152\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.1040 - accuracy: 0.9973 - val_loss: 0.1320 - val_accuracy: 0.9756\n",
      "Epoch 65/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0988 - accuracy: 0.9986\n",
      "Epoch 00065: loss improved from 0.10152 to 0.09881, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 4s 189ms/step - loss: 0.0988 - accuracy: 0.9986 - val_loss: 0.1468 - val_accuracy: 0.9715\n",
      "Epoch 66/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1143 - accuracy: 0.9891\n",
      "Epoch 00066: loss did not improve from 0.09881\n",
      "23/23 [==============================] - 5s 229ms/step - loss: 0.1143 - accuracy: 0.9891 - val_loss: 0.1530 - val_accuracy: 0.9797\n",
      "Epoch 67/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1147 - accuracy: 0.9932\n",
      "Epoch 00067: loss did not improve from 0.09881\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.1147 - accuracy: 0.9932 - val_loss: 0.1942 - val_accuracy: 0.9512\n",
      "Epoch 68/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1423 - accuracy: 0.9837\n",
      "Epoch 00068: loss did not improve from 0.09881\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.1423 - accuracy: 0.9837 - val_loss: 0.3161 - val_accuracy: 0.9187\n",
      "Epoch 69/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1786 - accuracy: 0.9714\n",
      "Epoch 00069: loss did not improve from 0.09881\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.1786 - accuracy: 0.9714 - val_loss: 0.1920 - val_accuracy: 0.9675\n",
      "Epoch 70/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1204 - accuracy: 0.9932\n",
      "Epoch 00070: loss did not improve from 0.09881\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.1204 - accuracy: 0.9932 - val_loss: 0.1552 - val_accuracy: 0.9837\n",
      "Epoch 71/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1267 - accuracy: 0.9932\n",
      "Epoch 00071: loss did not improve from 0.09881\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.1267 - accuracy: 0.9932 - val_loss: 0.1613 - val_accuracy: 0.9715\n",
      "Epoch 72/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1427 - accuracy: 0.9837\n",
      "Epoch 00072: loss did not improve from 0.09881\n",
      "23/23 [==============================] - 5s 227ms/step - loss: 0.1427 - accuracy: 0.9837 - val_loss: 0.1824 - val_accuracy: 0.9756\n",
      "Epoch 73/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1333 - accuracy: 0.9918\n",
      "Epoch 00073: loss did not improve from 0.09881\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.1333 - accuracy: 0.9918 - val_loss: 0.1911 - val_accuracy: 0.9675\n",
      "Epoch 74/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1123 - accuracy: 0.9973\n",
      "Epoch 00074: loss did not improve from 0.09881\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.1123 - accuracy: 0.9973 - val_loss: 0.1352 - val_accuracy: 0.9878\n",
      "Epoch 75/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1023 - accuracy: 1.0000\n",
      "Epoch 00075: loss did not improve from 0.09881\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.1023 - accuracy: 1.0000 - val_loss: 0.1136 - val_accuracy: 0.9959\n",
      "Epoch 76/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1008 - accuracy: 0.9973\n",
      "Epoch 00076: loss did not improve from 0.09881\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.1008 - accuracy: 0.9973 - val_loss: 0.1310 - val_accuracy: 0.9797\n",
      "Epoch 77/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1012 - accuracy: 0.9973\n",
      "Epoch 00077: loss did not improve from 0.09881\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.1012 - accuracy: 0.9973 - val_loss: 0.1406 - val_accuracy: 0.9837\n",
      "Epoch 78/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0999 - accuracy: 0.9973\n",
      "Epoch 00078: loss did not improve from 0.09881\n",
      "23/23 [==============================] - 4s 184ms/step - loss: 0.0999 - accuracy: 0.9973 - val_loss: 0.1381 - val_accuracy: 0.9797\n",
      "Epoch 79/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1156 - accuracy: 0.9959\n",
      "Epoch 00079: loss did not improve from 0.09881\n",
      "23/23 [==============================] - 5s 199ms/step - loss: 0.1156 - accuracy: 0.9959 - val_loss: 0.1161 - val_accuracy: 0.9959\n",
      "Epoch 80/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1007 - accuracy: 0.9959\n",
      "Epoch 00080: loss did not improve from 0.09881\n",
      "23/23 [==============================] - 5s 197ms/step - loss: 0.1007 - accuracy: 0.9959 - val_loss: 0.1715 - val_accuracy: 0.9756\n",
      "Epoch 81/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1129 - accuracy: 0.9918\n",
      "Epoch 00081: loss did not improve from 0.09881\n",
      "23/23 [==============================] - 4s 193ms/step - loss: 0.1129 - accuracy: 0.9918 - val_loss: 0.1588 - val_accuracy: 0.9715\n",
      "Epoch 82/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1196 - accuracy: 0.9905\n",
      "Epoch 00082: loss did not improve from 0.09881\n",
      "23/23 [==============================] - 4s 190ms/step - loss: 0.1196 - accuracy: 0.9905 - val_loss: 0.1284 - val_accuracy: 0.9878\n",
      "Epoch 83/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0947 - accuracy: 0.9986\n",
      "Epoch 00083: loss improved from 0.09881 to 0.09475, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 4s 195ms/step - loss: 0.0947 - accuracy: 0.9986 - val_loss: 0.1083 - val_accuracy: 0.9919\n",
      "Epoch 84/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1081 - accuracy: 0.9905\n",
      "Epoch 00084: loss did not improve from 0.09475\n",
      "23/23 [==============================] - 5s 234ms/step - loss: 0.1081 - accuracy: 0.9905 - val_loss: 0.2263 - val_accuracy: 0.9756\n",
      "Epoch 85/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1370 - accuracy: 0.9864\n",
      "Epoch 00085: loss did not improve from 0.09475\n",
      "23/23 [==============================] - 4s 186ms/step - loss: 0.1370 - accuracy: 0.9864 - val_loss: 0.1864 - val_accuracy: 0.9593\n",
      "Epoch 86/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - ETA: 0s - loss: 0.1167 - accuracy: 0.9905\n",
      "Epoch 00086: loss did not improve from 0.09475\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.1167 - accuracy: 0.9905 - val_loss: 0.1335 - val_accuracy: 0.9919\n",
      "Epoch 87/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1369 - accuracy: 0.9878\n",
      "Epoch 00087: loss did not improve from 0.09475\n",
      "23/23 [==============================] - 4s 184ms/step - loss: 0.1369 - accuracy: 0.9878 - val_loss: 0.1684 - val_accuracy: 0.9756\n",
      "Epoch 88/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1148 - accuracy: 0.9959\n",
      "Epoch 00088: loss did not improve from 0.09475\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.1148 - accuracy: 0.9959 - val_loss: 0.1409 - val_accuracy: 0.9837\n",
      "Epoch 89/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1123 - accuracy: 0.9918\n",
      "Epoch 00089: loss did not improve from 0.09475\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.1123 - accuracy: 0.9918 - val_loss: 0.1619 - val_accuracy: 0.9797\n",
      "Epoch 90/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1147 - accuracy: 0.9918\n",
      "Epoch 00090: loss did not improve from 0.09475\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.1147 - accuracy: 0.9918 - val_loss: 0.1425 - val_accuracy: 0.9878\n",
      "Epoch 91/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1032 - accuracy: 0.9959\n",
      "Epoch 00091: loss did not improve from 0.09475\n",
      "23/23 [==============================] - 4s 184ms/step - loss: 0.1032 - accuracy: 0.9959 - val_loss: 0.1252 - val_accuracy: 0.9919\n",
      "Epoch 92/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1025 - accuracy: 0.9946\n",
      "Epoch 00092: loss did not improve from 0.09475\n",
      "23/23 [==============================] - 4s 185ms/step - loss: 0.1025 - accuracy: 0.9946 - val_loss: 0.1434 - val_accuracy: 0.9837\n",
      "Epoch 93/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1127 - accuracy: 0.9905\n",
      "Epoch 00093: loss did not improve from 0.09475\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.1127 - accuracy: 0.9905 - val_loss: 0.2109 - val_accuracy: 0.9634\n",
      "Epoch 94/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1200 - accuracy: 0.9864\n",
      "Epoch 00094: loss did not improve from 0.09475\n",
      "23/23 [==============================] - 4s 190ms/step - loss: 0.1200 - accuracy: 0.9864 - val_loss: 0.1598 - val_accuracy: 0.9756\n",
      "Epoch 95/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1276 - accuracy: 0.9905\n",
      "Epoch 00095: loss did not improve from 0.09475\n",
      "23/23 [==============================] - 4s 188ms/step - loss: 0.1276 - accuracy: 0.9905 - val_loss: 0.1842 - val_accuracy: 0.9675\n",
      "Epoch 96/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1158 - accuracy: 0.9932\n",
      "Epoch 00096: loss did not improve from 0.09475\n",
      "23/23 [==============================] - 4s 188ms/step - loss: 0.1158 - accuracy: 0.9932 - val_loss: 0.1519 - val_accuracy: 0.9797\n",
      "Epoch 97/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1119 - accuracy: 0.9959\n",
      "Epoch 00097: loss did not improve from 0.09475\n",
      "23/23 [==============================] - 4s 189ms/step - loss: 0.1119 - accuracy: 0.9959 - val_loss: 0.1389 - val_accuracy: 0.9837\n",
      "Epoch 98/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0985 - accuracy: 0.9973\n",
      "Epoch 00098: loss did not improve from 0.09475\n",
      "23/23 [==============================] - 5s 215ms/step - loss: 0.0985 - accuracy: 0.9973 - val_loss: 0.1212 - val_accuracy: 0.9837\n",
      "Epoch 99/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0936 - accuracy: 0.9986\n",
      "Epoch 00099: loss improved from 0.09475 to 0.09362, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 5s 209ms/step - loss: 0.0936 - accuracy: 0.9986 - val_loss: 0.1212 - val_accuracy: 0.9919\n",
      "Epoch 100/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0951 - accuracy: 0.9959\n",
      "Epoch 00100: loss did not improve from 0.09362\n",
      "23/23 [==============================] - 5s 222ms/step - loss: 0.0951 - accuracy: 0.9959 - val_loss: 0.1590 - val_accuracy: 0.9797\n",
      "Epoch 101/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0965 - accuracy: 0.9973\n",
      "Epoch 00101: loss did not improve from 0.09362\n",
      "23/23 [==============================] - 4s 185ms/step - loss: 0.0965 - accuracy: 0.9973 - val_loss: 0.1430 - val_accuracy: 0.9837\n",
      "Epoch 102/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1045 - accuracy: 0.9959\n",
      "Epoch 00102: loss did not improve from 0.09362\n",
      "23/23 [==============================] - 4s 187ms/step - loss: 0.1045 - accuracy: 0.9959 - val_loss: 0.1386 - val_accuracy: 0.9837\n",
      "Epoch 103/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1023 - accuracy: 0.9932\n",
      "Epoch 00103: loss did not improve from 0.09362\n",
      "23/23 [==============================] - 4s 186ms/step - loss: 0.1023 - accuracy: 0.9932 - val_loss: 0.1142 - val_accuracy: 0.9878\n",
      "Epoch 104/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0961 - accuracy: 0.9986\n",
      "Epoch 00104: loss did not improve from 0.09362\n",
      "23/23 [==============================] - 4s 186ms/step - loss: 0.0961 - accuracy: 0.9986 - val_loss: 0.1067 - val_accuracy: 0.9919\n",
      "Epoch 105/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1001 - accuracy: 0.9946\n",
      "Epoch 00105: loss did not improve from 0.09362\n",
      "23/23 [==============================] - 4s 185ms/step - loss: 0.1001 - accuracy: 0.9946 - val_loss: 0.1372 - val_accuracy: 0.9837\n",
      "Epoch 106/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1105 - accuracy: 0.9905\n",
      "Epoch 00106: loss did not improve from 0.09362\n",
      "23/23 [==============================] - 4s 191ms/step - loss: 0.1105 - accuracy: 0.9905 - val_loss: 0.1156 - val_accuracy: 0.9959\n",
      "Epoch 107/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1391 - accuracy: 0.9823\n",
      "Epoch 00107: loss did not improve from 0.09362\n",
      "23/23 [==============================] - 4s 185ms/step - loss: 0.1391 - accuracy: 0.9823 - val_loss: 0.2633 - val_accuracy: 0.9553\n",
      "Epoch 108/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1396 - accuracy: 0.9823\n",
      "Epoch 00108: loss did not improve from 0.09362\n",
      "23/23 [==============================] - 4s 187ms/step - loss: 0.1396 - accuracy: 0.9823 - val_loss: 0.1343 - val_accuracy: 0.9878\n",
      "Epoch 109/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1144 - accuracy: 0.9946\n",
      "Epoch 00109: loss did not improve from 0.09362\n",
      "23/23 [==============================] - 4s 185ms/step - loss: 0.1144 - accuracy: 0.9946 - val_loss: 0.1311 - val_accuracy: 0.9878\n",
      "Epoch 110/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0992 - accuracy: 0.9959\n",
      "Epoch 00110: loss did not improve from 0.09362\n",
      "23/23 [==============================] - 4s 189ms/step - loss: 0.0992 - accuracy: 0.9959 - val_loss: 0.1180 - val_accuracy: 0.9878\n",
      "Epoch 111/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0970 - accuracy: 0.9973\n",
      "Epoch 00111: loss did not improve from 0.09362\n",
      "23/23 [==============================] - 4s 188ms/step - loss: 0.0970 - accuracy: 0.9973 - val_loss: 0.1332 - val_accuracy: 0.9878\n",
      "Epoch 112/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0903 - accuracy: 1.0000\n",
      "Epoch 00112: loss improved from 0.09362 to 0.09032, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 4s 192ms/step - loss: 0.0903 - accuracy: 1.0000 - val_loss: 0.1073 - val_accuracy: 0.9959\n",
      "Epoch 113/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1013 - accuracy: 0.9932\n",
      "Epoch 00113: loss did not improve from 0.09032\n",
      "23/23 [==============================] - 5s 237ms/step - loss: 0.1013 - accuracy: 0.9932 - val_loss: 0.1197 - val_accuracy: 0.9837\n",
      "Epoch 114/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0905 - accuracy: 1.0000\n",
      "Epoch 00114: loss did not improve from 0.09032\n",
      "23/23 [==============================] - 4s 186ms/step - loss: 0.0905 - accuracy: 1.0000 - val_loss: 0.1906 - val_accuracy: 0.9756\n",
      "Epoch 115/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1084 - accuracy: 0.9973\n",
      "Epoch 00115: loss did not improve from 0.09032\n",
      "23/23 [==============================] - 5s 196ms/step - loss: 0.1084 - accuracy: 0.9973 - val_loss: 0.1379 - val_accuracy: 0.9797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0943 - accuracy: 0.9986\n",
      "Epoch 00116: loss did not improve from 0.09032\n",
      "23/23 [==============================] - 4s 190ms/step - loss: 0.0943 - accuracy: 0.9986 - val_loss: 0.1245 - val_accuracy: 0.9797\n",
      "Epoch 117/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1007 - accuracy: 0.9946\n",
      "Epoch 00117: loss did not improve from 0.09032\n",
      "23/23 [==============================] - 4s 187ms/step - loss: 0.1007 - accuracy: 0.9946 - val_loss: 0.1335 - val_accuracy: 0.9837\n",
      "Epoch 118/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0932 - accuracy: 0.9959\n",
      "Epoch 00118: loss did not improve from 0.09032\n",
      "23/23 [==============================] - 4s 186ms/step - loss: 0.0932 - accuracy: 0.9959 - val_loss: 0.1358 - val_accuracy: 0.9797\n",
      "Epoch 119/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0958 - accuracy: 0.9946\n",
      "Epoch 00119: loss did not improve from 0.09032\n",
      "23/23 [==============================] - 4s 186ms/step - loss: 0.0958 - accuracy: 0.9946 - val_loss: 0.1237 - val_accuracy: 0.9919\n",
      "Epoch 120/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0883 - accuracy: 0.9986\n",
      "Epoch 00120: loss improved from 0.09032 to 0.08827, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 5s 222ms/step - loss: 0.0883 - accuracy: 0.9986 - val_loss: 0.1041 - val_accuracy: 0.9878\n",
      "Epoch 121/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0820 - accuracy: 1.0000\n",
      "Epoch 00121: loss improved from 0.08827 to 0.08199, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 7s 286ms/step - loss: 0.0820 - accuracy: 1.0000 - val_loss: 0.1195 - val_accuracy: 0.9837\n",
      "Epoch 122/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0808 - accuracy: 0.9986\n",
      "Epoch 00122: loss improved from 0.08199 to 0.08084, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 5s 224ms/step - loss: 0.0808 - accuracy: 0.9986 - val_loss: 0.0937 - val_accuracy: 0.9919\n",
      "Epoch 123/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0756 - accuracy: 1.0000\n",
      "Epoch 00123: loss improved from 0.08084 to 0.07561, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 5s 239ms/step - loss: 0.0756 - accuracy: 1.0000 - val_loss: 0.0948 - val_accuracy: 0.9959\n",
      "Epoch 124/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0740 - accuracy: 1.0000\n",
      "Epoch 00124: loss improved from 0.07561 to 0.07400, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 6s 254ms/step - loss: 0.0740 - accuracy: 1.0000 - val_loss: 0.0946 - val_accuracy: 0.9878\n",
      "Epoch 125/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0717 - accuracy: 1.0000\n",
      "Epoch 00125: loss improved from 0.07400 to 0.07170, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 5s 233ms/step - loss: 0.0717 - accuracy: 1.0000 - val_loss: 0.0895 - val_accuracy: 0.9959\n",
      "Epoch 126/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0681 - accuracy: 1.0000\n",
      "Epoch 00126: loss improved from 0.07170 to 0.06812, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 5s 228ms/step - loss: 0.0681 - accuracy: 1.0000 - val_loss: 0.0824 - val_accuracy: 0.9959\n",
      "Epoch 127/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0662 - accuracy: 1.0000\n",
      "Epoch 00127: loss improved from 0.06812 to 0.06621, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 5s 229ms/step - loss: 0.0662 - accuracy: 1.0000 - val_loss: 0.0883 - val_accuracy: 0.9878\n",
      "Epoch 128/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0641 - accuracy: 1.0000\n",
      "Epoch 00128: loss improved from 0.06621 to 0.06408, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 5s 224ms/step - loss: 0.0641 - accuracy: 1.0000 - val_loss: 0.0869 - val_accuracy: 0.9878\n",
      "Epoch 129/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0626 - accuracy: 1.0000\n",
      "Epoch 00129: loss improved from 0.06408 to 0.06262, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 5s 227ms/step - loss: 0.0626 - accuracy: 1.0000 - val_loss: 0.0891 - val_accuracy: 0.9919\n",
      "Epoch 130/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0608 - accuracy: 1.0000\n",
      "Epoch 00130: loss improved from 0.06262 to 0.06081, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 5s 229ms/step - loss: 0.0608 - accuracy: 1.0000 - val_loss: 0.0769 - val_accuracy: 0.9959\n",
      "Epoch 131/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0616 - accuracy: 1.0000\n",
      "Epoch 00131: loss did not improve from 0.06081\n",
      "23/23 [==============================] - 5s 221ms/step - loss: 0.0616 - accuracy: 1.0000 - val_loss: 0.1301 - val_accuracy: 0.9837\n",
      "Epoch 132/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0595 - accuracy: 1.0000\n",
      "Epoch 00132: loss improved from 0.06081 to 0.05946, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 4s 190ms/step - loss: 0.0595 - accuracy: 1.0000 - val_loss: 0.0827 - val_accuracy: 0.9878\n",
      "Epoch 133/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0606 - accuracy: 0.9973\n",
      "Epoch 00133: loss did not improve from 0.05946\n",
      "23/23 [==============================] - 5s 222ms/step - loss: 0.0606 - accuracy: 0.9973 - val_loss: 0.0888 - val_accuracy: 0.9878\n",
      "Epoch 134/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0601 - accuracy: 1.0000\n",
      "Epoch 00134: loss did not improve from 0.05946\n",
      "23/23 [==============================] - 4s 184ms/step - loss: 0.0601 - accuracy: 1.0000 - val_loss: 0.0778 - val_accuracy: 0.9959\n",
      "Epoch 135/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0569 - accuracy: 1.0000\n",
      "Epoch 00135: loss improved from 0.05946 to 0.05695, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 4s 189ms/step - loss: 0.0569 - accuracy: 1.0000 - val_loss: 0.0915 - val_accuracy: 0.9837\n",
      "Epoch 136/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0747 - accuracy: 0.9932\n",
      "Epoch 00136: loss did not improve from 0.05695\n",
      "23/23 [==============================] - 5s 221ms/step - loss: 0.0747 - accuracy: 0.9932 - val_loss: 0.1019 - val_accuracy: 0.9878\n",
      "Epoch 137/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0891 - accuracy: 0.9905\n",
      "Epoch 00137: loss did not improve from 0.05695\n",
      "23/23 [==============================] - 4s 184ms/step - loss: 0.0891 - accuracy: 0.9905 - val_loss: 0.1151 - val_accuracy: 0.9878\n",
      "Epoch 138/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1368 - accuracy: 0.9796\n",
      "Epoch 00138: loss did not improve from 0.05695\n",
      "23/23 [==============================] - 4s 185ms/step - loss: 0.1368 - accuracy: 0.9796 - val_loss: 0.2049 - val_accuracy: 0.9472\n",
      "Epoch 139/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1221 - accuracy: 0.9864\n",
      "Epoch 00139: loss did not improve from 0.05695\n",
      "23/23 [==============================] - 4s 184ms/step - loss: 0.1221 - accuracy: 0.9864 - val_loss: 0.1724 - val_accuracy: 0.9593\n",
      "Epoch 140/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1203 - accuracy: 0.9918\n",
      "Epoch 00140: loss did not improve from 0.05695\n",
      "23/23 [==============================] - 4s 184ms/step - loss: 0.1203 - accuracy: 0.9918 - val_loss: 0.1087 - val_accuracy: 0.9878\n",
      "Epoch 141/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1250 - accuracy: 0.9850\n",
      "Epoch 00141: loss did not improve from 0.05695\n",
      "23/23 [==============================] - 4s 184ms/step - loss: 0.1250 - accuracy: 0.9850 - val_loss: 0.1411 - val_accuracy: 0.9837\n",
      "Epoch 142/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1438 - accuracy: 0.9796\n",
      "Epoch 00142: loss did not improve from 0.05695\n",
      "23/23 [==============================] - 4s 184ms/step - loss: 0.1438 - accuracy: 0.9796 - val_loss: 0.1466 - val_accuracy: 0.9878\n",
      "Epoch 143/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1160 - accuracy: 0.9891\n",
      "Epoch 00143: loss did not improve from 0.05695\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.1160 - accuracy: 0.9891 - val_loss: 0.1569 - val_accuracy: 0.9797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 144/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0958 - accuracy: 0.9986\n",
      "Epoch 00144: loss did not improve from 0.05695\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0958 - accuracy: 0.9986 - val_loss: 0.1405 - val_accuracy: 0.9756\n",
      "Epoch 145/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1009 - accuracy: 0.9946\n",
      "Epoch 00145: loss did not improve from 0.05695\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.1009 - accuracy: 0.9946 - val_loss: 0.1493 - val_accuracy: 0.9837\n",
      "Epoch 146/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1098 - accuracy: 0.9918\n",
      "Epoch 00146: loss did not improve from 0.05695\n",
      "23/23 [==============================] - 4s 188ms/step - loss: 0.1098 - accuracy: 0.9918 - val_loss: 0.1174 - val_accuracy: 0.9919\n",
      "Epoch 147/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0962 - accuracy: 0.9932\n",
      "Epoch 00147: loss did not improve from 0.05695\n",
      "23/23 [==============================] - 4s 190ms/step - loss: 0.0962 - accuracy: 0.9932 - val_loss: 0.1142 - val_accuracy: 0.9919\n",
      "Epoch 148/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0916 - accuracy: 0.9973\n",
      "Epoch 00148: loss did not improve from 0.05695\n",
      "23/23 [==============================] - 5s 230ms/step - loss: 0.0916 - accuracy: 0.9973 - val_loss: 0.1247 - val_accuracy: 0.9837\n",
      "Epoch 149/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0934 - accuracy: 0.9946\n",
      "Epoch 00149: loss did not improve from 0.05695\n",
      "23/23 [==============================] - 4s 185ms/step - loss: 0.0934 - accuracy: 0.9946 - val_loss: 0.1484 - val_accuracy: 0.9797\n",
      "Epoch 150/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0897 - accuracy: 0.9959\n",
      "Epoch 00150: loss did not improve from 0.05695\n",
      "23/23 [==============================] - 4s 184ms/step - loss: 0.0897 - accuracy: 0.9959 - val_loss: 0.1233 - val_accuracy: 0.9878\n",
      "Epoch 151/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0943 - accuracy: 0.9918\n",
      "Epoch 00151: loss did not improve from 0.05695\n",
      "23/23 [==============================] - 4s 184ms/step - loss: 0.0943 - accuracy: 0.9918 - val_loss: 0.1141 - val_accuracy: 0.9878\n",
      "Epoch 152/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0915 - accuracy: 0.9973\n",
      "Epoch 00152: loss did not improve from 0.05695\n",
      "23/23 [==============================] - 4s 184ms/step - loss: 0.0915 - accuracy: 0.9973 - val_loss: 0.1226 - val_accuracy: 0.9756\n",
      "Epoch 153/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0832 - accuracy: 0.9986\n",
      "Epoch 00153: loss did not improve from 0.05695\n",
      "23/23 [==============================] - 4s 184ms/step - loss: 0.0832 - accuracy: 0.9986 - val_loss: 0.1000 - val_accuracy: 0.9919\n",
      "Epoch 154/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0829 - accuracy: 0.9973\n",
      "Epoch 00154: loss did not improve from 0.05695\n",
      "23/23 [==============================] - 4s 186ms/step - loss: 0.0829 - accuracy: 0.9973 - val_loss: 0.0910 - val_accuracy: 1.0000\n",
      "Epoch 155/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0784 - accuracy: 0.9986\n",
      "Epoch 00155: loss did not improve from 0.05695\n",
      "23/23 [==============================] - 4s 184ms/step - loss: 0.0784 - accuracy: 0.9986 - val_loss: 0.0933 - val_accuracy: 0.9919\n",
      "Epoch 156/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0963 - accuracy: 0.9918\n",
      "Epoch 00156: loss did not improve from 0.05695\n",
      "23/23 [==============================] - 4s 184ms/step - loss: 0.0963 - accuracy: 0.9918 - val_loss: 0.1149 - val_accuracy: 0.9919\n",
      "Epoch 157/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1142 - accuracy: 0.9918\n",
      "Epoch 00157: loss did not improve from 0.05695\n",
      "23/23 [==============================] - 4s 185ms/step - loss: 0.1142 - accuracy: 0.9918 - val_loss: 0.1664 - val_accuracy: 0.9675\n",
      "Epoch 158/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1442 - accuracy: 0.9796\n",
      "Epoch 00158: loss did not improve from 0.05695\n",
      "23/23 [==============================] - 4s 184ms/step - loss: 0.1442 - accuracy: 0.9796 - val_loss: 0.1649 - val_accuracy: 0.9797\n",
      "Epoch 159/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0949 - accuracy: 0.9959\n",
      "Epoch 00159: loss did not improve from 0.05695\n",
      "23/23 [==============================] - 4s 184ms/step - loss: 0.0949 - accuracy: 0.9959 - val_loss: 0.1376 - val_accuracy: 0.9837\n",
      "Epoch 160/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1628 - accuracy: 0.9741\n",
      "Epoch 00160: loss did not improve from 0.05695\n",
      "23/23 [==============================] - 4s 184ms/step - loss: 0.1628 - accuracy: 0.9741 - val_loss: 0.2045 - val_accuracy: 0.9715\n",
      "Epoch 161/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1112 - accuracy: 0.9946\n",
      "Epoch 00161: loss did not improve from 0.05695\n",
      "23/23 [==============================] - 4s 185ms/step - loss: 0.1112 - accuracy: 0.9946 - val_loss: 0.1539 - val_accuracy: 0.9715\n",
      "Epoch 162/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0909 - accuracy: 0.9986\n",
      "Epoch 00162: loss did not improve from 0.05695\n",
      "23/23 [==============================] - 4s 184ms/step - loss: 0.0909 - accuracy: 0.9986 - val_loss: 0.1127 - val_accuracy: 0.9878\n",
      "Epoch 163/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1085 - accuracy: 0.9905\n",
      "Epoch 00163: loss did not improve from 0.05695\n",
      "23/23 [==============================] - 4s 184ms/step - loss: 0.1085 - accuracy: 0.9905 - val_loss: 0.2242 - val_accuracy: 0.9715\n",
      "Epoch 164/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1000 - accuracy: 0.9946\n",
      "Epoch 00164: loss did not improve from 0.05695\n",
      "23/23 [==============================] - 4s 184ms/step - loss: 0.1000 - accuracy: 0.9946 - val_loss: 0.1028 - val_accuracy: 1.0000\n",
      "Epoch 165/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0866 - accuracy: 0.9973\n",
      "Epoch 00165: loss did not improve from 0.05695\n",
      "23/23 [==============================] - 4s 185ms/step - loss: 0.0866 - accuracy: 0.9973 - val_loss: 0.1322 - val_accuracy: 0.9878\n",
      "Epoch 166/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0835 - accuracy: 0.9986\n",
      "Epoch 00166: loss did not improve from 0.05695\n",
      "23/23 [==============================] - 4s 185ms/step - loss: 0.0835 - accuracy: 0.9986 - val_loss: 0.0994 - val_accuracy: 0.9919\n",
      "Epoch 167/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0815 - accuracy: 0.9986\n",
      "Epoch 00167: loss did not improve from 0.05695\n",
      "23/23 [==============================] - 4s 185ms/step - loss: 0.0815 - accuracy: 0.9986 - val_loss: 0.0871 - val_accuracy: 0.9959\n",
      "Epoch 168/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0793 - accuracy: 0.9986\n",
      "Epoch 00168: loss did not improve from 0.05695\n",
      "23/23 [==============================] - 4s 184ms/step - loss: 0.0793 - accuracy: 0.9986 - val_loss: 0.2238 - val_accuracy: 0.9878\n",
      "Epoch 169/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0785 - accuracy: 0.9986\n",
      "Epoch 00169: loss did not improve from 0.05695\n",
      "23/23 [==============================] - 4s 188ms/step - loss: 0.0785 - accuracy: 0.9986 - val_loss: 0.1469 - val_accuracy: 0.9797\n",
      "Epoch 170/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0774 - accuracy: 0.9986\n",
      "Epoch 00170: loss did not improve from 0.05695\n",
      "23/23 [==============================] - 4s 186ms/step - loss: 0.0774 - accuracy: 0.9986 - val_loss: 0.1688 - val_accuracy: 0.9797\n",
      "Epoch 171/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0717 - accuracy: 1.0000\n",
      "Epoch 00171: loss did not improve from 0.05695\n",
      "23/23 [==============================] - 4s 185ms/step - loss: 0.0717 - accuracy: 1.0000 - val_loss: 0.1441 - val_accuracy: 0.9797\n",
      "Epoch 172/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0715 - accuracy: 1.0000\n",
      "Epoch 00172: loss did not improve from 0.05695\n",
      "23/23 [==============================] - 4s 186ms/step - loss: 0.0715 - accuracy: 1.0000 - val_loss: 0.1786 - val_accuracy: 0.9837\n",
      "Epoch 173/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0680 - accuracy: 1.0000\n",
      "Epoch 00173: loss did not improve from 0.05695\n",
      "23/23 [==============================] - 4s 186ms/step - loss: 0.0680 - accuracy: 1.0000 - val_loss: 0.1786 - val_accuracy: 0.9837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 174/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0665 - accuracy: 1.0000\n",
      "Epoch 00174: loss did not improve from 0.05695\n",
      "23/23 [==============================] - 5s 202ms/step - loss: 0.0665 - accuracy: 1.0000 - val_loss: 0.1693 - val_accuracy: 0.9837\n",
      "Epoch 175/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0640 - accuracy: 1.0000\n",
      "Epoch 00175: loss did not improve from 0.05695\n",
      "23/23 [==============================] - 5s 209ms/step - loss: 0.0640 - accuracy: 1.0000 - val_loss: 0.1299 - val_accuracy: 0.9837\n",
      "Epoch 176/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0674 - accuracy: 0.9959\n",
      "Epoch 00176: loss did not improve from 0.05695\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0674 - accuracy: 0.9959 - val_loss: 0.1048 - val_accuracy: 0.9837\n",
      "Epoch 177/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0648 - accuracy: 1.0000\n",
      "Epoch 00177: loss did not improve from 0.05695\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0648 - accuracy: 1.0000 - val_loss: 0.1155 - val_accuracy: 0.9837\n",
      "Epoch 178/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0626 - accuracy: 1.0000\n",
      "Epoch 00178: loss did not improve from 0.05695\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0626 - accuracy: 1.0000 - val_loss: 0.0803 - val_accuracy: 0.9919\n",
      "Epoch 179/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0618 - accuracy: 1.0000\n",
      "Epoch 00179: loss did not improve from 0.05695\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0618 - accuracy: 1.0000 - val_loss: 0.0764 - val_accuracy: 0.9959\n",
      "Epoch 180/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0596 - accuracy: 1.0000\n",
      "Epoch 00180: loss did not improve from 0.05695\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0596 - accuracy: 1.0000 - val_loss: 0.1185 - val_accuracy: 0.9797\n",
      "Epoch 181/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0573 - accuracy: 1.0000\n",
      "Epoch 00181: loss did not improve from 0.05695\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0573 - accuracy: 1.0000 - val_loss: 0.1185 - val_accuracy: 0.9837\n",
      "Epoch 182/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0564 - accuracy: 0.9986\n",
      "Epoch 00182: loss improved from 0.05695 to 0.05644, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 4s 187ms/step - loss: 0.0564 - accuracy: 0.9986 - val_loss: 0.1006 - val_accuracy: 0.9878\n",
      "Epoch 183/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0573 - accuracy: 0.9986\n",
      "Epoch 00183: loss did not improve from 0.05644\n",
      "23/23 [==============================] - 5s 226ms/step - loss: 0.0573 - accuracy: 0.9986 - val_loss: 0.1014 - val_accuracy: 0.9837\n",
      "Epoch 184/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0541 - accuracy: 1.0000\n",
      "Epoch 00184: loss improved from 0.05644 to 0.05415, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 4s 186ms/step - loss: 0.0541 - accuracy: 1.0000 - val_loss: 0.1343 - val_accuracy: 0.9797\n",
      "Epoch 185/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0618 - accuracy: 0.9973\n",
      "Epoch 00185: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 5s 223ms/step - loss: 0.0618 - accuracy: 0.9973 - val_loss: 0.1021 - val_accuracy: 0.9837\n",
      "Epoch 186/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0647 - accuracy: 0.9946\n",
      "Epoch 00186: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0647 - accuracy: 0.9946 - val_loss: 0.1230 - val_accuracy: 0.9715\n",
      "Epoch 187/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0685 - accuracy: 0.9946\n",
      "Epoch 00187: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.0685 - accuracy: 0.9946 - val_loss: 0.1134 - val_accuracy: 0.9715\n",
      "Epoch 188/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0696 - accuracy: 0.9946\n",
      "Epoch 00188: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.0696 - accuracy: 0.9946 - val_loss: 0.0965 - val_accuracy: 0.9837\n",
      "Epoch 189/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0768 - accuracy: 0.9959\n",
      "Epoch 00189: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0768 - accuracy: 0.9959 - val_loss: 0.1301 - val_accuracy: 0.9797\n",
      "Epoch 190/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0751 - accuracy: 0.9959\n",
      "Epoch 00190: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0751 - accuracy: 0.9959 - val_loss: 0.1562 - val_accuracy: 0.9756\n",
      "Epoch 191/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0708 - accuracy: 0.9959\n",
      "Epoch 00191: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.0708 - accuracy: 0.9959 - val_loss: 0.1610 - val_accuracy: 0.9797\n",
      "Epoch 192/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0770 - accuracy: 0.9946\n",
      "Epoch 00192: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.0770 - accuracy: 0.9946 - val_loss: 0.1247 - val_accuracy: 0.9878\n",
      "Epoch 193/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0833 - accuracy: 0.9918\n",
      "Epoch 00193: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0833 - accuracy: 0.9918 - val_loss: 0.1540 - val_accuracy: 0.9837\n",
      "Epoch 194/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0902 - accuracy: 0.9878\n",
      "Epoch 00194: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0902 - accuracy: 0.9878 - val_loss: 0.0956 - val_accuracy: 0.9837\n",
      "Epoch 195/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0884 - accuracy: 0.9905\n",
      "Epoch 00195: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.0884 - accuracy: 0.9905 - val_loss: 0.1071 - val_accuracy: 0.9837\n",
      "Epoch 196/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0760 - accuracy: 0.9973\n",
      "Epoch 00196: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.0760 - accuracy: 0.9973 - val_loss: 0.0909 - val_accuracy: 0.9919\n",
      "Epoch 197/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0756 - accuracy: 0.9959\n",
      "Epoch 00197: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0756 - accuracy: 0.9959 - val_loss: 0.1212 - val_accuracy: 0.9837\n",
      "Epoch 198/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0761 - accuracy: 0.9973\n",
      "Epoch 00198: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0761 - accuracy: 0.9973 - val_loss: 0.1074 - val_accuracy: 0.9756\n",
      "Epoch 199/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0747 - accuracy: 0.9946\n",
      "Epoch 00199: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.0747 - accuracy: 0.9946 - val_loss: 0.1108 - val_accuracy: 0.9756\n",
      "Epoch 200/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0764 - accuracy: 0.9946\n",
      "Epoch 00200: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.0764 - accuracy: 0.9946 - val_loss: 0.0912 - val_accuracy: 0.9837\n",
      "Epoch 201/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0662 - accuracy: 0.9986\n",
      "Epoch 00201: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 5s 215ms/step - loss: 0.0662 - accuracy: 0.9986 - val_loss: 0.1120 - val_accuracy: 0.9797\n",
      "Epoch 202/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0641 - accuracy: 1.0000\n",
      "Epoch 00202: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 5s 196ms/step - loss: 0.0641 - accuracy: 1.0000 - val_loss: 0.0891 - val_accuracy: 0.9837\n",
      "Epoch 203/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0627 - accuracy: 0.9986\n",
      "Epoch 00203: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0627 - accuracy: 0.9986 - val_loss: 0.1147 - val_accuracy: 0.9797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 204/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0606 - accuracy: 0.9986\n",
      "Epoch 00204: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0606 - accuracy: 0.9986 - val_loss: 0.1339 - val_accuracy: 0.9797\n",
      "Epoch 205/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0602 - accuracy: 1.0000\n",
      "Epoch 00205: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0602 - accuracy: 1.0000 - val_loss: 0.0913 - val_accuracy: 0.9878\n",
      "Epoch 206/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0584 - accuracy: 1.0000\n",
      "Epoch 00206: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0584 - accuracy: 1.0000 - val_loss: 0.1086 - val_accuracy: 0.9837\n",
      "Epoch 207/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0580 - accuracy: 0.9986\n",
      "Epoch 00207: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0580 - accuracy: 0.9986 - val_loss: 0.1154 - val_accuracy: 0.9797\n",
      "Epoch 208/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0561 - accuracy: 0.9986\n",
      "Epoch 00208: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0561 - accuracy: 0.9986 - val_loss: 0.0856 - val_accuracy: 0.9878\n",
      "Epoch 209/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0597 - accuracy: 0.9973\n",
      "Epoch 00209: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0597 - accuracy: 0.9973 - val_loss: 0.1038 - val_accuracy: 0.9797\n",
      "Epoch 210/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0683 - accuracy: 0.9932\n",
      "Epoch 00210: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0683 - accuracy: 0.9932 - val_loss: 0.0980 - val_accuracy: 0.9837\n",
      "Epoch 211/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0674 - accuracy: 0.9959\n",
      "Epoch 00211: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0674 - accuracy: 0.9959 - val_loss: 0.1131 - val_accuracy: 0.9715\n",
      "Epoch 212/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0640 - accuracy: 0.9986\n",
      "Epoch 00212: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 5s 200ms/step - loss: 0.0640 - accuracy: 0.9986 - val_loss: 0.0886 - val_accuracy: 0.9878\n",
      "Epoch 213/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0593 - accuracy: 0.9986\n",
      "Epoch 00213: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.0593 - accuracy: 0.9986 - val_loss: 0.0727 - val_accuracy: 0.9919\n",
      "Epoch 214/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0718 - accuracy: 0.9959\n",
      "Epoch 00214: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0718 - accuracy: 0.9959 - val_loss: 0.1757 - val_accuracy: 0.9634\n",
      "Epoch 215/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1211 - accuracy: 0.9837\n",
      "Epoch 00215: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.1211 - accuracy: 0.9837 - val_loss: 0.1709 - val_accuracy: 0.9715\n",
      "Epoch 216/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0877 - accuracy: 0.9918\n",
      "Epoch 00216: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0877 - accuracy: 0.9918 - val_loss: 0.2371 - val_accuracy: 0.9268\n",
      "Epoch 217/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1021 - accuracy: 0.9864\n",
      "Epoch 00217: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 4s 191ms/step - loss: 0.1021 - accuracy: 0.9864 - val_loss: 0.1079 - val_accuracy: 0.9837\n",
      "Epoch 218/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0813 - accuracy: 0.9946\n",
      "Epoch 00218: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.0813 - accuracy: 0.9946 - val_loss: 0.1469 - val_accuracy: 0.9837\n",
      "Epoch 219/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0742 - accuracy: 0.9959\n",
      "Epoch 00219: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0742 - accuracy: 0.9959 - val_loss: 0.1227 - val_accuracy: 0.9919\n",
      "Epoch 220/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0867 - accuracy: 0.9905\n",
      "Epoch 00220: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0867 - accuracy: 0.9905 - val_loss: 0.1252 - val_accuracy: 0.9797\n",
      "Epoch 221/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0706 - accuracy: 0.9986\n",
      "Epoch 00221: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.0706 - accuracy: 0.9986 - val_loss: 0.1028 - val_accuracy: 0.9837\n",
      "Epoch 222/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0672 - accuracy: 0.9986\n",
      "Epoch 00222: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0672 - accuracy: 0.9986 - val_loss: 0.0842 - val_accuracy: 0.9919\n",
      "Epoch 223/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0690 - accuracy: 0.9973\n",
      "Epoch 00223: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0690 - accuracy: 0.9973 - val_loss: 0.1610 - val_accuracy: 0.9715\n",
      "Epoch 224/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0829 - accuracy: 0.9918\n",
      "Epoch 00224: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0829 - accuracy: 0.9918 - val_loss: 0.1004 - val_accuracy: 0.9837\n",
      "Epoch 225/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0683 - accuracy: 0.9986\n",
      "Epoch 00225: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.0683 - accuracy: 0.9986 - val_loss: 0.0882 - val_accuracy: 0.9919\n",
      "Epoch 226/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0969 - accuracy: 0.9918\n",
      "Epoch 00226: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0969 - accuracy: 0.9918 - val_loss: 0.1087 - val_accuracy: 0.9797\n",
      "Epoch 227/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0869 - accuracy: 0.9918\n",
      "Epoch 00227: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.0869 - accuracy: 0.9918 - val_loss: 0.1264 - val_accuracy: 0.9756\n",
      "Epoch 228/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0797 - accuracy: 0.9946\n",
      "Epoch 00228: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.0797 - accuracy: 0.9946 - val_loss: 0.1179 - val_accuracy: 0.9837\n",
      "Epoch 229/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0676 - accuracy: 0.9986\n",
      "Epoch 00229: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 5s 206ms/step - loss: 0.0676 - accuracy: 0.9986 - val_loss: 0.1637 - val_accuracy: 0.9756\n",
      "Epoch 230/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0656 - accuracy: 0.9986\n",
      "Epoch 00230: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 4s 188ms/step - loss: 0.0656 - accuracy: 0.9986 - val_loss: 0.0990 - val_accuracy: 0.9878\n",
      "Epoch 231/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0723 - accuracy: 0.9946\n",
      "Epoch 00231: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 4s 185ms/step - loss: 0.0723 - accuracy: 0.9946 - val_loss: 0.1380 - val_accuracy: 0.9756\n",
      "Epoch 232/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0693 - accuracy: 0.9973\n",
      "Epoch 00232: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.0693 - accuracy: 0.9973 - val_loss: 0.1770 - val_accuracy: 0.9715\n",
      "Epoch 233/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0703 - accuracy: 0.9973\n",
      "Epoch 00233: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0703 - accuracy: 0.9973 - val_loss: 0.1036 - val_accuracy: 0.9837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 234/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0819 - accuracy: 0.9946\n",
      "Epoch 00234: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0819 - accuracy: 0.9946 - val_loss: 0.1006 - val_accuracy: 0.9797\n",
      "Epoch 235/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0714 - accuracy: 0.9959\n",
      "Epoch 00235: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 5s 196ms/step - loss: 0.0714 - accuracy: 0.9959 - val_loss: 0.1470 - val_accuracy: 0.9715\n",
      "Epoch 236/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0719 - accuracy: 0.9946\n",
      "Epoch 00236: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 4s 189ms/step - loss: 0.0719 - accuracy: 0.9946 - val_loss: 0.2159 - val_accuracy: 0.9593\n",
      "Epoch 237/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0994 - accuracy: 0.9864\n",
      "Epoch 00237: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 4s 192ms/step - loss: 0.0994 - accuracy: 0.9864 - val_loss: 0.0971 - val_accuracy: 0.9919\n",
      "Epoch 238/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0879 - accuracy: 0.9905\n",
      "Epoch 00238: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 4s 186ms/step - loss: 0.0879 - accuracy: 0.9905 - val_loss: 0.0988 - val_accuracy: 0.9878\n",
      "Epoch 239/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0870 - accuracy: 0.9932\n",
      "Epoch 00239: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 4s 187ms/step - loss: 0.0870 - accuracy: 0.9932 - val_loss: 0.1022 - val_accuracy: 0.9878\n",
      "Epoch 240/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0791 - accuracy: 0.9946\n",
      "Epoch 00240: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 4s 189ms/step - loss: 0.0791 - accuracy: 0.9946 - val_loss: 0.1149 - val_accuracy: 0.9756\n",
      "Epoch 241/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0706 - accuracy: 0.9986\n",
      "Epoch 00241: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 4s 195ms/step - loss: 0.0706 - accuracy: 0.9986 - val_loss: 0.0851 - val_accuracy: 0.9919\n",
      "Epoch 242/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0724 - accuracy: 0.9973\n",
      "Epoch 00242: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 5s 197ms/step - loss: 0.0724 - accuracy: 0.9973 - val_loss: 0.0813 - val_accuracy: 0.9959\n",
      "Epoch 243/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0699 - accuracy: 1.0000\n",
      "Epoch 00243: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 4s 187ms/step - loss: 0.0699 - accuracy: 1.0000 - val_loss: 0.1231 - val_accuracy: 0.9756\n",
      "Epoch 244/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0652 - accuracy: 0.9986\n",
      "Epoch 00244: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 4s 184ms/step - loss: 0.0652 - accuracy: 0.9986 - val_loss: 0.0760 - val_accuracy: 0.9919\n",
      "Epoch 245/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0636 - accuracy: 1.0000\n",
      "Epoch 00245: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 4s 186ms/step - loss: 0.0636 - accuracy: 1.0000 - val_loss: 0.0775 - val_accuracy: 0.9919\n",
      "Epoch 246/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0607 - accuracy: 1.0000\n",
      "Epoch 00246: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 4s 184ms/step - loss: 0.0607 - accuracy: 1.0000 - val_loss: 0.0651 - val_accuracy: 1.0000\n",
      "Epoch 247/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0589 - accuracy: 1.0000\n",
      "Epoch 00247: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 4s 185ms/step - loss: 0.0589 - accuracy: 1.0000 - val_loss: 0.0625 - val_accuracy: 1.0000\n",
      "Epoch 248/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0577 - accuracy: 1.0000\n",
      "Epoch 00248: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 4s 184ms/step - loss: 0.0577 - accuracy: 1.0000 - val_loss: 0.0633 - val_accuracy: 1.0000\n",
      "Epoch 249/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0595 - accuracy: 1.0000\n",
      "Epoch 00249: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 4s 184ms/step - loss: 0.0595 - accuracy: 1.0000 - val_loss: 0.0638 - val_accuracy: 1.0000\n",
      "Epoch 250/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0551 - accuracy: 1.0000\n",
      "Epoch 00250: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 4s 184ms/step - loss: 0.0551 - accuracy: 1.0000 - val_loss: 0.0596 - val_accuracy: 1.0000\n",
      "Epoch 251/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0544 - accuracy: 1.0000\n",
      "Epoch 00251: loss did not improve from 0.05415\n",
      "23/23 [==============================] - 4s 184ms/step - loss: 0.0544 - accuracy: 1.0000 - val_loss: 0.0615 - val_accuracy: 1.0000\n",
      "Epoch 252/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0531 - accuracy: 1.0000\n",
      "Epoch 00252: loss improved from 0.05415 to 0.05306, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 5s 196ms/step - loss: 0.0531 - accuracy: 1.0000 - val_loss: 0.0713 - val_accuracy: 0.9919\n",
      "Epoch 253/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0521 - accuracy: 1.0000\n",
      "Epoch 00253: loss improved from 0.05306 to 0.05212, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 5s 235ms/step - loss: 0.0521 - accuracy: 1.0000 - val_loss: 0.0650 - val_accuracy: 0.9959\n",
      "Epoch 254/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0515 - accuracy: 1.0000\n",
      "Epoch 00254: loss improved from 0.05212 to 0.05152, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 6s 239ms/step - loss: 0.0515 - accuracy: 1.0000 - val_loss: 0.0677 - val_accuracy: 0.9959\n",
      "Epoch 255/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0510 - accuracy: 1.0000\n",
      "Epoch 00255: loss improved from 0.05152 to 0.05096, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 6s 240ms/step - loss: 0.0510 - accuracy: 1.0000 - val_loss: 0.0534 - val_accuracy: 1.0000\n",
      "Epoch 256/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0549 - accuracy: 0.9973\n",
      "Epoch 00256: loss did not improve from 0.05096\n",
      "23/23 [==============================] - 5s 233ms/step - loss: 0.0549 - accuracy: 0.9973 - val_loss: 0.0753 - val_accuracy: 0.9878\n",
      "Epoch 257/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0483 - accuracy: 1.0000\n",
      "Epoch 00257: loss improved from 0.05096 to 0.04830, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 5s 198ms/step - loss: 0.0483 - accuracy: 1.0000 - val_loss: 0.0754 - val_accuracy: 0.9878\n",
      "Epoch 258/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0490 - accuracy: 1.0000\n",
      "Epoch 00258: loss did not improve from 0.04830\n",
      "23/23 [==============================] - 5s 222ms/step - loss: 0.0490 - accuracy: 1.0000 - val_loss: 0.1024 - val_accuracy: 0.9878\n",
      "Epoch 259/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0489 - accuracy: 0.9986\n",
      "Epoch 00259: loss did not improve from 0.04830\n",
      "23/23 [==============================] - 4s 184ms/step - loss: 0.0489 - accuracy: 0.9986 - val_loss: 0.0821 - val_accuracy: 0.9878\n",
      "Epoch 260/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0489 - accuracy: 1.0000\n",
      "Epoch 00260: loss did not improve from 0.04830\n",
      "23/23 [==============================] - 4s 186ms/step - loss: 0.0489 - accuracy: 1.0000 - val_loss: 0.0536 - val_accuracy: 1.0000\n",
      "Epoch 261/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0461 - accuracy: 1.0000\n",
      "Epoch 00261: loss improved from 0.04830 to 0.04612, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 4s 192ms/step - loss: 0.0461 - accuracy: 1.0000 - val_loss: 0.0512 - val_accuracy: 1.0000\n",
      "Epoch 262/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0443 - accuracy: 1.0000\n",
      "Epoch 00262: loss improved from 0.04612 to 0.04427, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 5s 228ms/step - loss: 0.0443 - accuracy: 1.0000 - val_loss: 0.0590 - val_accuracy: 0.9959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 263/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0431 - accuracy: 1.0000\n",
      "Epoch 00263: loss improved from 0.04427 to 0.04315, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 5s 223ms/step - loss: 0.0431 - accuracy: 1.0000 - val_loss: 0.0729 - val_accuracy: 0.9878\n",
      "Epoch 264/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0467 - accuracy: 0.9973\n",
      "Epoch 00264: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 5s 219ms/step - loss: 0.0467 - accuracy: 0.9973 - val_loss: 0.0705 - val_accuracy: 0.9837\n",
      "Epoch 265/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0552 - accuracy: 0.9973\n",
      "Epoch 00265: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0552 - accuracy: 0.9973 - val_loss: 0.1080 - val_accuracy: 0.9756\n",
      "Epoch 266/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0601 - accuracy: 0.9973\n",
      "Epoch 00266: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 4s 184ms/step - loss: 0.0601 - accuracy: 0.9973 - val_loss: 0.0974 - val_accuracy: 0.9797\n",
      "Epoch 267/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0755 - accuracy: 0.9891\n",
      "Epoch 00267: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0755 - accuracy: 0.9891 - val_loss: 0.1114 - val_accuracy: 0.9797\n",
      "Epoch 268/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0884 - accuracy: 0.9878\n",
      "Epoch 00268: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0884 - accuracy: 0.9878 - val_loss: 0.1207 - val_accuracy: 0.9715\n",
      "Epoch 269/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0688 - accuracy: 0.9946\n",
      "Epoch 00269: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0688 - accuracy: 0.9946 - val_loss: 0.1135 - val_accuracy: 0.9756\n",
      "Epoch 270/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0666 - accuracy: 0.9959\n",
      "Epoch 00270: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0666 - accuracy: 0.9959 - val_loss: 0.0876 - val_accuracy: 0.9878\n",
      "Epoch 271/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0654 - accuracy: 0.9959\n",
      "Epoch 00271: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0654 - accuracy: 0.9959 - val_loss: 0.1677 - val_accuracy: 0.9797\n",
      "Epoch 272/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0920 - accuracy: 0.9850\n",
      "Epoch 00272: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0920 - accuracy: 0.9850 - val_loss: 0.1695 - val_accuracy: 0.9472\n",
      "Epoch 273/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0835 - accuracy: 0.9918\n",
      "Epoch 00273: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0835 - accuracy: 0.9918 - val_loss: 0.1144 - val_accuracy: 0.9715\n",
      "Epoch 274/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0826 - accuracy: 0.9932\n",
      "Epoch 00274: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0826 - accuracy: 0.9932 - val_loss: 0.1021 - val_accuracy: 0.9878\n",
      "Epoch 275/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0626 - accuracy: 1.0000\n",
      "Epoch 00275: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.0626 - accuracy: 1.0000 - val_loss: 0.1011 - val_accuracy: 0.9797\n",
      "Epoch 276/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0711 - accuracy: 0.9959\n",
      "Epoch 00276: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0711 - accuracy: 0.9959 - val_loss: 0.0817 - val_accuracy: 0.9878\n",
      "Epoch 277/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0923 - accuracy: 0.9905\n",
      "Epoch 00277: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0923 - accuracy: 0.9905 - val_loss: 0.1206 - val_accuracy: 0.9756\n",
      "Epoch 278/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0914 - accuracy: 0.9891\n",
      "Epoch 00278: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0914 - accuracy: 0.9891 - val_loss: 0.1545 - val_accuracy: 0.9634\n",
      "Epoch 279/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0781 - accuracy: 0.9932\n",
      "Epoch 00279: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0781 - accuracy: 0.9932 - val_loss: 0.0967 - val_accuracy: 0.9878\n",
      "Epoch 280/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0773 - accuracy: 0.9918\n",
      "Epoch 00280: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 5s 223ms/step - loss: 0.0773 - accuracy: 0.9918 - val_loss: 0.1079 - val_accuracy: 0.9837\n",
      "Epoch 281/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0712 - accuracy: 0.9973\n",
      "Epoch 00281: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 4s 193ms/step - loss: 0.0712 - accuracy: 0.9973 - val_loss: 0.1000 - val_accuracy: 0.9797\n",
      "Epoch 282/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0788 - accuracy: 0.9959\n",
      "Epoch 00282: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0788 - accuracy: 0.9959 - val_loss: 0.0924 - val_accuracy: 0.9878\n",
      "Epoch 283/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0671 - accuracy: 0.9986\n",
      "Epoch 00283: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0671 - accuracy: 0.9986 - val_loss: 0.1115 - val_accuracy: 0.9837\n",
      "Epoch 284/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0640 - accuracy: 1.0000\n",
      "Epoch 00284: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0640 - accuracy: 1.0000 - val_loss: 0.0886 - val_accuracy: 0.9837\n",
      "Epoch 285/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0592 - accuracy: 1.0000\n",
      "Epoch 00285: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.0592 - accuracy: 1.0000 - val_loss: 0.1004 - val_accuracy: 0.9878\n",
      "Epoch 286/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0564 - accuracy: 1.0000\n",
      "Epoch 00286: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0564 - accuracy: 1.0000 - val_loss: 0.0878 - val_accuracy: 0.9837\n",
      "Epoch 287/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0554 - accuracy: 1.0000\n",
      "Epoch 00287: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0554 - accuracy: 1.0000 - val_loss: 0.0823 - val_accuracy: 0.9878\n",
      "Epoch 288/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0538 - accuracy: 1.0000\n",
      "Epoch 00288: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0538 - accuracy: 1.0000 - val_loss: 0.0753 - val_accuracy: 0.9878\n",
      "Epoch 289/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0525 - accuracy: 1.0000\n",
      "Epoch 00289: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0525 - accuracy: 1.0000 - val_loss: 0.0772 - val_accuracy: 0.9837\n",
      "Epoch 290/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0537 - accuracy: 0.9986\n",
      "Epoch 00290: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.0537 - accuracy: 0.9986 - val_loss: 0.1161 - val_accuracy: 0.9756\n",
      "Epoch 291/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0548 - accuracy: 0.9973\n",
      "Epoch 00291: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0548 - accuracy: 0.9973 - val_loss: 0.1174 - val_accuracy: 0.9715\n",
      "Epoch 292/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0560 - accuracy: 0.9959\n",
      "Epoch 00292: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0560 - accuracy: 0.9959 - val_loss: 0.0769 - val_accuracy: 0.9878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 293/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0574 - accuracy: 0.9973\n",
      "Epoch 00293: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0574 - accuracy: 0.9973 - val_loss: 0.0934 - val_accuracy: 0.9756\n",
      "Epoch 294/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0532 - accuracy: 0.9986\n",
      "Epoch 00294: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0532 - accuracy: 0.9986 - val_loss: 0.0793 - val_accuracy: 0.9837\n",
      "Epoch 295/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0512 - accuracy: 1.0000\n",
      "Epoch 00295: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0512 - accuracy: 1.0000 - val_loss: 0.1003 - val_accuracy: 0.9756\n",
      "Epoch 296/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0899 - accuracy: 0.9905\n",
      "Epoch 00296: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0899 - accuracy: 0.9905 - val_loss: 0.1049 - val_accuracy: 0.9878\n",
      "Epoch 297/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1089 - accuracy: 0.9837\n",
      "Epoch 00297: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.1089 - accuracy: 0.9837 - val_loss: 0.1561 - val_accuracy: 0.9715\n",
      "Epoch 298/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0813 - accuracy: 0.9918\n",
      "Epoch 00298: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0813 - accuracy: 0.9918 - val_loss: 0.1781 - val_accuracy: 0.9837\n",
      "Epoch 299/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0988 - accuracy: 0.9905\n",
      "Epoch 00299: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0988 - accuracy: 0.9905 - val_loss: 0.0971 - val_accuracy: 0.9878\n",
      "Epoch 300/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0900 - accuracy: 0.9918\n",
      "Epoch 00300: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0900 - accuracy: 0.9918 - val_loss: 0.1069 - val_accuracy: 0.9837\n",
      "Epoch 301/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0601 - accuracy: 1.0000\n",
      "Epoch 00301: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0601 - accuracy: 1.0000 - val_loss: 0.0917 - val_accuracy: 0.9837\n",
      "Epoch 302/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0612 - accuracy: 0.9986\n",
      "Epoch 00302: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 4s 187ms/step - loss: 0.0612 - accuracy: 0.9986 - val_loss: 0.1001 - val_accuracy: 0.9837\n",
      "Epoch 303/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0593 - accuracy: 1.0000\n",
      "Epoch 00303: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0593 - accuracy: 1.0000 - val_loss: 0.1432 - val_accuracy: 0.9797\n",
      "Epoch 304/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0578 - accuracy: 0.9986\n",
      "Epoch 00304: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0578 - accuracy: 0.9986 - val_loss: 0.1172 - val_accuracy: 0.9837\n",
      "Epoch 305/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0552 - accuracy: 1.0000\n",
      "Epoch 00305: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 4s 188ms/step - loss: 0.0552 - accuracy: 1.0000 - val_loss: 0.1097 - val_accuracy: 0.9837\n",
      "Epoch 306/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0557 - accuracy: 0.9986\n",
      "Epoch 00306: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 4s 185ms/step - loss: 0.0557 - accuracy: 0.9986 - val_loss: 0.1276 - val_accuracy: 0.9837\n",
      "Epoch 307/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0611 - accuracy: 0.9973\n",
      "Epoch 00307: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 5s 215ms/step - loss: 0.0611 - accuracy: 0.9973 - val_loss: 0.0794 - val_accuracy: 0.9919\n",
      "Epoch 308/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0554 - accuracy: 1.0000\n",
      "Epoch 00308: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 5s 201ms/step - loss: 0.0554 - accuracy: 1.0000 - val_loss: 0.1078 - val_accuracy: 0.9837\n",
      "Epoch 309/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0538 - accuracy: 0.9986\n",
      "Epoch 00309: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.0538 - accuracy: 0.9986 - val_loss: 0.1161 - val_accuracy: 0.9878\n",
      "Epoch 310/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0539 - accuracy: 0.9986\n",
      "Epoch 00310: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0539 - accuracy: 0.9986 - val_loss: 0.1288 - val_accuracy: 0.9756\n",
      "Epoch 311/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0536 - accuracy: 0.9986\n",
      "Epoch 00311: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0536 - accuracy: 0.9986 - val_loss: 0.1076 - val_accuracy: 0.9756\n",
      "Epoch 312/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0520 - accuracy: 0.9986\n",
      "Epoch 00312: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0520 - accuracy: 0.9986 - val_loss: 0.1309 - val_accuracy: 0.9756\n",
      "Epoch 313/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0497 - accuracy: 1.0000\n",
      "Epoch 00313: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0497 - accuracy: 1.0000 - val_loss: 0.1082 - val_accuracy: 0.9837\n",
      "Epoch 314/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0478 - accuracy: 1.0000\n",
      "Epoch 00314: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0478 - accuracy: 1.0000 - val_loss: 0.1084 - val_accuracy: 0.9878\n",
      "Epoch 315/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0471 - accuracy: 0.9986\n",
      "Epoch 00315: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0471 - accuracy: 0.9986 - val_loss: 0.1206 - val_accuracy: 0.9797\n",
      "Epoch 316/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0487 - accuracy: 0.9986\n",
      "Epoch 00316: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.0487 - accuracy: 0.9986 - val_loss: 0.1001 - val_accuracy: 0.9837\n",
      "Epoch 317/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0601 - accuracy: 0.9959\n",
      "Epoch 00317: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0601 - accuracy: 0.9959 - val_loss: 0.1029 - val_accuracy: 0.9756\n",
      "Epoch 318/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0528 - accuracy: 0.9986\n",
      "Epoch 00318: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 4s 185ms/step - loss: 0.0528 - accuracy: 0.9986 - val_loss: 0.1165 - val_accuracy: 0.9878\n",
      "Epoch 319/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0513 - accuracy: 0.9986\n",
      "Epoch 00319: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0513 - accuracy: 0.9986 - val_loss: 0.1197 - val_accuracy: 0.9797\n",
      "Epoch 320/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0977 - accuracy: 0.9850\n",
      "Epoch 00320: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.0977 - accuracy: 0.9850 - val_loss: 0.1230 - val_accuracy: 0.9675\n",
      "Epoch 321/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0642 - accuracy: 0.9946\n",
      "Epoch 00321: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.0642 - accuracy: 0.9946 - val_loss: 0.1067 - val_accuracy: 0.9675\n",
      "Epoch 322/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0523 - accuracy: 1.0000\n",
      "Epoch 00322: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0523 - accuracy: 1.0000 - val_loss: 0.0969 - val_accuracy: 0.9797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 323/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0508 - accuracy: 1.0000\n",
      "Epoch 00323: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0508 - accuracy: 1.0000 - val_loss: 0.1056 - val_accuracy: 0.9797\n",
      "Epoch 324/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0489 - accuracy: 1.0000\n",
      "Epoch 00324: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0489 - accuracy: 1.0000 - val_loss: 0.1112 - val_accuracy: 0.9715\n",
      "Epoch 325/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0473 - accuracy: 1.0000\n",
      "Epoch 00325: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0473 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9756\n",
      "Epoch 326/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0465 - accuracy: 1.0000\n",
      "Epoch 00326: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0465 - accuracy: 1.0000 - val_loss: 0.0908 - val_accuracy: 0.9837\n",
      "Epoch 327/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0462 - accuracy: 1.0000\n",
      "Epoch 00327: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0462 - accuracy: 1.0000 - val_loss: 0.0923 - val_accuracy: 0.9797\n",
      "Epoch 328/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0451 - accuracy: 1.0000\n",
      "Epoch 00328: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0451 - accuracy: 1.0000 - val_loss: 0.0854 - val_accuracy: 0.9756\n",
      "Epoch 329/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0458 - accuracy: 1.0000\n",
      "Epoch 00329: loss did not improve from 0.04315\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0458 - accuracy: 1.0000 - val_loss: 0.0641 - val_accuracy: 0.9919\n",
      "Epoch 330/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0429 - accuracy: 1.0000\n",
      "Epoch 00330: loss improved from 0.04315 to 0.04290, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 5s 202ms/step - loss: 0.0429 - accuracy: 1.0000 - val_loss: 0.0921 - val_accuracy: 0.9837\n",
      "Epoch 331/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0422 - accuracy: 1.0000\n",
      "Epoch 00331: loss improved from 0.04290 to 0.04216, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 5s 228ms/step - loss: 0.0422 - accuracy: 1.0000 - val_loss: 0.0919 - val_accuracy: 0.9837\n",
      "Epoch 332/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0430 - accuracy: 0.9986\n",
      "Epoch 00332: loss did not improve from 0.04216\n",
      "23/23 [==============================] - 5s 219ms/step - loss: 0.0430 - accuracy: 0.9986 - val_loss: 0.1302 - val_accuracy: 0.9715\n",
      "Epoch 333/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0658 - accuracy: 0.9918\n",
      "Epoch 00333: loss did not improve from 0.04216\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0658 - accuracy: 0.9918 - val_loss: 0.1251 - val_accuracy: 0.9675\n",
      "Epoch 334/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0909 - accuracy: 0.9864\n",
      "Epoch 00334: loss did not improve from 0.04216\n",
      "23/23 [==============================] - 5s 225ms/step - loss: 0.0909 - accuracy: 0.9864 - val_loss: 0.1168 - val_accuracy: 0.9675\n",
      "Epoch 335/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0692 - accuracy: 0.9918\n",
      "Epoch 00335: loss did not improve from 0.04216\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0692 - accuracy: 0.9918 - val_loss: 0.1229 - val_accuracy: 0.9715\n",
      "Epoch 336/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0560 - accuracy: 0.9973\n",
      "Epoch 00336: loss did not improve from 0.04216\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0560 - accuracy: 0.9973 - val_loss: 0.1109 - val_accuracy: 0.9756\n",
      "Epoch 337/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0749 - accuracy: 0.9946\n",
      "Epoch 00337: loss did not improve from 0.04216\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0749 - accuracy: 0.9946 - val_loss: 0.1530 - val_accuracy: 0.9715\n",
      "Epoch 338/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0627 - accuracy: 0.9946\n",
      "Epoch 00338: loss did not improve from 0.04216\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0627 - accuracy: 0.9946 - val_loss: 0.1362 - val_accuracy: 0.9756\n",
      "Epoch 339/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0849 - accuracy: 0.9878\n",
      "Epoch 00339: loss did not improve from 0.04216\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.0849 - accuracy: 0.9878 - val_loss: 0.1288 - val_accuracy: 0.9675\n",
      "Epoch 340/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0922 - accuracy: 0.9864\n",
      "Epoch 00340: loss did not improve from 0.04216\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.0922 - accuracy: 0.9864 - val_loss: 0.1205 - val_accuracy: 0.9675\n",
      "Epoch 341/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0648 - accuracy: 0.9959\n",
      "Epoch 00341: loss did not improve from 0.04216\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0648 - accuracy: 0.9959 - val_loss: 0.1008 - val_accuracy: 0.9837\n",
      "Epoch 342/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0703 - accuracy: 0.9932\n",
      "Epoch 00342: loss did not improve from 0.04216\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0703 - accuracy: 0.9932 - val_loss: 0.0950 - val_accuracy: 0.9756\n",
      "Epoch 343/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0734 - accuracy: 0.9986\n",
      "Epoch 00343: loss did not improve from 0.04216\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0734 - accuracy: 0.9986 - val_loss: 0.0730 - val_accuracy: 0.9919\n",
      "Epoch 344/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0850 - accuracy: 0.9918\n",
      "Epoch 00344: loss did not improve from 0.04216\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.0850 - accuracy: 0.9918 - val_loss: 0.0998 - val_accuracy: 0.9878\n",
      "Epoch 345/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0632 - accuracy: 0.9973\n",
      "Epoch 00345: loss did not improve from 0.04216\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0632 - accuracy: 0.9973 - val_loss: 0.0823 - val_accuracy: 0.9878\n",
      "Epoch 346/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0566 - accuracy: 0.9973\n",
      "Epoch 00346: loss did not improve from 0.04216\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0566 - accuracy: 0.9973 - val_loss: 0.1423 - val_accuracy: 0.9675\n",
      "Epoch 347/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0560 - accuracy: 0.9986\n",
      "Epoch 00347: loss did not improve from 0.04216\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0560 - accuracy: 0.9986 - val_loss: 0.1218 - val_accuracy: 0.9756\n",
      "Epoch 348/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0541 - accuracy: 1.0000\n",
      "Epoch 00348: loss did not improve from 0.04216\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0541 - accuracy: 1.0000 - val_loss: 0.0936 - val_accuracy: 0.9837\n",
      "Epoch 349/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0533 - accuracy: 1.0000\n",
      "Epoch 00349: loss did not improve from 0.04216\n",
      "23/23 [==============================] - 4s 192ms/step - loss: 0.0533 - accuracy: 1.0000 - val_loss: 0.0800 - val_accuracy: 0.9837\n",
      "Epoch 350/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0524 - accuracy: 1.0000\n",
      "Epoch 00350: loss did not improve from 0.04216\n",
      "23/23 [==============================] - 5s 199ms/step - loss: 0.0524 - accuracy: 1.0000 - val_loss: 0.1010 - val_accuracy: 0.9837\n",
      "Epoch 351/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0521 - accuracy: 0.9986\n",
      "Epoch 00351: loss did not improve from 0.04216\n",
      "23/23 [==============================] - 5s 209ms/step - loss: 0.0521 - accuracy: 0.9986 - val_loss: 0.0950 - val_accuracy: 0.9837\n",
      "Epoch 352/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0498 - accuracy: 1.0000\n",
      "Epoch 00352: loss did not improve from 0.04216\n",
      "23/23 [==============================] - 4s 188ms/step - loss: 0.0498 - accuracy: 1.0000 - val_loss: 0.1351 - val_accuracy: 0.9837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 353/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0510 - accuracy: 0.9986\n",
      "Epoch 00353: loss did not improve from 0.04216\n",
      "23/23 [==============================] - 4s 185ms/step - loss: 0.0510 - accuracy: 0.9986 - val_loss: 0.1485 - val_accuracy: 0.9797\n",
      "Epoch 354/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0532 - accuracy: 0.9986\n",
      "Epoch 00354: loss did not improve from 0.04216\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.0532 - accuracy: 0.9986 - val_loss: 0.1456 - val_accuracy: 0.9756\n",
      "Epoch 355/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0501 - accuracy: 1.0000\n",
      "Epoch 00355: loss did not improve from 0.04216\n",
      "23/23 [==============================] - 4s 185ms/step - loss: 0.0501 - accuracy: 1.0000 - val_loss: 0.1661 - val_accuracy: 0.9797\n",
      "Epoch 356/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0467 - accuracy: 1.0000\n",
      "Epoch 00356: loss did not improve from 0.04216\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.0467 - accuracy: 1.0000 - val_loss: 0.1119 - val_accuracy: 0.9837\n",
      "Epoch 357/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0464 - accuracy: 1.0000\n",
      "Epoch 00357: loss did not improve from 0.04216\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.0464 - accuracy: 1.0000 - val_loss: 0.1582 - val_accuracy: 0.9756\n",
      "Epoch 358/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0471 - accuracy: 1.0000\n",
      "Epoch 00358: loss did not improve from 0.04216\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0471 - accuracy: 1.0000 - val_loss: 0.1297 - val_accuracy: 0.9797\n",
      "Epoch 359/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0446 - accuracy: 1.0000\n",
      "Epoch 00359: loss did not improve from 0.04216\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0446 - accuracy: 1.0000 - val_loss: 0.1053 - val_accuracy: 0.9797\n",
      "Epoch 360/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0434 - accuracy: 1.0000\n",
      "Epoch 00360: loss did not improve from 0.04216\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0434 - accuracy: 1.0000 - val_loss: 0.1081 - val_accuracy: 0.9756\n",
      "Epoch 361/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0441 - accuracy: 1.0000\n",
      "Epoch 00361: loss did not improve from 0.04216\n",
      "23/23 [==============================] - 5s 230ms/step - loss: 0.0441 - accuracy: 1.0000 - val_loss: 0.0921 - val_accuracy: 0.9878\n",
      "Epoch 362/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0755 - accuracy: 0.9932\n",
      "Epoch 00362: loss did not improve from 0.04216\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0755 - accuracy: 0.9932 - val_loss: 0.1058 - val_accuracy: 0.9756\n",
      "Epoch 363/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0536 - accuracy: 0.9986\n",
      "Epoch 00363: loss did not improve from 0.04216\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0536 - accuracy: 0.9986 - val_loss: 0.0864 - val_accuracy: 0.9878\n",
      "Epoch 364/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0572 - accuracy: 0.9973\n",
      "Epoch 00364: loss did not improve from 0.04216\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0572 - accuracy: 0.9973 - val_loss: 0.0728 - val_accuracy: 0.9878\n",
      "Epoch 365/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0509 - accuracy: 0.9973\n",
      "Epoch 00365: loss did not improve from 0.04216\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0509 - accuracy: 0.9973 - val_loss: 0.2053 - val_accuracy: 0.9472\n",
      "Epoch 366/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0873 - accuracy: 0.9837\n",
      "Epoch 00366: loss did not improve from 0.04216\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0873 - accuracy: 0.9837 - val_loss: 0.0829 - val_accuracy: 0.9878\n",
      "Epoch 367/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0682 - accuracy: 0.9905\n",
      "Epoch 00367: loss did not improve from 0.04216\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.0682 - accuracy: 0.9905 - val_loss: 0.0802 - val_accuracy: 0.9878\n",
      "Epoch 368/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0612 - accuracy: 0.9932\n",
      "Epoch 00368: loss did not improve from 0.04216\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0612 - accuracy: 0.9932 - val_loss: 0.1025 - val_accuracy: 0.9837\n",
      "Epoch 369/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0523 - accuracy: 0.9986\n",
      "Epoch 00369: loss did not improve from 0.04216\n",
      "23/23 [==============================] - 4s 187ms/step - loss: 0.0523 - accuracy: 0.9986 - val_loss: 0.0899 - val_accuracy: 0.9837\n",
      "Epoch 370/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0482 - accuracy: 1.0000\n",
      "Epoch 00370: loss did not improve from 0.04216\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0482 - accuracy: 1.0000 - val_loss: 0.0604 - val_accuracy: 0.9959\n",
      "Epoch 371/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0472 - accuracy: 1.0000\n",
      "Epoch 00371: loss did not improve from 0.04216\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0472 - accuracy: 1.0000 - val_loss: 0.0575 - val_accuracy: 0.9959\n",
      "Epoch 372/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0486 - accuracy: 0.9986\n",
      "Epoch 00372: loss did not improve from 0.04216\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0486 - accuracy: 0.9986 - val_loss: 0.0601 - val_accuracy: 0.9919\n",
      "Epoch 373/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0566 - accuracy: 0.9959\n",
      "Epoch 00373: loss did not improve from 0.04216\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.0566 - accuracy: 0.9959 - val_loss: 0.0768 - val_accuracy: 0.9919\n",
      "Epoch 374/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0610 - accuracy: 0.9932\n",
      "Epoch 00374: loss did not improve from 0.04216\n",
      "23/23 [==============================] - 4s 184ms/step - loss: 0.0610 - accuracy: 0.9932 - val_loss: 0.0864 - val_accuracy: 0.9756\n",
      "Epoch 375/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0550 - accuracy: 0.9973\n",
      "Epoch 00375: loss did not improve from 0.04216\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.0550 - accuracy: 0.9973 - val_loss: 0.1251 - val_accuracy: 0.9756\n",
      "Epoch 376/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0534 - accuracy: 0.9986\n",
      "Epoch 00376: loss did not improve from 0.04216\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0534 - accuracy: 0.9986 - val_loss: 0.1089 - val_accuracy: 0.9797\n",
      "Epoch 377/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0511 - accuracy: 0.9973\n",
      "Epoch 00377: loss did not improve from 0.04216\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0511 - accuracy: 0.9973 - val_loss: 0.0952 - val_accuracy: 0.9797\n",
      "Epoch 378/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0554 - accuracy: 0.9946\n",
      "Epoch 00378: loss did not improve from 0.04216\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0554 - accuracy: 0.9946 - val_loss: 0.0702 - val_accuracy: 0.9919\n",
      "Epoch 379/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0553 - accuracy: 0.9973\n",
      "Epoch 00379: loss did not improve from 0.04216\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0553 - accuracy: 0.9973 - val_loss: 0.0990 - val_accuracy: 0.9837\n",
      "Epoch 380/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0658 - accuracy: 0.9946\n",
      "Epoch 00380: loss did not improve from 0.04216\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.0658 - accuracy: 0.9946 - val_loss: 0.1619 - val_accuracy: 0.9715\n",
      "Epoch 381/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0769 - accuracy: 0.9864\n",
      "Epoch 00381: loss did not improve from 0.04216\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.0769 - accuracy: 0.9864 - val_loss: 0.1386 - val_accuracy: 0.9675\n",
      "Epoch 382/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0677 - accuracy: 0.9918\n",
      "Epoch 00382: loss did not improve from 0.04216\n",
      "23/23 [==============================] - 4s 185ms/step - loss: 0.0677 - accuracy: 0.9918 - val_loss: 0.1325 - val_accuracy: 0.9715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 383/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0514 - accuracy: 1.0000\n",
      "Epoch 00383: loss did not improve from 0.04216\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0514 - accuracy: 1.0000 - val_loss: 0.1144 - val_accuracy: 0.9878\n",
      "Epoch 384/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0521 - accuracy: 0.9986\n",
      "Epoch 00384: loss did not improve from 0.04216\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0521 - accuracy: 0.9986 - val_loss: 0.1218 - val_accuracy: 0.9837\n",
      "Epoch 385/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0620 - accuracy: 0.9932\n",
      "Epoch 00385: loss did not improve from 0.04216\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0620 - accuracy: 0.9932 - val_loss: 0.0880 - val_accuracy: 0.9878\n",
      "Epoch 386/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0542 - accuracy: 0.9986\n",
      "Epoch 00386: loss did not improve from 0.04216\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0542 - accuracy: 0.9986 - val_loss: 0.1451 - val_accuracy: 0.9675\n",
      "Epoch 387/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0531 - accuracy: 0.9959\n",
      "Epoch 00387: loss did not improve from 0.04216\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0531 - accuracy: 0.9959 - val_loss: 0.0939 - val_accuracy: 0.9878\n",
      "Epoch 388/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0480 - accuracy: 1.0000\n",
      "Epoch 00388: loss did not improve from 0.04216\n",
      "23/23 [==============================] - 5s 226ms/step - loss: 0.0480 - accuracy: 1.0000 - val_loss: 0.1176 - val_accuracy: 0.9756\n",
      "Epoch 389/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0470 - accuracy: 1.0000\n",
      "Epoch 00389: loss did not improve from 0.04216\n",
      "23/23 [==============================] - 4s 185ms/step - loss: 0.0470 - accuracy: 1.0000 - val_loss: 0.1284 - val_accuracy: 0.9675\n",
      "Epoch 390/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0496 - accuracy: 0.9959\n",
      "Epoch 00390: loss did not improve from 0.04216\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0496 - accuracy: 0.9959 - val_loss: 0.0972 - val_accuracy: 0.9837\n",
      "Epoch 391/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0459 - accuracy: 1.0000\n",
      "Epoch 00391: loss did not improve from 0.04216\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0459 - accuracy: 1.0000 - val_loss: 0.0967 - val_accuracy: 0.9756\n",
      "Epoch 392/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0450 - accuracy: 1.0000\n",
      "Epoch 00392: loss did not improve from 0.04216\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0450 - accuracy: 1.0000 - val_loss: 0.0969 - val_accuracy: 0.9756\n",
      "Epoch 393/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0440 - accuracy: 1.0000\n",
      "Epoch 00393: loss did not improve from 0.04216\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0440 - accuracy: 1.0000 - val_loss: 0.0890 - val_accuracy: 0.9837\n",
      "Epoch 394/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0428 - accuracy: 1.0000\n",
      "Epoch 00394: loss did not improve from 0.04216\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0428 - accuracy: 1.0000 - val_loss: 0.0915 - val_accuracy: 0.9797\n",
      "Epoch 395/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0427 - accuracy: 1.0000\n",
      "Epoch 00395: loss did not improve from 0.04216\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0427 - accuracy: 1.0000 - val_loss: 0.0961 - val_accuracy: 0.9797\n",
      "Epoch 396/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0413 - accuracy: 1.0000\n",
      "Epoch 00396: loss improved from 0.04216 to 0.04131, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 4s 188ms/step - loss: 0.0413 - accuracy: 1.0000 - val_loss: 0.0889 - val_accuracy: 0.9756\n",
      "Epoch 397/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0405 - accuracy: 1.0000\n",
      "Epoch 00397: loss improved from 0.04131 to 0.04051, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 5s 230ms/step - loss: 0.0405 - accuracy: 1.0000 - val_loss: 0.0875 - val_accuracy: 0.9756\n",
      "Epoch 398/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0400 - accuracy: 1.0000\n",
      "Epoch 00398: loss improved from 0.04051 to 0.03996, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 5s 223ms/step - loss: 0.0400 - accuracy: 1.0000 - val_loss: 0.0902 - val_accuracy: 0.9675\n",
      "Epoch 399/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0397 - accuracy: 1.0000\n",
      "Epoch 00399: loss improved from 0.03996 to 0.03965, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 5s 222ms/step - loss: 0.0397 - accuracy: 1.0000 - val_loss: 0.0849 - val_accuracy: 0.9837\n",
      "Epoch 400/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0391 - accuracy: 1.0000\n",
      "Epoch 00400: loss improved from 0.03965 to 0.03910, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 5s 223ms/step - loss: 0.0391 - accuracy: 1.0000 - val_loss: 0.0640 - val_accuracy: 0.9878\n",
      "Epoch 401/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0400 - accuracy: 1.0000\n",
      "Epoch 00401: loss did not improve from 0.03910\n",
      "23/23 [==============================] - 5s 219ms/step - loss: 0.0400 - accuracy: 1.0000 - val_loss: 0.0471 - val_accuracy: 1.0000\n",
      "Epoch 402/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0395 - accuracy: 0.9986\n",
      "Epoch 00402: loss did not improve from 0.03910\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0395 - accuracy: 0.9986 - val_loss: 0.0468 - val_accuracy: 0.9959\n",
      "Epoch 403/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0411 - accuracy: 0.9986\n",
      "Epoch 00403: loss did not improve from 0.03910\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0411 - accuracy: 0.9986 - val_loss: 0.0616 - val_accuracy: 0.9919\n",
      "Epoch 404/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0384 - accuracy: 1.0000\n",
      "Epoch 00404: loss improved from 0.03910 to 0.03843, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 4s 186ms/step - loss: 0.0384 - accuracy: 1.0000 - val_loss: 0.0689 - val_accuracy: 0.9837\n",
      "Epoch 405/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0387 - accuracy: 1.0000\n",
      "Epoch 00405: loss did not improve from 0.03843\n",
      "23/23 [==============================] - 5s 218ms/step - loss: 0.0387 - accuracy: 1.0000 - val_loss: 0.0406 - val_accuracy: 1.0000\n",
      "Epoch 406/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0366 - accuracy: 1.0000\n",
      "Epoch 00406: loss improved from 0.03843 to 0.03664, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 4s 186ms/step - loss: 0.0366 - accuracy: 1.0000 - val_loss: 0.0529 - val_accuracy: 0.9919\n",
      "Epoch 407/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0360 - accuracy: 1.0000\n",
      "Epoch 00407: loss improved from 0.03664 to 0.03601, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 5s 224ms/step - loss: 0.0360 - accuracy: 1.0000 - val_loss: 0.0489 - val_accuracy: 0.9959\n",
      "Epoch 408/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0361 - accuracy: 1.0000\n",
      "Epoch 00408: loss did not improve from 0.03601\n",
      "23/23 [==============================] - 5s 220ms/step - loss: 0.0361 - accuracy: 1.0000 - val_loss: 0.0511 - val_accuracy: 0.9919\n",
      "Epoch 409/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0342 - accuracy: 1.0000\n",
      "Epoch 00409: loss improved from 0.03601 to 0.03423, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 4s 187ms/step - loss: 0.0342 - accuracy: 1.0000 - val_loss: 0.0810 - val_accuracy: 0.9878\n",
      "Epoch 410/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0352 - accuracy: 1.0000\n",
      "Epoch 00410: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 5s 219ms/step - loss: 0.0352 - accuracy: 1.0000 - val_loss: 0.0456 - val_accuracy: 0.9919\n",
      "Epoch 411/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - ETA: 0s - loss: 0.0377 - accuracy: 0.9986\n",
      "Epoch 00411: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0377 - accuracy: 0.9986 - val_loss: 0.0731 - val_accuracy: 0.9837\n",
      "Epoch 412/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0369 - accuracy: 1.0000\n",
      "Epoch 00412: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0369 - accuracy: 1.0000 - val_loss: 0.0946 - val_accuracy: 0.9878\n",
      "Epoch 413/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0346 - accuracy: 1.0000\n",
      "Epoch 00413: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 5s 219ms/step - loss: 0.0346 - accuracy: 1.0000 - val_loss: 0.0714 - val_accuracy: 0.9837\n",
      "Epoch 414/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0366 - accuracy: 0.9986\n",
      "Epoch 00414: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 186ms/step - loss: 0.0366 - accuracy: 0.9986 - val_loss: 0.1211 - val_accuracy: 0.9634\n",
      "Epoch 415/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0352 - accuracy: 1.0000\n",
      "Epoch 00415: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0352 - accuracy: 1.0000 - val_loss: 0.0585 - val_accuracy: 0.9837\n",
      "Epoch 416/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0362 - accuracy: 0.9986\n",
      "Epoch 00416: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0362 - accuracy: 0.9986 - val_loss: 0.1280 - val_accuracy: 0.9715\n",
      "Epoch 417/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0347 - accuracy: 1.0000\n",
      "Epoch 00417: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0347 - accuracy: 1.0000 - val_loss: 0.0654 - val_accuracy: 0.9878\n",
      "Epoch 418/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0524 - accuracy: 0.9959\n",
      "Epoch 00418: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0524 - accuracy: 0.9959 - val_loss: 0.1290 - val_accuracy: 0.9756\n",
      "Epoch 419/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0701 - accuracy: 0.9850\n",
      "Epoch 00419: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0701 - accuracy: 0.9850 - val_loss: 0.1191 - val_accuracy: 0.9756\n",
      "Epoch 420/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0619 - accuracy: 0.9905\n",
      "Epoch 00420: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0619 - accuracy: 0.9905 - val_loss: 0.1149 - val_accuracy: 0.9797\n",
      "Epoch 421/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0518 - accuracy: 0.9973\n",
      "Epoch 00421: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 185ms/step - loss: 0.0518 - accuracy: 0.9973 - val_loss: 0.0744 - val_accuracy: 0.9837\n",
      "Epoch 422/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0485 - accuracy: 0.9973\n",
      "Epoch 00422: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.0485 - accuracy: 0.9973 - val_loss: 0.1666 - val_accuracy: 0.9797\n",
      "Epoch 423/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0396 - accuracy: 1.0000\n",
      "Epoch 00423: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0396 - accuracy: 1.0000 - val_loss: 0.1345 - val_accuracy: 0.9756\n",
      "Epoch 424/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0451 - accuracy: 0.9973\n",
      "Epoch 00424: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0451 - accuracy: 0.9973 - val_loss: 0.1477 - val_accuracy: 0.9715\n",
      "Epoch 425/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0417 - accuracy: 1.0000\n",
      "Epoch 00425: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0417 - accuracy: 1.0000 - val_loss: 0.1763 - val_accuracy: 0.9797\n",
      "Epoch 426/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0479 - accuracy: 0.9946\n",
      "Epoch 00426: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0479 - accuracy: 0.9946 - val_loss: 0.1506 - val_accuracy: 0.9715\n",
      "Epoch 427/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0585 - accuracy: 0.9946\n",
      "Epoch 00427: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0585 - accuracy: 0.9946 - val_loss: 0.1749 - val_accuracy: 0.9553\n",
      "Epoch 428/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0696 - accuracy: 0.9905\n",
      "Epoch 00428: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0696 - accuracy: 0.9905 - val_loss: 0.0906 - val_accuracy: 0.9878\n",
      "Epoch 429/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0780 - accuracy: 0.9878\n",
      "Epoch 00429: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0780 - accuracy: 0.9878 - val_loss: 0.1626 - val_accuracy: 0.9715\n",
      "Epoch 430/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0752 - accuracy: 0.9905\n",
      "Epoch 00430: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0752 - accuracy: 0.9905 - val_loss: 0.1615 - val_accuracy: 0.9593\n",
      "Epoch 431/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0817 - accuracy: 0.9864\n",
      "Epoch 00431: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0817 - accuracy: 0.9864 - val_loss: 0.1475 - val_accuracy: 0.9675\n",
      "Epoch 432/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0559 - accuracy: 0.9959\n",
      "Epoch 00432: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0559 - accuracy: 0.9959 - val_loss: 0.1000 - val_accuracy: 0.9878\n",
      "Epoch 433/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0495 - accuracy: 0.9986\n",
      "Epoch 00433: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0495 - accuracy: 0.9986 - val_loss: 0.0966 - val_accuracy: 0.9715\n",
      "Epoch 434/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0611 - accuracy: 0.9946\n",
      "Epoch 00434: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 185ms/step - loss: 0.0611 - accuracy: 0.9946 - val_loss: 0.1144 - val_accuracy: 0.9878\n",
      "Epoch 435/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0513 - accuracy: 0.9973\n",
      "Epoch 00435: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 5s 197ms/step - loss: 0.0513 - accuracy: 0.9973 - val_loss: 0.1110 - val_accuracy: 0.9797\n",
      "Epoch 436/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0608 - accuracy: 0.9959\n",
      "Epoch 00436: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 191ms/step - loss: 0.0608 - accuracy: 0.9959 - val_loss: 0.0630 - val_accuracy: 0.9919\n",
      "Epoch 437/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0487 - accuracy: 0.9986\n",
      "Epoch 00437: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 188ms/step - loss: 0.0487 - accuracy: 0.9986 - val_loss: 0.0958 - val_accuracy: 0.9878\n",
      "Epoch 438/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0449 - accuracy: 1.0000\n",
      "Epoch 00438: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 5s 201ms/step - loss: 0.0449 - accuracy: 1.0000 - val_loss: 0.0819 - val_accuracy: 0.9878\n",
      "Epoch 439/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0436 - accuracy: 1.0000\n",
      "Epoch 00439: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 5s 204ms/step - loss: 0.0436 - accuracy: 1.0000 - val_loss: 0.0747 - val_accuracy: 0.9878\n",
      "Epoch 440/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0453 - accuracy: 0.9986\n",
      "Epoch 00440: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 6s 256ms/step - loss: 0.0453 - accuracy: 0.9986 - val_loss: 0.0624 - val_accuracy: 0.9919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 441/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0449 - accuracy: 0.9986\n",
      "Epoch 00441: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 184ms/step - loss: 0.0449 - accuracy: 0.9986 - val_loss: 0.1089 - val_accuracy: 0.9878\n",
      "Epoch 442/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0479 - accuracy: 0.9986\n",
      "Epoch 00442: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0479 - accuracy: 0.9986 - val_loss: 0.1190 - val_accuracy: 0.9675\n",
      "Epoch 443/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0596 - accuracy: 0.9973\n",
      "Epoch 00443: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.0596 - accuracy: 0.9973 - val_loss: 0.0612 - val_accuracy: 0.9878\n",
      "Epoch 444/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0449 - accuracy: 0.9986\n",
      "Epoch 00444: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0449 - accuracy: 0.9986 - val_loss: 0.0553 - val_accuracy: 0.9959\n",
      "Epoch 445/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0696 - accuracy: 0.9905\n",
      "Epoch 00445: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0696 - accuracy: 0.9905 - val_loss: 0.1404 - val_accuracy: 0.9756\n",
      "Epoch 446/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1009 - accuracy: 0.9810\n",
      "Epoch 00446: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.1009 - accuracy: 0.9810 - val_loss: 0.0677 - val_accuracy: 0.9959\n",
      "Epoch 447/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0597 - accuracy: 0.9959\n",
      "Epoch 00447: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0597 - accuracy: 0.9959 - val_loss: 0.0938 - val_accuracy: 0.9837\n",
      "Epoch 448/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0472 - accuracy: 1.0000\n",
      "Epoch 00448: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0472 - accuracy: 1.0000 - val_loss: 0.0655 - val_accuracy: 0.9919\n",
      "Epoch 449/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0466 - accuracy: 0.9986\n",
      "Epoch 00449: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0466 - accuracy: 0.9986 - val_loss: 0.0517 - val_accuracy: 1.0000\n",
      "Epoch 450/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0521 - accuracy: 0.9973\n",
      "Epoch 00450: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0521 - accuracy: 0.9973 - val_loss: 0.1447 - val_accuracy: 0.9715\n",
      "Epoch 451/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0542 - accuracy: 0.9973\n",
      "Epoch 00451: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0542 - accuracy: 0.9973 - val_loss: 0.0575 - val_accuracy: 0.9959\n",
      "Epoch 452/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0589 - accuracy: 0.9946\n",
      "Epoch 00452: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0589 - accuracy: 0.9946 - val_loss: 0.0541 - val_accuracy: 1.0000\n",
      "Epoch 453/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0578 - accuracy: 0.9973\n",
      "Epoch 00453: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0578 - accuracy: 0.9973 - val_loss: 0.0813 - val_accuracy: 0.9878\n",
      "Epoch 454/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0453 - accuracy: 1.0000\n",
      "Epoch 00454: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0453 - accuracy: 1.0000 - val_loss: 0.0539 - val_accuracy: 1.0000\n",
      "Epoch 455/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0453 - accuracy: 1.0000\n",
      "Epoch 00455: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0453 - accuracy: 1.0000 - val_loss: 0.0547 - val_accuracy: 0.9959\n",
      "Epoch 456/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0459 - accuracy: 0.9986\n",
      "Epoch 00456: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0459 - accuracy: 0.9986 - val_loss: 0.1053 - val_accuracy: 0.9837\n",
      "Epoch 457/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0449 - accuracy: 1.0000\n",
      "Epoch 00457: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0449 - accuracy: 1.0000 - val_loss: 0.0708 - val_accuracy: 0.9878\n",
      "Epoch 458/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0429 - accuracy: 1.0000\n",
      "Epoch 00458: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0429 - accuracy: 1.0000 - val_loss: 0.0623 - val_accuracy: 0.9919\n",
      "Epoch 459/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0418 - accuracy: 1.0000\n",
      "Epoch 00459: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0418 - accuracy: 1.0000 - val_loss: 0.0516 - val_accuracy: 0.9919\n",
      "Epoch 460/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0470 - accuracy: 0.9959\n",
      "Epoch 00460: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0470 - accuracy: 0.9959 - val_loss: 0.0613 - val_accuracy: 0.9919\n",
      "Epoch 461/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0640 - accuracy: 0.9905\n",
      "Epoch 00461: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0640 - accuracy: 0.9905 - val_loss: 0.0809 - val_accuracy: 0.9878\n",
      "Epoch 462/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0501 - accuracy: 0.9986\n",
      "Epoch 00462: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0501 - accuracy: 0.9986 - val_loss: 0.0621 - val_accuracy: 0.9959\n",
      "Epoch 463/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0461 - accuracy: 0.9986\n",
      "Epoch 00463: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0461 - accuracy: 0.9986 - val_loss: 0.0803 - val_accuracy: 0.9756\n",
      "Epoch 464/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0437 - accuracy: 1.0000\n",
      "Epoch 00464: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0437 - accuracy: 1.0000 - val_loss: 0.0503 - val_accuracy: 1.0000\n",
      "Epoch 465/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0410 - accuracy: 1.0000\n",
      "Epoch 00465: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0410 - accuracy: 1.0000 - val_loss: 0.0441 - val_accuracy: 1.0000\n",
      "Epoch 466/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0409 - accuracy: 1.0000\n",
      "Epoch 00466: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0409 - accuracy: 1.0000 - val_loss: 0.0443 - val_accuracy: 1.0000\n",
      "Epoch 467/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0409 - accuracy: 1.0000\n",
      "Epoch 00467: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 5s 213ms/step - loss: 0.0409 - accuracy: 1.0000 - val_loss: 0.0406 - val_accuracy: 1.0000\n",
      "Epoch 468/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0398 - accuracy: 1.0000\n",
      "Epoch 00468: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 5s 198ms/step - loss: 0.0398 - accuracy: 1.0000 - val_loss: 0.0737 - val_accuracy: 0.9878\n",
      "Epoch 469/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0387 - accuracy: 1.0000\n",
      "Epoch 00469: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0387 - accuracy: 1.0000 - val_loss: 0.0721 - val_accuracy: 0.9878\n",
      "Epoch 470/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0393 - accuracy: 0.9986\n",
      "Epoch 00470: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0393 - accuracy: 0.9986 - val_loss: 0.0521 - val_accuracy: 0.9919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 471/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0378 - accuracy: 1.0000\n",
      "Epoch 00471: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0378 - accuracy: 1.0000 - val_loss: 0.0413 - val_accuracy: 1.0000\n",
      "Epoch 472/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0368 - accuracy: 1.0000\n",
      "Epoch 00472: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0368 - accuracy: 1.0000 - val_loss: 0.0385 - val_accuracy: 1.0000\n",
      "Epoch 473/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0365 - accuracy: 1.0000\n",
      "Epoch 00473: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0365 - accuracy: 1.0000 - val_loss: 0.0375 - val_accuracy: 1.0000\n",
      "Epoch 474/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0367 - accuracy: 1.0000\n",
      "Epoch 00474: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0367 - accuracy: 1.0000 - val_loss: 0.0379 - val_accuracy: 1.0000\n",
      "Epoch 475/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0356 - accuracy: 1.0000\n",
      "Epoch 00475: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.0356 - accuracy: 1.0000 - val_loss: 0.0369 - val_accuracy: 1.0000\n",
      "Epoch 476/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0351 - accuracy: 1.0000\n",
      "Epoch 00476: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.0351 - accuracy: 1.0000 - val_loss: 0.0657 - val_accuracy: 0.9878\n",
      "Epoch 477/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0360 - accuracy: 1.0000\n",
      "Epoch 00477: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0360 - accuracy: 1.0000 - val_loss: 0.0402 - val_accuracy: 1.0000\n",
      "Epoch 478/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0410 - accuracy: 0.9973\n",
      "Epoch 00478: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0410 - accuracy: 0.9973 - val_loss: 0.0644 - val_accuracy: 0.9837\n",
      "Epoch 479/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0414 - accuracy: 0.9986\n",
      "Epoch 00479: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0414 - accuracy: 0.9986 - val_loss: 0.0408 - val_accuracy: 1.0000\n",
      "Epoch 480/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0358 - accuracy: 1.0000\n",
      "Epoch 00480: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0358 - accuracy: 1.0000 - val_loss: 0.0433 - val_accuracy: 1.0000\n",
      "Epoch 481/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0365 - accuracy: 1.0000\n",
      "Epoch 00481: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0365 - accuracy: 1.0000 - val_loss: 0.0398 - val_accuracy: 1.0000\n",
      "Epoch 482/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0515 - accuracy: 0.9946\n",
      "Epoch 00482: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0515 - accuracy: 0.9946 - val_loss: 0.0700 - val_accuracy: 0.9797\n",
      "Epoch 483/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0570 - accuracy: 0.9905\n",
      "Epoch 00483: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 190ms/step - loss: 0.0570 - accuracy: 0.9905 - val_loss: 0.0848 - val_accuracy: 0.9878\n",
      "Epoch 484/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0442 - accuracy: 0.9973\n",
      "Epoch 00484: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 194ms/step - loss: 0.0442 - accuracy: 0.9973 - val_loss: 0.0494 - val_accuracy: 1.0000\n",
      "Epoch 485/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0418 - accuracy: 0.9986\n",
      "Epoch 00485: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 188ms/step - loss: 0.0418 - accuracy: 0.9986 - val_loss: 0.0529 - val_accuracy: 0.9959\n",
      "Epoch 486/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0424 - accuracy: 0.9973\n",
      "Epoch 00486: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 186ms/step - loss: 0.0424 - accuracy: 0.9973 - val_loss: 0.0766 - val_accuracy: 0.9837\n",
      "Epoch 487/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0470 - accuracy: 0.9973\n",
      "Epoch 00487: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 188ms/step - loss: 0.0470 - accuracy: 0.9973 - val_loss: 0.0723 - val_accuracy: 0.9797\n",
      "Epoch 488/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0538 - accuracy: 0.9959\n",
      "Epoch 00488: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 188ms/step - loss: 0.0538 - accuracy: 0.9959 - val_loss: 0.0561 - val_accuracy: 0.9919\n",
      "Epoch 489/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0652 - accuracy: 0.9918\n",
      "Epoch 00489: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 186ms/step - loss: 0.0652 - accuracy: 0.9918 - val_loss: 0.0722 - val_accuracy: 0.9919\n",
      "Epoch 490/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0429 - accuracy: 1.0000\n",
      "Epoch 00490: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.0429 - accuracy: 1.0000 - val_loss: 0.0852 - val_accuracy: 0.9756\n",
      "Epoch 491/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0402 - accuracy: 1.0000\n",
      "Epoch 00491: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.0402 - accuracy: 1.0000 - val_loss: 0.0760 - val_accuracy: 0.9797\n",
      "Epoch 492/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0398 - accuracy: 1.0000\n",
      "Epoch 00492: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.0398 - accuracy: 1.0000 - val_loss: 0.0652 - val_accuracy: 0.9878\n",
      "Epoch 493/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0389 - accuracy: 0.9986\n",
      "Epoch 00493: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 184ms/step - loss: 0.0389 - accuracy: 0.9986 - val_loss: 0.1115 - val_accuracy: 0.9837\n",
      "Epoch 494/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0502 - accuracy: 0.9946\n",
      "Epoch 00494: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 5s 219ms/step - loss: 0.0502 - accuracy: 0.9946 - val_loss: 0.0770 - val_accuracy: 0.9797\n",
      "Epoch 495/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0479 - accuracy: 0.9932\n",
      "Epoch 00495: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 5s 198ms/step - loss: 0.0479 - accuracy: 0.9932 - val_loss: 0.0839 - val_accuracy: 0.9797\n",
      "Epoch 496/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0390 - accuracy: 1.0000\n",
      "Epoch 00496: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 185ms/step - loss: 0.0390 - accuracy: 1.0000 - val_loss: 0.0844 - val_accuracy: 0.9837\n",
      "Epoch 497/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0454 - accuracy: 0.9973\n",
      "Epoch 00497: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 184ms/step - loss: 0.0454 - accuracy: 0.9973 - val_loss: 0.0884 - val_accuracy: 0.9837\n",
      "Epoch 498/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0387 - accuracy: 1.0000\n",
      "Epoch 00498: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 184ms/step - loss: 0.0387 - accuracy: 1.0000 - val_loss: 0.0787 - val_accuracy: 0.9837\n",
      "Epoch 499/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0435 - accuracy: 0.9986\n",
      "Epoch 00499: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 186ms/step - loss: 0.0435 - accuracy: 0.9986 - val_loss: 0.0778 - val_accuracy: 0.9837\n",
      "Epoch 500/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0382 - accuracy: 1.0000\n",
      "Epoch 00500: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 185ms/step - loss: 0.0382 - accuracy: 1.0000 - val_loss: 0.0778 - val_accuracy: 0.9837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 501/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0468 - accuracy: 0.9973\n",
      "Epoch 00501: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0468 - accuracy: 0.9973 - val_loss: 0.0561 - val_accuracy: 0.9959\n",
      "Epoch 502/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0414 - accuracy: 0.9973\n",
      "Epoch 00502: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 185ms/step - loss: 0.0414 - accuracy: 0.9973 - val_loss: 0.0817 - val_accuracy: 0.9837\n",
      "Epoch 503/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0462 - accuracy: 0.9986\n",
      "Epoch 00503: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0462 - accuracy: 0.9986 - val_loss: 0.0860 - val_accuracy: 0.9878\n",
      "Epoch 504/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0409 - accuracy: 1.0000\n",
      "Epoch 00504: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0409 - accuracy: 1.0000 - val_loss: 0.0823 - val_accuracy: 0.9878\n",
      "Epoch 505/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0371 - accuracy: 1.0000\n",
      "Epoch 00505: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0371 - accuracy: 1.0000 - val_loss: 0.0669 - val_accuracy: 0.9878\n",
      "Epoch 506/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0367 - accuracy: 1.0000\n",
      "Epoch 00506: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0367 - accuracy: 1.0000 - val_loss: 0.0950 - val_accuracy: 0.9837\n",
      "Epoch 507/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0376 - accuracy: 0.9986\n",
      "Epoch 00507: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0376 - accuracy: 0.9986 - val_loss: 0.0888 - val_accuracy: 0.9878\n",
      "Epoch 508/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0346 - accuracy: 1.0000\n",
      "Epoch 00508: loss did not improve from 0.03423\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0346 - accuracy: 1.0000 - val_loss: 0.0744 - val_accuracy: 0.9837\n",
      "Epoch 509/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0341 - accuracy: 1.0000\n",
      "Epoch 00509: loss improved from 0.03423 to 0.03413, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 4s 186ms/step - loss: 0.0341 - accuracy: 1.0000 - val_loss: 0.0748 - val_accuracy: 0.9837\n",
      "Epoch 510/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0336 - accuracy: 1.0000\n",
      "Epoch 00510: loss improved from 0.03413 to 0.03358, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 5s 233ms/step - loss: 0.0336 - accuracy: 1.0000 - val_loss: 0.0643 - val_accuracy: 0.9837\n",
      "Epoch 511/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0330 - accuracy: 1.0000\n",
      "Epoch 00511: loss improved from 0.03358 to 0.03304, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 5s 226ms/step - loss: 0.0330 - accuracy: 1.0000 - val_loss: 0.0647 - val_accuracy: 0.9837\n",
      "Epoch 512/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0323 - accuracy: 1.0000\n",
      "Epoch 00512: loss improved from 0.03304 to 0.03229, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 5s 239ms/step - loss: 0.0323 - accuracy: 1.0000 - val_loss: 0.0677 - val_accuracy: 0.9797\n",
      "Epoch 513/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0318 - accuracy: 1.0000\n",
      "Epoch 00513: loss improved from 0.03229 to 0.03181, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 5s 235ms/step - loss: 0.0318 - accuracy: 1.0000 - val_loss: 0.0679 - val_accuracy: 0.9837\n",
      "Epoch 514/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0347 - accuracy: 0.9986\n",
      "Epoch 00514: loss did not improve from 0.03181\n",
      "23/23 [==============================] - 5s 219ms/step - loss: 0.0347 - accuracy: 0.9986 - val_loss: 0.0844 - val_accuracy: 0.9878\n",
      "Epoch 515/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0358 - accuracy: 1.0000\n",
      "Epoch 00515: loss did not improve from 0.03181\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0358 - accuracy: 1.0000 - val_loss: 0.0509 - val_accuracy: 0.9919\n",
      "Epoch 516/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0329 - accuracy: 1.0000\n",
      "Epoch 00516: loss did not improve from 0.03181\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0329 - accuracy: 1.0000 - val_loss: 0.1000 - val_accuracy: 0.9797\n",
      "Epoch 517/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0323 - accuracy: 0.9986\n",
      "Epoch 00517: loss did not improve from 0.03181\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0323 - accuracy: 0.9986 - val_loss: 0.0852 - val_accuracy: 0.9878\n",
      "Epoch 518/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0314 - accuracy: 1.0000\n",
      "Epoch 00518: loss improved from 0.03181 to 0.03144, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 4s 185ms/step - loss: 0.0314 - accuracy: 1.0000 - val_loss: 0.0604 - val_accuracy: 0.9878\n",
      "Epoch 519/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0305 - accuracy: 1.0000\n",
      "Epoch 00519: loss improved from 0.03144 to 0.03053, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 5s 234ms/step - loss: 0.0305 - accuracy: 1.0000 - val_loss: 0.0756 - val_accuracy: 0.9837\n",
      "Epoch 520/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0324 - accuracy: 1.0000\n",
      "Epoch 00520: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 5s 228ms/step - loss: 0.0324 - accuracy: 1.0000 - val_loss: 0.0756 - val_accuracy: 0.9797\n",
      "Epoch 521/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0349 - accuracy: 0.9973\n",
      "Epoch 00521: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 5s 203ms/step - loss: 0.0349 - accuracy: 0.9973 - val_loss: 0.0860 - val_accuracy: 0.9878\n",
      "Epoch 522/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0491 - accuracy: 0.9946\n",
      "Epoch 00522: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0491 - accuracy: 0.9946 - val_loss: 0.0649 - val_accuracy: 0.9797\n",
      "Epoch 523/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0427 - accuracy: 0.9959\n",
      "Epoch 00523: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0427 - accuracy: 0.9959 - val_loss: 0.0998 - val_accuracy: 0.9797\n",
      "Epoch 524/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0599 - accuracy: 0.9878\n",
      "Epoch 00524: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0599 - accuracy: 0.9878 - val_loss: 0.1095 - val_accuracy: 0.9797\n",
      "Epoch 525/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1155 - accuracy: 0.9714\n",
      "Epoch 00525: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.1155 - accuracy: 0.9714 - val_loss: 0.0770 - val_accuracy: 0.9959\n",
      "Epoch 526/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0565 - accuracy: 0.9959\n",
      "Epoch 00526: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0565 - accuracy: 0.9959 - val_loss: 0.0692 - val_accuracy: 0.9837\n",
      "Epoch 527/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0482 - accuracy: 0.9973\n",
      "Epoch 00527: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0482 - accuracy: 0.9973 - val_loss: 0.0819 - val_accuracy: 0.9837\n",
      "Epoch 528/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0423 - accuracy: 0.9973\n",
      "Epoch 00528: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0423 - accuracy: 0.9973 - val_loss: 0.0511 - val_accuracy: 0.9959\n",
      "Epoch 529/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0406 - accuracy: 1.0000\n",
      "Epoch 00529: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0406 - accuracy: 1.0000 - val_loss: 0.0460 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 530/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0380 - accuracy: 1.0000\n",
      "Epoch 00530: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0380 - accuracy: 1.0000 - val_loss: 0.0448 - val_accuracy: 1.0000\n",
      "Epoch 531/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0374 - accuracy: 1.0000\n",
      "Epoch 00531: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0374 - accuracy: 1.0000 - val_loss: 0.0555 - val_accuracy: 0.9959\n",
      "Epoch 532/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0382 - accuracy: 1.0000\n",
      "Epoch 00532: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0382 - accuracy: 1.0000 - val_loss: 0.0590 - val_accuracy: 0.9919\n",
      "Epoch 533/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0397 - accuracy: 0.9973\n",
      "Epoch 00533: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0397 - accuracy: 0.9973 - val_loss: 0.0914 - val_accuracy: 0.9756\n",
      "Epoch 534/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0427 - accuracy: 0.9986\n",
      "Epoch 00534: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0427 - accuracy: 0.9986 - val_loss: 0.0845 - val_accuracy: 0.9756\n",
      "Epoch 535/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0381 - accuracy: 1.0000\n",
      "Epoch 00535: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0381 - accuracy: 1.0000 - val_loss: 0.0705 - val_accuracy: 0.9797\n",
      "Epoch 536/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0366 - accuracy: 1.0000\n",
      "Epoch 00536: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0366 - accuracy: 1.0000 - val_loss: 0.0518 - val_accuracy: 0.9878\n",
      "Epoch 537/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0373 - accuracy: 0.9986\n",
      "Epoch 00537: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0373 - accuracy: 0.9986 - val_loss: 0.0758 - val_accuracy: 0.9797\n",
      "Epoch 538/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0366 - accuracy: 1.0000\n",
      "Epoch 00538: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0366 - accuracy: 1.0000 - val_loss: 0.0566 - val_accuracy: 0.9837\n",
      "Epoch 539/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0375 - accuracy: 0.9986\n",
      "Epoch 00539: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0375 - accuracy: 0.9986 - val_loss: 0.0596 - val_accuracy: 0.9878\n",
      "Epoch 540/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0363 - accuracy: 0.9986\n",
      "Epoch 00540: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0363 - accuracy: 0.9986 - val_loss: 0.0503 - val_accuracy: 0.9919\n",
      "Epoch 541/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0337 - accuracy: 1.0000\n",
      "Epoch 00541: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0337 - accuracy: 1.0000 - val_loss: 0.0461 - val_accuracy: 0.9959\n",
      "Epoch 542/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0334 - accuracy: 1.0000\n",
      "Epoch 00542: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0334 - accuracy: 1.0000 - val_loss: 0.0514 - val_accuracy: 0.9878\n",
      "Epoch 543/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0332 - accuracy: 1.0000\n",
      "Epoch 00543: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0332 - accuracy: 1.0000 - val_loss: 0.0527 - val_accuracy: 0.9919\n",
      "Epoch 544/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0327 - accuracy: 1.0000\n",
      "Epoch 00544: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.0327 - accuracy: 1.0000 - val_loss: 0.0513 - val_accuracy: 0.9878\n",
      "Epoch 545/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0319 - accuracy: 1.0000\n",
      "Epoch 00545: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0319 - accuracy: 1.0000 - val_loss: 0.0524 - val_accuracy: 0.9837\n",
      "Epoch 546/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0313 - accuracy: 1.0000\n",
      "Epoch 00546: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0313 - accuracy: 1.0000 - val_loss: 0.0504 - val_accuracy: 0.9878\n",
      "Epoch 547/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0366 - accuracy: 0.9986\n",
      "Epoch 00547: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 5s 227ms/step - loss: 0.0366 - accuracy: 0.9986 - val_loss: 0.0836 - val_accuracy: 0.9756\n",
      "Epoch 548/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0342 - accuracy: 0.9986\n",
      "Epoch 00548: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 184ms/step - loss: 0.0342 - accuracy: 0.9986 - val_loss: 0.0955 - val_accuracy: 0.9797\n",
      "Epoch 549/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0520 - accuracy: 0.9891\n",
      "Epoch 00549: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0520 - accuracy: 0.9891 - val_loss: 0.0757 - val_accuracy: 0.9878\n",
      "Epoch 550/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0573 - accuracy: 0.9891\n",
      "Epoch 00550: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0573 - accuracy: 0.9891 - val_loss: 0.0826 - val_accuracy: 0.9837\n",
      "Epoch 551/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0575 - accuracy: 0.9932\n",
      "Epoch 00551: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0575 - accuracy: 0.9932 - val_loss: 0.1212 - val_accuracy: 0.9837\n",
      "Epoch 552/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0574 - accuracy: 0.9918\n",
      "Epoch 00552: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0574 - accuracy: 0.9918 - val_loss: 0.1601 - val_accuracy: 0.9553\n",
      "Epoch 553/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0972 - accuracy: 0.9796\n",
      "Epoch 00553: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 186ms/step - loss: 0.0972 - accuracy: 0.9796 - val_loss: 0.1292 - val_accuracy: 0.9675\n",
      "Epoch 554/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0559 - accuracy: 0.9918\n",
      "Epoch 00554: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.0559 - accuracy: 0.9918 - val_loss: 0.0951 - val_accuracy: 0.9837\n",
      "Epoch 555/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0537 - accuracy: 0.9959\n",
      "Epoch 00555: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0537 - accuracy: 0.9959 - val_loss: 0.0545 - val_accuracy: 0.9959\n",
      "Epoch 556/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0421 - accuracy: 1.0000\n",
      "Epoch 00556: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.0421 - accuracy: 1.0000 - val_loss: 0.0548 - val_accuracy: 0.9959\n",
      "Epoch 557/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0409 - accuracy: 1.0000\n",
      "Epoch 00557: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0409 - accuracy: 1.0000 - val_loss: 0.0478 - val_accuracy: 0.9959\n",
      "Epoch 558/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0449 - accuracy: 0.9973\n",
      "Epoch 00558: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0449 - accuracy: 0.9973 - val_loss: 0.0632 - val_accuracy: 0.9919\n",
      "Epoch 559/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0582 - accuracy: 0.9932\n",
      "Epoch 00559: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0582 - accuracy: 0.9932 - val_loss: 0.0609 - val_accuracy: 0.9878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 560/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0424 - accuracy: 1.0000\n",
      "Epoch 00560: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0424 - accuracy: 1.0000 - val_loss: 0.0549 - val_accuracy: 0.9959\n",
      "Epoch 561/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0397 - accuracy: 1.0000\n",
      "Epoch 00561: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0397 - accuracy: 1.0000 - val_loss: 0.0458 - val_accuracy: 0.9959\n",
      "Epoch 562/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0386 - accuracy: 1.0000\n",
      "Epoch 00562: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0386 - accuracy: 1.0000 - val_loss: 0.0445 - val_accuracy: 0.9959\n",
      "Epoch 563/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0371 - accuracy: 1.0000\n",
      "Epoch 00563: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0371 - accuracy: 1.0000 - val_loss: 0.0426 - val_accuracy: 0.9959\n",
      "Epoch 564/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0367 - accuracy: 1.0000\n",
      "Epoch 00564: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0367 - accuracy: 1.0000 - val_loss: 0.0440 - val_accuracy: 0.9959\n",
      "Epoch 565/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0367 - accuracy: 1.0000\n",
      "Epoch 00565: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0367 - accuracy: 1.0000 - val_loss: 0.0412 - val_accuracy: 0.9959\n",
      "Epoch 566/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0464 - accuracy: 0.9973\n",
      "Epoch 00566: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0464 - accuracy: 0.9973 - val_loss: 0.0531 - val_accuracy: 0.9878\n",
      "Epoch 567/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0375 - accuracy: 1.0000\n",
      "Epoch 00567: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0375 - accuracy: 1.0000 - val_loss: 0.0522 - val_accuracy: 0.9959\n",
      "Epoch 568/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0381 - accuracy: 1.0000\n",
      "Epoch 00568: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.0381 - accuracy: 1.0000 - val_loss: 0.0436 - val_accuracy: 1.0000\n",
      "Epoch 569/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0444 - accuracy: 0.9959\n",
      "Epoch 00569: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0444 - accuracy: 0.9959 - val_loss: 0.0451 - val_accuracy: 0.9959\n",
      "Epoch 570/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0411 - accuracy: 0.9959\n",
      "Epoch 00570: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0411 - accuracy: 0.9959 - val_loss: 0.0532 - val_accuracy: 0.9919\n",
      "Epoch 571/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0519 - accuracy: 0.9932\n",
      "Epoch 00571: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0519 - accuracy: 0.9932 - val_loss: 0.0592 - val_accuracy: 0.9959\n",
      "Epoch 572/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0500 - accuracy: 0.9946\n",
      "Epoch 00572: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0500 - accuracy: 0.9946 - val_loss: 0.0954 - val_accuracy: 0.9797\n",
      "Epoch 573/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0433 - accuracy: 0.9986\n",
      "Epoch 00573: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.0433 - accuracy: 0.9986 - val_loss: 0.0486 - val_accuracy: 0.9959\n",
      "Epoch 574/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0380 - accuracy: 1.0000\n",
      "Epoch 00574: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 5s 218ms/step - loss: 0.0380 - accuracy: 1.0000 - val_loss: 0.0486 - val_accuracy: 0.9959\n",
      "Epoch 575/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0364 - accuracy: 1.0000\n",
      "Epoch 00575: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 195ms/step - loss: 0.0364 - accuracy: 1.0000 - val_loss: 0.0462 - val_accuracy: 0.9919\n",
      "Epoch 576/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0354 - accuracy: 1.0000\n",
      "Epoch 00576: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 193ms/step - loss: 0.0354 - accuracy: 1.0000 - val_loss: 0.0469 - val_accuracy: 0.9919\n",
      "Epoch 577/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0349 - accuracy: 1.0000\n",
      "Epoch 00577: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 189ms/step - loss: 0.0349 - accuracy: 1.0000 - val_loss: 0.0486 - val_accuracy: 0.9919\n",
      "Epoch 578/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0343 - accuracy: 1.0000\n",
      "Epoch 00578: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 189ms/step - loss: 0.0343 - accuracy: 1.0000 - val_loss: 0.0466 - val_accuracy: 0.9919\n",
      "Epoch 579/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0339 - accuracy: 1.0000\n",
      "Epoch 00579: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 188ms/step - loss: 0.0339 - accuracy: 1.0000 - val_loss: 0.0441 - val_accuracy: 0.9959\n",
      "Epoch 580/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0332 - accuracy: 1.0000\n",
      "Epoch 00580: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 188ms/step - loss: 0.0332 - accuracy: 1.0000 - val_loss: 0.0448 - val_accuracy: 0.9959\n",
      "Epoch 581/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0330 - accuracy: 1.0000\n",
      "Epoch 00581: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 187ms/step - loss: 0.0330 - accuracy: 1.0000 - val_loss: 0.0457 - val_accuracy: 0.9959\n",
      "Epoch 582/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0329 - accuracy: 1.0000\n",
      "Epoch 00582: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 185ms/step - loss: 0.0329 - accuracy: 1.0000 - val_loss: 0.0482 - val_accuracy: 0.9959\n",
      "Epoch 583/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0336 - accuracy: 1.0000\n",
      "Epoch 00583: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 187ms/step - loss: 0.0336 - accuracy: 1.0000 - val_loss: 0.0413 - val_accuracy: 0.9959\n",
      "Epoch 584/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0340 - accuracy: 0.9986\n",
      "Epoch 00584: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 5s 196ms/step - loss: 0.0340 - accuracy: 0.9986 - val_loss: 0.0431 - val_accuracy: 0.9959\n",
      "Epoch 585/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0330 - accuracy: 1.0000\n",
      "Epoch 00585: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 195ms/step - loss: 0.0330 - accuracy: 1.0000 - val_loss: 0.0684 - val_accuracy: 0.9878\n",
      "Epoch 586/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0312 - accuracy: 1.0000\n",
      "Epoch 00586: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 186ms/step - loss: 0.0312 - accuracy: 1.0000 - val_loss: 0.0591 - val_accuracy: 0.9837\n",
      "Epoch 587/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0309 - accuracy: 1.0000\n",
      "Epoch 00587: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 186ms/step - loss: 0.0309 - accuracy: 1.0000 - val_loss: 0.0465 - val_accuracy: 0.9919\n",
      "Epoch 588/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0388 - accuracy: 0.9973\n",
      "Epoch 00588: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 187ms/step - loss: 0.0388 - accuracy: 0.9973 - val_loss: 0.0456 - val_accuracy: 0.9959\n",
      "Epoch 589/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0361 - accuracy: 0.9973\n",
      "Epoch 00589: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 185ms/step - loss: 0.0361 - accuracy: 0.9973 - val_loss: 0.0766 - val_accuracy: 0.9837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 590/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0408 - accuracy: 0.9973\n",
      "Epoch 00590: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.0408 - accuracy: 0.9973 - val_loss: 0.0789 - val_accuracy: 0.9837\n",
      "Epoch 591/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0677 - accuracy: 0.9891\n",
      "Epoch 00591: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0677 - accuracy: 0.9891 - val_loss: 0.0609 - val_accuracy: 0.9878\n",
      "Epoch 592/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0424 - accuracy: 0.9973\n",
      "Epoch 00592: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 179ms/step - loss: 0.0424 - accuracy: 0.9973 - val_loss: 0.0583 - val_accuracy: 0.9878\n",
      "Epoch 593/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0353 - accuracy: 0.9986\n",
      "Epoch 00593: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0353 - accuracy: 0.9986 - val_loss: 0.0444 - val_accuracy: 0.9919\n",
      "Epoch 594/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0335 - accuracy: 1.0000\n",
      "Epoch 00594: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0335 - accuracy: 1.0000 - val_loss: 0.0426 - val_accuracy: 0.9959\n",
      "Epoch 595/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0472 - accuracy: 0.9946\n",
      "Epoch 00595: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0472 - accuracy: 0.9946 - val_loss: 0.0502 - val_accuracy: 0.9919\n",
      "Epoch 596/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0421 - accuracy: 0.9959\n",
      "Epoch 00596: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0421 - accuracy: 0.9959 - val_loss: 0.0625 - val_accuracy: 0.9878\n",
      "Epoch 597/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0363 - accuracy: 1.0000\n",
      "Epoch 00597: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0363 - accuracy: 1.0000 - val_loss: 0.0695 - val_accuracy: 0.9797\n",
      "Epoch 598/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0387 - accuracy: 0.9973\n",
      "Epoch 00598: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 179ms/step - loss: 0.0387 - accuracy: 0.9973 - val_loss: 0.0542 - val_accuracy: 0.9919\n",
      "Epoch 599/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0339 - accuracy: 1.0000\n",
      "Epoch 00599: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0339 - accuracy: 1.0000 - val_loss: 0.0371 - val_accuracy: 1.0000\n",
      "Epoch 600/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0330 - accuracy: 1.0000\n",
      "Epoch 00600: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0330 - accuracy: 1.0000 - val_loss: 0.0384 - val_accuracy: 1.0000\n",
      "Epoch 601/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0333 - accuracy: 0.9986\n",
      "Epoch 00601: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 5s 224ms/step - loss: 0.0333 - accuracy: 0.9986 - val_loss: 0.0448 - val_accuracy: 0.9959\n",
      "Epoch 602/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0704 - accuracy: 0.9864\n",
      "Epoch 00602: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 184ms/step - loss: 0.0704 - accuracy: 0.9864 - val_loss: 0.0791 - val_accuracy: 0.9797\n",
      "Epoch 603/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0599 - accuracy: 0.9905\n",
      "Epoch 00603: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0599 - accuracy: 0.9905 - val_loss: 0.0815 - val_accuracy: 0.9797\n",
      "Epoch 604/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0528 - accuracy: 0.9946\n",
      "Epoch 00604: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0528 - accuracy: 0.9946 - val_loss: 0.0765 - val_accuracy: 0.9797\n",
      "Epoch 605/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0408 - accuracy: 0.9973\n",
      "Epoch 00605: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0408 - accuracy: 0.9973 - val_loss: 0.0721 - val_accuracy: 0.9878\n",
      "Epoch 606/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0365 - accuracy: 1.0000\n",
      "Epoch 00606: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0365 - accuracy: 1.0000 - val_loss: 0.0699 - val_accuracy: 0.9878\n",
      "Epoch 607/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0416 - accuracy: 0.9959\n",
      "Epoch 00607: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0416 - accuracy: 0.9959 - val_loss: 0.0441 - val_accuracy: 0.9959\n",
      "Epoch 608/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0459 - accuracy: 0.9959\n",
      "Epoch 00608: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0459 - accuracy: 0.9959 - val_loss: 0.0803 - val_accuracy: 0.9878\n",
      "Epoch 609/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0526 - accuracy: 0.9932\n",
      "Epoch 00609: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0526 - accuracy: 0.9932 - val_loss: 0.0420 - val_accuracy: 1.0000\n",
      "Epoch 610/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0507 - accuracy: 0.9959\n",
      "Epoch 00610: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0507 - accuracy: 0.9959 - val_loss: 0.1014 - val_accuracy: 0.9797\n",
      "Epoch 611/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0512 - accuracy: 0.9986\n",
      "Epoch 00611: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0512 - accuracy: 0.9986 - val_loss: 0.0802 - val_accuracy: 0.9878\n",
      "Epoch 612/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0454 - accuracy: 0.9973\n",
      "Epoch 00612: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0454 - accuracy: 0.9973 - val_loss: 0.0808 - val_accuracy: 0.9837\n",
      "Epoch 613/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0433 - accuracy: 0.9973\n",
      "Epoch 00613: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 184ms/step - loss: 0.0433 - accuracy: 0.9973 - val_loss: 0.0533 - val_accuracy: 0.9919\n",
      "Epoch 614/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0424 - accuracy: 0.9986\n",
      "Epoch 00614: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0424 - accuracy: 0.9986 - val_loss: 0.0571 - val_accuracy: 0.9959\n",
      "Epoch 615/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0369 - accuracy: 1.0000\n",
      "Epoch 00615: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0369 - accuracy: 1.0000 - val_loss: 0.0445 - val_accuracy: 0.9959\n",
      "Epoch 616/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0357 - accuracy: 1.0000\n",
      "Epoch 00616: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0357 - accuracy: 1.0000 - val_loss: 0.0468 - val_accuracy: 0.9959\n",
      "Epoch 617/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0348 - accuracy: 1.0000\n",
      "Epoch 00617: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0348 - accuracy: 1.0000 - val_loss: 0.0437 - val_accuracy: 0.9959\n",
      "Epoch 618/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0344 - accuracy: 1.0000\n",
      "Epoch 00618: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.0344 - accuracy: 1.0000 - val_loss: 0.0384 - val_accuracy: 1.0000\n",
      "Epoch 619/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0343 - accuracy: 1.0000\n",
      "Epoch 00619: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 5s 200ms/step - loss: 0.0343 - accuracy: 1.0000 - val_loss: 0.0378 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 620/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0332 - accuracy: 1.0000\n",
      "Epoch 00620: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0332 - accuracy: 1.0000 - val_loss: 0.0369 - val_accuracy: 1.0000\n",
      "Epoch 621/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0369 - accuracy: 0.9986\n",
      "Epoch 00621: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0369 - accuracy: 0.9986 - val_loss: 0.0460 - val_accuracy: 0.9919\n",
      "Epoch 622/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0332 - accuracy: 1.0000\n",
      "Epoch 00622: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0332 - accuracy: 1.0000 - val_loss: 0.0449 - val_accuracy: 0.9919\n",
      "Epoch 623/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0321 - accuracy: 1.0000\n",
      "Epoch 00623: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0321 - accuracy: 1.0000 - val_loss: 0.0354 - val_accuracy: 1.0000\n",
      "Epoch 624/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0632 - accuracy: 0.9878\n",
      "Epoch 00624: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0632 - accuracy: 0.9878 - val_loss: 0.0526 - val_accuracy: 0.9959\n",
      "Epoch 625/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0842 - accuracy: 0.9823\n",
      "Epoch 00625: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0842 - accuracy: 0.9823 - val_loss: 0.0771 - val_accuracy: 0.9919\n",
      "Epoch 626/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0580 - accuracy: 0.9946\n",
      "Epoch 00626: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0580 - accuracy: 0.9946 - val_loss: 0.0584 - val_accuracy: 0.9919\n",
      "Epoch 627/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0415 - accuracy: 1.0000\n",
      "Epoch 00627: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0415 - accuracy: 1.0000 - val_loss: 0.0656 - val_accuracy: 0.9797\n",
      "Epoch 628/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0382 - accuracy: 0.9986\n",
      "Epoch 00628: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 5s 217ms/step - loss: 0.0382 - accuracy: 0.9986 - val_loss: 0.1362 - val_accuracy: 0.9797\n",
      "Epoch 629/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0517 - accuracy: 0.9932\n",
      "Epoch 00629: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 194ms/step - loss: 0.0517 - accuracy: 0.9932 - val_loss: 0.0583 - val_accuracy: 0.9919\n",
      "Epoch 630/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0483 - accuracy: 0.9932\n",
      "Epoch 00630: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0483 - accuracy: 0.9932 - val_loss: 0.0893 - val_accuracy: 0.9837\n",
      "Epoch 631/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0434 - accuracy: 0.9959\n",
      "Epoch 00631: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0434 - accuracy: 0.9959 - val_loss: 0.0785 - val_accuracy: 0.9878\n",
      "Epoch 632/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0402 - accuracy: 0.9986\n",
      "Epoch 00632: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0402 - accuracy: 0.9986 - val_loss: 0.0767 - val_accuracy: 0.9837\n",
      "Epoch 633/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0370 - accuracy: 1.0000\n",
      "Epoch 00633: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0370 - accuracy: 1.0000 - val_loss: 0.0533 - val_accuracy: 0.9919\n",
      "Epoch 634/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0368 - accuracy: 1.0000\n",
      "Epoch 00634: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0368 - accuracy: 1.0000 - val_loss: 0.0453 - val_accuracy: 0.9959\n",
      "Epoch 635/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0361 - accuracy: 1.0000\n",
      "Epoch 00635: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0361 - accuracy: 1.0000 - val_loss: 0.0434 - val_accuracy: 0.9959\n",
      "Epoch 636/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0348 - accuracy: 1.0000\n",
      "Epoch 00636: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 186ms/step - loss: 0.0348 - accuracy: 1.0000 - val_loss: 0.0407 - val_accuracy: 1.0000\n",
      "Epoch 637/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0345 - accuracy: 1.0000\n",
      "Epoch 00637: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0345 - accuracy: 1.0000 - val_loss: 0.0388 - val_accuracy: 1.0000\n",
      "Epoch 638/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0340 - accuracy: 1.0000\n",
      "Epoch 00638: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0340 - accuracy: 1.0000 - val_loss: 0.0397 - val_accuracy: 1.0000\n",
      "Epoch 639/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0337 - accuracy: 1.0000\n",
      "Epoch 00639: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0337 - accuracy: 1.0000 - val_loss: 0.0401 - val_accuracy: 0.9959\n",
      "Epoch 640/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0326 - accuracy: 1.0000\n",
      "Epoch 00640: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0326 - accuracy: 1.0000 - val_loss: 0.0416 - val_accuracy: 0.9959\n",
      "Epoch 641/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0423 - accuracy: 0.9959\n",
      "Epoch 00641: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0423 - accuracy: 0.9959 - val_loss: 0.0405 - val_accuracy: 1.0000\n",
      "Epoch 642/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0407 - accuracy: 0.9973\n",
      "Epoch 00642: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0407 - accuracy: 0.9973 - val_loss: 0.0699 - val_accuracy: 0.9837\n",
      "Epoch 643/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0375 - accuracy: 0.9986\n",
      "Epoch 00643: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0375 - accuracy: 0.9986 - val_loss: 0.0491 - val_accuracy: 0.9959\n",
      "Epoch 644/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0523 - accuracy: 0.9932\n",
      "Epoch 00644: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0523 - accuracy: 0.9932 - val_loss: 0.1014 - val_accuracy: 0.9715\n",
      "Epoch 645/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0389 - accuracy: 0.9986\n",
      "Epoch 00645: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0389 - accuracy: 0.9986 - val_loss: 0.0638 - val_accuracy: 0.9837\n",
      "Epoch 646/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0383 - accuracy: 0.9973\n",
      "Epoch 00646: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0383 - accuracy: 0.9973 - val_loss: 0.0615 - val_accuracy: 0.9878\n",
      "Epoch 647/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0470 - accuracy: 0.9959\n",
      "Epoch 00647: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 186ms/step - loss: 0.0470 - accuracy: 0.9959 - val_loss: 0.0559 - val_accuracy: 0.9919\n",
      "Epoch 648/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0394 - accuracy: 0.9986\n",
      "Epoch 00648: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.0394 - accuracy: 0.9986 - val_loss: 0.0612 - val_accuracy: 0.9878\n",
      "Epoch 649/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0350 - accuracy: 1.0000\n",
      "Epoch 00649: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0350 - accuracy: 1.0000 - val_loss: 0.0660 - val_accuracy: 0.9837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 650/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0342 - accuracy: 1.0000\n",
      "Epoch 00650: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0342 - accuracy: 1.0000 - val_loss: 0.0635 - val_accuracy: 0.9837\n",
      "Epoch 651/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0333 - accuracy: 1.0000\n",
      "Epoch 00651: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0333 - accuracy: 1.0000 - val_loss: 0.0692 - val_accuracy: 0.9837\n",
      "Epoch 652/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0336 - accuracy: 1.0000\n",
      "Epoch 00652: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0336 - accuracy: 1.0000 - val_loss: 0.0528 - val_accuracy: 0.9837\n",
      "Epoch 653/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0325 - accuracy: 1.0000\n",
      "Epoch 00653: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0325 - accuracy: 1.0000 - val_loss: 0.0549 - val_accuracy: 0.9878\n",
      "Epoch 654/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0318 - accuracy: 1.0000\n",
      "Epoch 00654: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0318 - accuracy: 1.0000 - val_loss: 0.0620 - val_accuracy: 0.9837\n",
      "Epoch 655/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0316 - accuracy: 1.0000\n",
      "Epoch 00655: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 5s 199ms/step - loss: 0.0316 - accuracy: 1.0000 - val_loss: 0.0570 - val_accuracy: 0.9878\n",
      "Epoch 656/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0310 - accuracy: 1.0000\n",
      "Epoch 00656: loss did not improve from 0.03053\n",
      "23/23 [==============================] - 5s 212ms/step - loss: 0.0310 - accuracy: 1.0000 - val_loss: 0.0541 - val_accuracy: 0.9878\n",
      "Epoch 657/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0305 - accuracy: 1.0000\n",
      "Epoch 00657: loss improved from 0.03053 to 0.03046, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 4s 187ms/step - loss: 0.0305 - accuracy: 1.0000 - val_loss: 0.0578 - val_accuracy: 0.9878\n",
      "Epoch 658/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0301 - accuracy: 1.0000\n",
      "Epoch 00658: loss improved from 0.03046 to 0.03013, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 5s 229ms/step - loss: 0.0301 - accuracy: 1.0000 - val_loss: 0.0619 - val_accuracy: 0.9878\n",
      "Epoch 659/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0296 - accuracy: 1.0000\n",
      "Epoch 00659: loss improved from 0.03013 to 0.02957, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 5s 230ms/step - loss: 0.0296 - accuracy: 1.0000 - val_loss: 0.0583 - val_accuracy: 0.9878\n",
      "Epoch 660/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0292 - accuracy: 1.0000\n",
      "Epoch 00660: loss improved from 0.02957 to 0.02920, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 5s 231ms/step - loss: 0.0292 - accuracy: 1.0000 - val_loss: 0.0544 - val_accuracy: 0.9878\n",
      "Epoch 661/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0286 - accuracy: 1.0000\n",
      "Epoch 00661: loss improved from 0.02920 to 0.02857, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 6s 252ms/step - loss: 0.0286 - accuracy: 1.0000 - val_loss: 0.0534 - val_accuracy: 0.9878\n",
      "Epoch 662/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0284 - accuracy: 1.0000\n",
      "Epoch 00662: loss improved from 0.02857 to 0.02842, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 6s 246ms/step - loss: 0.0284 - accuracy: 1.0000 - val_loss: 0.0528 - val_accuracy: 0.9878\n",
      "Epoch 663/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0281 - accuracy: 1.0000\n",
      "Epoch 00663: loss improved from 0.02842 to 0.02809, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 5s 238ms/step - loss: 0.0281 - accuracy: 1.0000 - val_loss: 0.0535 - val_accuracy: 0.9878\n",
      "Epoch 664/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0278 - accuracy: 1.0000\n",
      "Epoch 00664: loss improved from 0.02809 to 0.02779, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 5s 238ms/step - loss: 0.0278 - accuracy: 1.0000 - val_loss: 0.0454 - val_accuracy: 0.9878\n",
      "Epoch 665/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0378 - accuracy: 0.9973\n",
      "Epoch 00665: loss did not improve from 0.02779\n",
      "23/23 [==============================] - 5s 222ms/step - loss: 0.0378 - accuracy: 0.9973 - val_loss: 0.0602 - val_accuracy: 0.9837\n",
      "Epoch 666/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0351 - accuracy: 0.9959\n",
      "Epoch 00666: loss did not improve from 0.02779\n",
      "23/23 [==============================] - 4s 184ms/step - loss: 0.0351 - accuracy: 0.9959 - val_loss: 0.0597 - val_accuracy: 0.9878\n",
      "Epoch 667/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0307 - accuracy: 0.9986\n",
      "Epoch 00667: loss did not improve from 0.02779\n",
      "23/23 [==============================] - 4s 185ms/step - loss: 0.0307 - accuracy: 0.9986 - val_loss: 0.0527 - val_accuracy: 0.9878\n",
      "Epoch 668/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0287 - accuracy: 1.0000\n",
      "Epoch 00668: loss did not improve from 0.02779\n",
      "23/23 [==============================] - 4s 185ms/step - loss: 0.0287 - accuracy: 1.0000 - val_loss: 0.0394 - val_accuracy: 0.9959\n",
      "Epoch 669/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0282 - accuracy: 1.0000\n",
      "Epoch 00669: loss did not improve from 0.02779\n",
      "23/23 [==============================] - 4s 184ms/step - loss: 0.0282 - accuracy: 1.0000 - val_loss: 0.0330 - val_accuracy: 1.0000\n",
      "Epoch 670/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0278 - accuracy: 1.0000\n",
      "Epoch 00670: loss did not improve from 0.02779\n",
      "23/23 [==============================] - 4s 186ms/step - loss: 0.0278 - accuracy: 1.0000 - val_loss: 0.0334 - val_accuracy: 1.0000\n",
      "Epoch 671/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0271 - accuracy: 1.0000\n",
      "Epoch 00671: loss improved from 0.02779 to 0.02713, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 4s 190ms/step - loss: 0.0271 - accuracy: 1.0000 - val_loss: 0.0406 - val_accuracy: 0.9959\n",
      "Epoch 672/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0279 - accuracy: 1.0000\n",
      "Epoch 00672: loss did not improve from 0.02713\n",
      "23/23 [==============================] - 5s 226ms/step - loss: 0.0279 - accuracy: 1.0000 - val_loss: 0.0462 - val_accuracy: 0.9959\n",
      "Epoch 673/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0267 - accuracy: 1.0000\n",
      "Epoch 00673: loss improved from 0.02713 to 0.02670, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 4s 190ms/step - loss: 0.0267 - accuracy: 1.0000 - val_loss: 0.0406 - val_accuracy: 0.9919\n",
      "Epoch 674/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0268 - accuracy: 1.0000\n",
      "Epoch 00674: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 5s 231ms/step - loss: 0.0268 - accuracy: 1.0000 - val_loss: 0.0353 - val_accuracy: 0.9959\n",
      "Epoch 675/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0289 - accuracy: 0.9986\n",
      "Epoch 00675: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 4s 191ms/step - loss: 0.0289 - accuracy: 0.9986 - val_loss: 0.0297 - val_accuracy: 1.0000\n",
      "Epoch 676/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0422 - accuracy: 0.9946\n",
      "Epoch 00676: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 4s 188ms/step - loss: 0.0422 - accuracy: 0.9946 - val_loss: 0.0512 - val_accuracy: 1.0000\n",
      "Epoch 677/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0788 - accuracy: 0.9823\n",
      "Epoch 00677: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 4s 188ms/step - loss: 0.0788 - accuracy: 0.9823 - val_loss: 0.1308 - val_accuracy: 0.9634\n",
      "Epoch 678/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - ETA: 0s - loss: 0.0637 - accuracy: 0.9891\n",
      "Epoch 00678: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0637 - accuracy: 0.9891 - val_loss: 0.1270 - val_accuracy: 0.9756\n",
      "Epoch 679/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0521 - accuracy: 0.9946\n",
      "Epoch 00679: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 5s 199ms/step - loss: 0.0521 - accuracy: 0.9946 - val_loss: 0.0631 - val_accuracy: 0.9878\n",
      "Epoch 680/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0391 - accuracy: 0.9986\n",
      "Epoch 00680: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 5s 205ms/step - loss: 0.0391 - accuracy: 0.9986 - val_loss: 0.0719 - val_accuracy: 0.9797\n",
      "Epoch 681/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0502 - accuracy: 0.9959\n",
      "Epoch 00681: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0502 - accuracy: 0.9959 - val_loss: 0.0760 - val_accuracy: 0.9878\n",
      "Epoch 682/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0492 - accuracy: 0.9932\n",
      "Epoch 00682: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0492 - accuracy: 0.9932 - val_loss: 0.0595 - val_accuracy: 0.9878\n",
      "Epoch 683/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0619 - accuracy: 0.9878\n",
      "Epoch 00683: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0619 - accuracy: 0.9878 - val_loss: 0.1101 - val_accuracy: 0.9756\n",
      "Epoch 684/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0421 - accuracy: 0.9973\n",
      "Epoch 00684: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0421 - accuracy: 0.9973 - val_loss: 0.0462 - val_accuracy: 0.9959\n",
      "Epoch 685/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0498 - accuracy: 0.9959\n",
      "Epoch 00685: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0498 - accuracy: 0.9959 - val_loss: 0.1677 - val_accuracy: 0.9756\n",
      "Epoch 686/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0426 - accuracy: 0.9973\n",
      "Epoch 00686: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0426 - accuracy: 0.9973 - val_loss: 0.1160 - val_accuracy: 0.9797\n",
      "Epoch 687/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0370 - accuracy: 0.9986\n",
      "Epoch 00687: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0370 - accuracy: 0.9986 - val_loss: 0.1043 - val_accuracy: 0.9756\n",
      "Epoch 688/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0358 - accuracy: 1.0000\n",
      "Epoch 00688: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0358 - accuracy: 1.0000 - val_loss: 0.1141 - val_accuracy: 0.9797\n",
      "Epoch 689/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0351 - accuracy: 1.0000\n",
      "Epoch 00689: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0351 - accuracy: 1.0000 - val_loss: 0.0979 - val_accuracy: 0.9837\n",
      "Epoch 690/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0376 - accuracy: 0.9973\n",
      "Epoch 00690: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0376 - accuracy: 0.9973 - val_loss: 0.0529 - val_accuracy: 0.9878\n",
      "Epoch 691/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0343 - accuracy: 1.0000\n",
      "Epoch 00691: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0343 - accuracy: 1.0000 - val_loss: 0.0586 - val_accuracy: 0.9878\n",
      "Epoch 692/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0331 - accuracy: 1.0000\n",
      "Epoch 00692: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0331 - accuracy: 1.0000 - val_loss: 0.0726 - val_accuracy: 0.9837\n",
      "Epoch 693/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0322 - accuracy: 1.0000\n",
      "Epoch 00693: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0322 - accuracy: 1.0000 - val_loss: 0.0666 - val_accuracy: 0.9837\n",
      "Epoch 694/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0320 - accuracy: 1.0000\n",
      "Epoch 00694: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0320 - accuracy: 1.0000 - val_loss: 0.0666 - val_accuracy: 0.9878\n",
      "Epoch 695/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0318 - accuracy: 1.0000\n",
      "Epoch 00695: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0318 - accuracy: 1.0000 - val_loss: 0.0590 - val_accuracy: 0.9878\n",
      "Epoch 696/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0312 - accuracy: 1.0000\n",
      "Epoch 00696: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0312 - accuracy: 1.0000 - val_loss: 0.0514 - val_accuracy: 0.9878\n",
      "Epoch 697/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0307 - accuracy: 1.0000\n",
      "Epoch 00697: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0307 - accuracy: 1.0000 - val_loss: 0.0496 - val_accuracy: 0.9878\n",
      "Epoch 698/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0303 - accuracy: 1.0000\n",
      "Epoch 00698: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0303 - accuracy: 1.0000 - val_loss: 0.0488 - val_accuracy: 0.9878\n",
      "Epoch 699/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0313 - accuracy: 1.0000\n",
      "Epoch 00699: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0313 - accuracy: 1.0000 - val_loss: 0.0894 - val_accuracy: 0.9756\n",
      "Epoch 700/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0390 - accuracy: 0.9946\n",
      "Epoch 00700: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0390 - accuracy: 0.9946 - val_loss: 0.0975 - val_accuracy: 0.9756\n",
      "Epoch 701/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0320 - accuracy: 1.0000\n",
      "Epoch 00701: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 4s 186ms/step - loss: 0.0320 - accuracy: 1.0000 - val_loss: 0.0823 - val_accuracy: 0.9797\n",
      "Epoch 702/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0316 - accuracy: 1.0000\n",
      "Epoch 00702: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0316 - accuracy: 1.0000 - val_loss: 0.1049 - val_accuracy: 0.9797\n",
      "Epoch 703/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0359 - accuracy: 0.9986\n",
      "Epoch 00703: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0359 - accuracy: 0.9986 - val_loss: 0.1446 - val_accuracy: 0.9675\n",
      "Epoch 704/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0451 - accuracy: 0.9959\n",
      "Epoch 00704: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.0451 - accuracy: 0.9959 - val_loss: 0.0916 - val_accuracy: 0.9756\n",
      "Epoch 705/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0362 - accuracy: 0.9986\n",
      "Epoch 00705: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 4s 184ms/step - loss: 0.0362 - accuracy: 0.9986 - val_loss: 0.0514 - val_accuracy: 0.9837\n",
      "Epoch 706/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0313 - accuracy: 1.0000\n",
      "Epoch 00706: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0313 - accuracy: 1.0000 - val_loss: 0.0411 - val_accuracy: 0.9959\n",
      "Epoch 707/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0336 - accuracy: 0.9973\n",
      "Epoch 00707: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 5s 232ms/step - loss: 0.0336 - accuracy: 0.9973 - val_loss: 0.0585 - val_accuracy: 0.9919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 708/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0373 - accuracy: 0.9959\n",
      "Epoch 00708: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0373 - accuracy: 0.9959 - val_loss: 0.0583 - val_accuracy: 0.9959\n",
      "Epoch 709/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0321 - accuracy: 1.0000\n",
      "Epoch 00709: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 4s 179ms/step - loss: 0.0321 - accuracy: 1.0000 - val_loss: 0.0477 - val_accuracy: 0.9919\n",
      "Epoch 710/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0296 - accuracy: 1.0000\n",
      "Epoch 00710: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0296 - accuracy: 1.0000 - val_loss: 0.0492 - val_accuracy: 0.9837\n",
      "Epoch 711/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0292 - accuracy: 1.0000\n",
      "Epoch 00711: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0292 - accuracy: 1.0000 - val_loss: 0.0477 - val_accuracy: 0.9837\n",
      "Epoch 712/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0296 - accuracy: 1.0000\n",
      "Epoch 00712: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 4s 179ms/step - loss: 0.0296 - accuracy: 1.0000 - val_loss: 0.0448 - val_accuracy: 0.9919\n",
      "Epoch 713/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0286 - accuracy: 1.0000\n",
      "Epoch 00713: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0286 - accuracy: 1.0000 - val_loss: 0.0406 - val_accuracy: 0.9919\n",
      "Epoch 714/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0280 - accuracy: 1.0000\n",
      "Epoch 00714: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0280 - accuracy: 1.0000 - val_loss: 0.0397 - val_accuracy: 0.9919\n",
      "Epoch 715/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0279 - accuracy: 1.0000\n",
      "Epoch 00715: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0279 - accuracy: 1.0000 - val_loss: 0.0384 - val_accuracy: 0.9919\n",
      "Epoch 716/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0278 - accuracy: 1.0000\n",
      "Epoch 00716: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0278 - accuracy: 1.0000 - val_loss: 0.0363 - val_accuracy: 0.9959\n",
      "Epoch 717/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0272 - accuracy: 1.0000\n",
      "Epoch 00717: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0272 - accuracy: 1.0000 - val_loss: 0.0328 - val_accuracy: 1.0000\n",
      "Epoch 718/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0306 - accuracy: 0.9986\n",
      "Epoch 00718: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.0306 - accuracy: 0.9986 - val_loss: 0.0410 - val_accuracy: 0.9919\n",
      "Epoch 719/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0557 - accuracy: 0.9905\n",
      "Epoch 00719: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.0557 - accuracy: 0.9905 - val_loss: 0.0886 - val_accuracy: 0.9756\n",
      "Epoch 720/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0690 - accuracy: 0.9905\n",
      "Epoch 00720: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0690 - accuracy: 0.9905 - val_loss: 0.0721 - val_accuracy: 0.9878\n",
      "Epoch 721/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0406 - accuracy: 0.9973\n",
      "Epoch 00721: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0406 - accuracy: 0.9973 - val_loss: 0.0850 - val_accuracy: 0.9715\n",
      "Epoch 722/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0482 - accuracy: 0.9946\n",
      "Epoch 00722: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.0482 - accuracy: 0.9946 - val_loss: 0.0759 - val_accuracy: 0.9837\n",
      "Epoch 723/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0446 - accuracy: 0.9946\n",
      "Epoch 00723: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 4s 190ms/step - loss: 0.0446 - accuracy: 0.9946 - val_loss: 0.0760 - val_accuracy: 0.9797\n",
      "Epoch 724/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0505 - accuracy: 0.9918\n",
      "Epoch 00724: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 4s 190ms/step - loss: 0.0505 - accuracy: 0.9918 - val_loss: 0.0843 - val_accuracy: 0.9797\n",
      "Epoch 725/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0604 - accuracy: 0.9918\n",
      "Epoch 00725: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.0604 - accuracy: 0.9918 - val_loss: 0.0608 - val_accuracy: 0.9919\n",
      "Epoch 726/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0356 - accuracy: 0.9986\n",
      "Epoch 00726: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0356 - accuracy: 0.9986 - val_loss: 0.0614 - val_accuracy: 0.9878\n",
      "Epoch 727/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0337 - accuracy: 1.0000\n",
      "Epoch 00727: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 4s 184ms/step - loss: 0.0337 - accuracy: 1.0000 - val_loss: 0.0496 - val_accuracy: 0.9919\n",
      "Epoch 728/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0341 - accuracy: 0.9986\n",
      "Epoch 00728: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0341 - accuracy: 0.9986 - val_loss: 0.0473 - val_accuracy: 0.9959\n",
      "Epoch 729/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0358 - accuracy: 0.9973\n",
      "Epoch 00729: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0358 - accuracy: 0.9973 - val_loss: 0.0424 - val_accuracy: 1.0000\n",
      "Epoch 730/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0328 - accuracy: 1.0000\n",
      "Epoch 00730: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0328 - accuracy: 1.0000 - val_loss: 0.0413 - val_accuracy: 0.9959\n",
      "Epoch 731/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0308 - accuracy: 1.0000\n",
      "Epoch 00731: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0308 - accuracy: 1.0000 - val_loss: 0.0364 - val_accuracy: 0.9959\n",
      "Epoch 732/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0305 - accuracy: 1.0000\n",
      "Epoch 00732: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0305 - accuracy: 1.0000 - val_loss: 0.0350 - val_accuracy: 0.9959\n",
      "Epoch 733/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0299 - accuracy: 1.0000\n",
      "Epoch 00733: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0299 - accuracy: 1.0000 - val_loss: 0.0315 - val_accuracy: 1.0000\n",
      "Epoch 734/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0293 - accuracy: 1.0000\n",
      "Epoch 00734: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 5s 221ms/step - loss: 0.0293 - accuracy: 1.0000 - val_loss: 0.0316 - val_accuracy: 1.0000\n",
      "Epoch 735/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0300 - accuracy: 0.9986\n",
      "Epoch 00735: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 4s 190ms/step - loss: 0.0300 - accuracy: 0.9986 - val_loss: 0.0316 - val_accuracy: 1.0000\n",
      "Epoch 736/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0291 - accuracy: 1.0000\n",
      "Epoch 00736: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0291 - accuracy: 1.0000 - val_loss: 0.0330 - val_accuracy: 1.0000\n",
      "Epoch 737/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0312 - accuracy: 0.9986\n",
      "Epoch 00737: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0312 - accuracy: 0.9986 - val_loss: 0.0384 - val_accuracy: 0.9959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 738/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0285 - accuracy: 1.0000\n",
      "Epoch 00738: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0285 - accuracy: 1.0000 - val_loss: 0.0370 - val_accuracy: 1.0000\n",
      "Epoch 739/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0285 - accuracy: 1.0000\n",
      "Epoch 00739: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0285 - accuracy: 1.0000 - val_loss: 0.0359 - val_accuracy: 0.9959\n",
      "Epoch 740/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0274 - accuracy: 1.0000\n",
      "Epoch 00740: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0274 - accuracy: 1.0000 - val_loss: 0.0378 - val_accuracy: 0.9959\n",
      "Epoch 741/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0271 - accuracy: 1.0000\n",
      "Epoch 00741: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0271 - accuracy: 1.0000 - val_loss: 0.0354 - val_accuracy: 0.9959\n",
      "Epoch 742/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0267 - accuracy: 1.0000\n",
      "Epoch 00742: loss did not improve from 0.02670\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0267 - accuracy: 1.0000 - val_loss: 0.0310 - val_accuracy: 1.0000\n",
      "Epoch 743/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0263 - accuracy: 1.0000\n",
      "Epoch 00743: loss improved from 0.02670 to 0.02630, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 4s 185ms/step - loss: 0.0263 - accuracy: 1.0000 - val_loss: 0.0284 - val_accuracy: 1.0000\n",
      "Epoch 744/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0259 - accuracy: 1.0000\n",
      "Epoch 00744: loss improved from 0.02630 to 0.02595, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 5s 229ms/step - loss: 0.0259 - accuracy: 1.0000 - val_loss: 0.0281 - val_accuracy: 1.0000\n",
      "Epoch 745/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0296 - accuracy: 0.9986\n",
      "Epoch 00745: loss did not improve from 0.02595\n",
      "23/23 [==============================] - 5s 217ms/step - loss: 0.0296 - accuracy: 0.9986 - val_loss: 0.0328 - val_accuracy: 1.0000\n",
      "Epoch 746/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0264 - accuracy: 1.0000\n",
      "Epoch 00746: loss did not improve from 0.02595\n",
      "23/23 [==============================] - 4s 179ms/step - loss: 0.0264 - accuracy: 1.0000 - val_loss: 0.0276 - val_accuracy: 1.0000\n",
      "Epoch 747/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0257 - accuracy: 1.0000\n",
      "Epoch 00747: loss improved from 0.02595 to 0.02568, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 4s 186ms/step - loss: 0.0257 - accuracy: 1.0000 - val_loss: 0.0266 - val_accuracy: 1.0000\n",
      "Epoch 748/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0254 - accuracy: 1.0000\n",
      "Epoch 00748: loss improved from 0.02568 to 0.02542, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 5s 223ms/step - loss: 0.0254 - accuracy: 1.0000 - val_loss: 0.0272 - val_accuracy: 1.0000\n",
      "Epoch 749/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0256 - accuracy: 1.0000\n",
      "Epoch 00749: loss did not improve from 0.02542\n",
      "23/23 [==============================] - 5s 219ms/step - loss: 0.0256 - accuracy: 1.0000 - val_loss: 0.0299 - val_accuracy: 1.0000\n",
      "Epoch 750/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0250 - accuracy: 1.0000\n",
      "Epoch 00750: loss improved from 0.02542 to 0.02499, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 4s 185ms/step - loss: 0.0250 - accuracy: 1.0000 - val_loss: 0.0314 - val_accuracy: 1.0000\n",
      "Epoch 751/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0247 - accuracy: 1.0000\n",
      "Epoch 00751: loss improved from 0.02499 to 0.02467, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 5s 223ms/step - loss: 0.0247 - accuracy: 1.0000 - val_loss: 0.0299 - val_accuracy: 1.0000\n",
      "Epoch 752/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0249 - accuracy: 1.0000\n",
      "Epoch 00752: loss did not improve from 0.02467\n",
      "23/23 [==============================] - 5s 219ms/step - loss: 0.0249 - accuracy: 1.0000 - val_loss: 0.0318 - val_accuracy: 0.9959\n",
      "Epoch 753/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0240 - accuracy: 1.0000\n",
      "Epoch 00753: loss improved from 0.02467 to 0.02404, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 4s 187ms/step - loss: 0.0240 - accuracy: 1.0000 - val_loss: 0.0291 - val_accuracy: 1.0000\n",
      "Epoch 754/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0235 - accuracy: 1.0000\n",
      "Epoch 00754: loss improved from 0.02404 to 0.02353, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 5s 223ms/step - loss: 0.0235 - accuracy: 1.0000 - val_loss: 0.0305 - val_accuracy: 1.0000\n",
      "Epoch 755/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0234 - accuracy: 1.0000\n",
      "Epoch 00755: loss improved from 0.02353 to 0.02344, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 5s 223ms/step - loss: 0.0234 - accuracy: 1.0000 - val_loss: 0.0352 - val_accuracy: 0.9959\n",
      "Epoch 756/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0227 - accuracy: 1.0000\n",
      "Epoch 00756: loss improved from 0.02344 to 0.02272, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 5s 224ms/step - loss: 0.0227 - accuracy: 1.0000 - val_loss: 0.0409 - val_accuracy: 0.9919\n",
      "Epoch 757/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0228 - accuracy: 1.0000\n",
      "Epoch 00757: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 5s 220ms/step - loss: 0.0228 - accuracy: 1.0000 - val_loss: 0.0338 - val_accuracy: 0.9959\n",
      "Epoch 758/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0232 - accuracy: 1.0000\n",
      "Epoch 00758: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0232 - accuracy: 1.0000 - val_loss: 0.0259 - val_accuracy: 1.0000\n",
      "Epoch 759/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0254 - accuracy: 1.0000\n",
      "Epoch 00759: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 5s 221ms/step - loss: 0.0254 - accuracy: 1.0000 - val_loss: 0.0322 - val_accuracy: 0.9959\n",
      "Epoch 760/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0279 - accuracy: 0.9986\n",
      "Epoch 00760: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 184ms/step - loss: 0.0279 - accuracy: 0.9986 - val_loss: 0.0515 - val_accuracy: 0.9959\n",
      "Epoch 761/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0449 - accuracy: 0.9905\n",
      "Epoch 00761: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0449 - accuracy: 0.9905 - val_loss: 0.1056 - val_accuracy: 0.9715\n",
      "Epoch 762/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0438 - accuracy: 0.9932\n",
      "Epoch 00762: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0438 - accuracy: 0.9932 - val_loss: 0.0873 - val_accuracy: 0.9715\n",
      "Epoch 763/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0409 - accuracy: 0.9932\n",
      "Epoch 00763: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0409 - accuracy: 0.9932 - val_loss: 0.0658 - val_accuracy: 0.9756\n",
      "Epoch 764/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0707 - accuracy: 0.9918\n",
      "Epoch 00764: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0707 - accuracy: 0.9918 - val_loss: 0.0964 - val_accuracy: 0.9837\n",
      "Epoch 765/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0381 - accuracy: 1.0000\n",
      "Epoch 00765: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.0381 - accuracy: 1.0000 - val_loss: 0.0916 - val_accuracy: 0.9756\n",
      "Epoch 766/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - ETA: 0s - loss: 0.0293 - accuracy: 1.0000\n",
      "Epoch 00766: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0293 - accuracy: 1.0000 - val_loss: 0.0542 - val_accuracy: 0.9878\n",
      "Epoch 767/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0293 - accuracy: 1.0000\n",
      "Epoch 00767: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 185ms/step - loss: 0.0293 - accuracy: 1.0000 - val_loss: 0.0780 - val_accuracy: 0.9837\n",
      "Epoch 768/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0459 - accuracy: 0.9959\n",
      "Epoch 00768: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 179ms/step - loss: 0.0459 - accuracy: 0.9959 - val_loss: 0.0617 - val_accuracy: 0.9837\n",
      "Epoch 769/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0392 - accuracy: 0.9959\n",
      "Epoch 00769: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0392 - accuracy: 0.9959 - val_loss: 0.0766 - val_accuracy: 0.9756\n",
      "Epoch 770/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0493 - accuracy: 0.9946\n",
      "Epoch 00770: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0493 - accuracy: 0.9946 - val_loss: 0.1332 - val_accuracy: 0.9634\n",
      "Epoch 771/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0477 - accuracy: 0.9946\n",
      "Epoch 00771: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0477 - accuracy: 0.9946 - val_loss: 0.0820 - val_accuracy: 0.9756\n",
      "Epoch 772/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0479 - accuracy: 0.9946\n",
      "Epoch 00772: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0479 - accuracy: 0.9946 - val_loss: 0.0901 - val_accuracy: 0.9715\n",
      "Epoch 773/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0409 - accuracy: 0.9905\n",
      "Epoch 00773: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 186ms/step - loss: 0.0409 - accuracy: 0.9905 - val_loss: 0.0416 - val_accuracy: 1.0000\n",
      "Epoch 774/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0386 - accuracy: 0.9986\n",
      "Epoch 00774: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0386 - accuracy: 0.9986 - val_loss: 0.1110 - val_accuracy: 0.9797\n",
      "Epoch 775/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0502 - accuracy: 0.9932\n",
      "Epoch 00775: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0502 - accuracy: 0.9932 - val_loss: 0.0654 - val_accuracy: 0.9878\n",
      "Epoch 776/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0728 - accuracy: 0.9905\n",
      "Epoch 00776: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0728 - accuracy: 0.9905 - val_loss: 0.0896 - val_accuracy: 0.9675\n",
      "Epoch 777/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0494 - accuracy: 0.9932\n",
      "Epoch 00777: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0494 - accuracy: 0.9932 - val_loss: 0.0548 - val_accuracy: 0.9878\n",
      "Epoch 778/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0401 - accuracy: 0.9986\n",
      "Epoch 00778: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0401 - accuracy: 0.9986 - val_loss: 0.0593 - val_accuracy: 0.9878\n",
      "Epoch 779/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0348 - accuracy: 0.9986\n",
      "Epoch 00779: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0348 - accuracy: 0.9986 - val_loss: 0.0758 - val_accuracy: 0.9878\n",
      "Epoch 780/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0342 - accuracy: 1.0000\n",
      "Epoch 00780: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0342 - accuracy: 1.0000 - val_loss: 0.0600 - val_accuracy: 0.9837\n",
      "Epoch 781/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0322 - accuracy: 1.0000\n",
      "Epoch 00781: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0322 - accuracy: 1.0000 - val_loss: 0.0621 - val_accuracy: 0.9878\n",
      "Epoch 782/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0316 - accuracy: 1.0000\n",
      "Epoch 00782: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0316 - accuracy: 1.0000 - val_loss: 0.0603 - val_accuracy: 0.9837\n",
      "Epoch 783/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0313 - accuracy: 1.0000\n",
      "Epoch 00783: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0313 - accuracy: 1.0000 - val_loss: 0.0574 - val_accuracy: 0.9837\n",
      "Epoch 784/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0315 - accuracy: 1.0000\n",
      "Epoch 00784: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0315 - accuracy: 1.0000 - val_loss: 0.0457 - val_accuracy: 0.9959\n",
      "Epoch 785/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0304 - accuracy: 1.0000\n",
      "Epoch 00785: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0304 - accuracy: 1.0000 - val_loss: 0.0430 - val_accuracy: 0.9959\n",
      "Epoch 786/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0307 - accuracy: 1.0000\n",
      "Epoch 00786: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 5s 204ms/step - loss: 0.0307 - accuracy: 1.0000 - val_loss: 0.0549 - val_accuracy: 0.9878\n",
      "Epoch 787/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0296 - accuracy: 1.0000\n",
      "Epoch 00787: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 5s 209ms/step - loss: 0.0296 - accuracy: 1.0000 - val_loss: 0.0479 - val_accuracy: 0.9878\n",
      "Epoch 788/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0302 - accuracy: 1.0000\n",
      "Epoch 00788: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0302 - accuracy: 1.0000 - val_loss: 0.0664 - val_accuracy: 0.9797\n",
      "Epoch 789/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0291 - accuracy: 1.0000\n",
      "Epoch 00789: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0291 - accuracy: 1.0000 - val_loss: 0.0530 - val_accuracy: 0.9878\n",
      "Epoch 790/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0313 - accuracy: 0.9986\n",
      "Epoch 00790: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.0313 - accuracy: 0.9986 - val_loss: 0.0555 - val_accuracy: 0.9878\n",
      "Epoch 791/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0285 - accuracy: 1.0000\n",
      "Epoch 00791: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.0285 - accuracy: 1.0000 - val_loss: 0.0460 - val_accuracy: 0.9878\n",
      "Epoch 792/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0293 - accuracy: 1.0000\n",
      "Epoch 00792: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0293 - accuracy: 1.0000 - val_loss: 0.0451 - val_accuracy: 0.9878\n",
      "Epoch 793/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0280 - accuracy: 1.0000\n",
      "Epoch 00793: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0280 - accuracy: 1.0000 - val_loss: 0.0402 - val_accuracy: 0.9959\n",
      "Epoch 794/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0300 - accuracy: 0.9986\n",
      "Epoch 00794: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0300 - accuracy: 0.9986 - val_loss: 0.0500 - val_accuracy: 0.9878\n",
      "Epoch 795/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0295 - accuracy: 0.9986\n",
      "Epoch 00795: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0295 - accuracy: 0.9986 - val_loss: 0.0425 - val_accuracy: 0.9959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 796/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0295 - accuracy: 1.0000\n",
      "Epoch 00796: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0295 - accuracy: 1.0000 - val_loss: 0.0425 - val_accuracy: 0.9959\n",
      "Epoch 797/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0286 - accuracy: 1.0000\n",
      "Epoch 00797: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0286 - accuracy: 1.0000 - val_loss: 0.0855 - val_accuracy: 0.9837\n",
      "Epoch 798/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0298 - accuracy: 0.9986\n",
      "Epoch 00798: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0298 - accuracy: 0.9986 - val_loss: 0.0489 - val_accuracy: 0.9919\n",
      "Epoch 799/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0277 - accuracy: 1.0000\n",
      "Epoch 00799: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0277 - accuracy: 1.0000 - val_loss: 0.0446 - val_accuracy: 0.9919\n",
      "Epoch 800/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0448 - accuracy: 0.9932\n",
      "Epoch 00800: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0448 - accuracy: 0.9932 - val_loss: 0.0641 - val_accuracy: 0.9919\n",
      "Epoch 801/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0415 - accuracy: 0.9946\n",
      "Epoch 00801: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0415 - accuracy: 0.9946 - val_loss: 0.0561 - val_accuracy: 0.9919\n",
      "Epoch 802/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0652 - accuracy: 0.9891\n",
      "Epoch 00802: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0652 - accuracy: 0.9891 - val_loss: 0.0520 - val_accuracy: 0.9959\n",
      "Epoch 803/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0458 - accuracy: 0.9959\n",
      "Epoch 00803: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0458 - accuracy: 0.9959 - val_loss: 0.0923 - val_accuracy: 0.9675\n",
      "Epoch 804/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0435 - accuracy: 0.9973\n",
      "Epoch 00804: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0435 - accuracy: 0.9973 - val_loss: 0.0848 - val_accuracy: 0.9797\n",
      "Epoch 805/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0399 - accuracy: 0.9959\n",
      "Epoch 00805: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0399 - accuracy: 0.9959 - val_loss: 0.0598 - val_accuracy: 0.9919\n",
      "Epoch 806/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0326 - accuracy: 1.0000\n",
      "Epoch 00806: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0326 - accuracy: 1.0000 - val_loss: 0.0432 - val_accuracy: 0.9919\n",
      "Epoch 807/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0302 - accuracy: 1.0000\n",
      "Epoch 00807: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0302 - accuracy: 1.0000 - val_loss: 0.0452 - val_accuracy: 0.9919\n",
      "Epoch 808/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0300 - accuracy: 1.0000\n",
      "Epoch 00808: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0300 - accuracy: 1.0000 - val_loss: 0.0444 - val_accuracy: 0.9919\n",
      "Epoch 809/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0295 - accuracy: 1.0000\n",
      "Epoch 00809: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0295 - accuracy: 1.0000 - val_loss: 0.0381 - val_accuracy: 1.0000\n",
      "Epoch 810/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0287 - accuracy: 1.0000\n",
      "Epoch 00810: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0287 - accuracy: 1.0000 - val_loss: 0.0374 - val_accuracy: 0.9959\n",
      "Epoch 811/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0294 - accuracy: 1.0000\n",
      "Epoch 00811: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0294 - accuracy: 1.0000 - val_loss: 0.0387 - val_accuracy: 0.9959\n",
      "Epoch 812/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0281 - accuracy: 1.0000\n",
      "Epoch 00812: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0281 - accuracy: 1.0000 - val_loss: 0.0447 - val_accuracy: 0.9959\n",
      "Epoch 813/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0313 - accuracy: 0.9986\n",
      "Epoch 00813: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0313 - accuracy: 0.9986 - val_loss: 0.1131 - val_accuracy: 0.9837\n",
      "Epoch 814/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0309 - accuracy: 0.9986\n",
      "Epoch 00814: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 5s 229ms/step - loss: 0.0309 - accuracy: 0.9986 - val_loss: 0.0522 - val_accuracy: 0.9959\n",
      "Epoch 815/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0289 - accuracy: 1.0000\n",
      "Epoch 00815: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0289 - accuracy: 1.0000 - val_loss: 0.0302 - val_accuracy: 1.0000\n",
      "Epoch 816/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0277 - accuracy: 1.0000\n",
      "Epoch 00816: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0277 - accuracy: 1.0000 - val_loss: 0.0499 - val_accuracy: 0.9878\n",
      "Epoch 817/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0274 - accuracy: 1.0000\n",
      "Epoch 00817: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0274 - accuracy: 1.0000 - val_loss: 0.0462 - val_accuracy: 0.9919\n",
      "Epoch 818/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0276 - accuracy: 1.0000\n",
      "Epoch 00818: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0276 - accuracy: 1.0000 - val_loss: 0.0426 - val_accuracy: 0.9919\n",
      "Epoch 819/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0267 - accuracy: 1.0000\n",
      "Epoch 00819: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0267 - accuracy: 1.0000 - val_loss: 0.0395 - val_accuracy: 0.9959\n",
      "Epoch 820/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0271 - accuracy: 1.0000\n",
      "Epoch 00820: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0271 - accuracy: 1.0000 - val_loss: 0.0611 - val_accuracy: 0.9797\n",
      "Epoch 821/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0312 - accuracy: 0.9986\n",
      "Epoch 00821: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 5s 205ms/step - loss: 0.0312 - accuracy: 0.9986 - val_loss: 0.0938 - val_accuracy: 0.9837\n",
      "Epoch 822/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0487 - accuracy: 0.9959\n",
      "Epoch 00822: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0487 - accuracy: 0.9959 - val_loss: 0.0438 - val_accuracy: 0.9959\n",
      "Epoch 823/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0356 - accuracy: 0.9973\n",
      "Epoch 00823: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0356 - accuracy: 0.9973 - val_loss: 0.0632 - val_accuracy: 0.9797\n",
      "Epoch 824/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0300 - accuracy: 1.0000\n",
      "Epoch 00824: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0300 - accuracy: 1.0000 - val_loss: 0.0511 - val_accuracy: 0.9878\n",
      "Epoch 825/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0375 - accuracy: 0.9986\n",
      "Epoch 00825: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0375 - accuracy: 0.9986 - val_loss: 0.0403 - val_accuracy: 0.9959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 826/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0297 - accuracy: 1.0000\n",
      "Epoch 00826: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0297 - accuracy: 1.0000 - val_loss: 0.0408 - val_accuracy: 0.9959\n",
      "Epoch 827/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0284 - accuracy: 1.0000\n",
      "Epoch 00827: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0284 - accuracy: 1.0000 - val_loss: 0.0420 - val_accuracy: 0.9959\n",
      "Epoch 828/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0275 - accuracy: 1.0000\n",
      "Epoch 00828: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 179ms/step - loss: 0.0275 - accuracy: 1.0000 - val_loss: 0.0336 - val_accuracy: 1.0000\n",
      "Epoch 829/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0268 - accuracy: 1.0000\n",
      "Epoch 00829: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0268 - accuracy: 1.0000 - val_loss: 0.0331 - val_accuracy: 1.0000\n",
      "Epoch 830/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0269 - accuracy: 1.0000\n",
      "Epoch 00830: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 179ms/step - loss: 0.0269 - accuracy: 1.0000 - val_loss: 0.0398 - val_accuracy: 0.9959\n",
      "Epoch 831/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0262 - accuracy: 1.0000\n",
      "Epoch 00831: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0262 - accuracy: 1.0000 - val_loss: 0.0416 - val_accuracy: 0.9959\n",
      "Epoch 832/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0262 - accuracy: 1.0000\n",
      "Epoch 00832: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0262 - accuracy: 1.0000 - val_loss: 0.0423 - val_accuracy: 0.9919\n",
      "Epoch 833/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0257 - accuracy: 1.0000\n",
      "Epoch 00833: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0257 - accuracy: 1.0000 - val_loss: 0.0443 - val_accuracy: 0.9878\n",
      "Epoch 834/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0255 - accuracy: 1.0000\n",
      "Epoch 00834: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0255 - accuracy: 1.0000 - val_loss: 0.0660 - val_accuracy: 0.9837\n",
      "Epoch 835/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0250 - accuracy: 1.0000\n",
      "Epoch 00835: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 184ms/step - loss: 0.0250 - accuracy: 1.0000 - val_loss: 0.0527 - val_accuracy: 0.9837\n",
      "Epoch 836/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0249 - accuracy: 1.0000\n",
      "Epoch 00836: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 179ms/step - loss: 0.0249 - accuracy: 1.0000 - val_loss: 0.0307 - val_accuracy: 1.0000\n",
      "Epoch 837/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0245 - accuracy: 1.0000\n",
      "Epoch 00837: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0245 - accuracy: 1.0000 - val_loss: 0.0293 - val_accuracy: 1.0000\n",
      "Epoch 838/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0241 - accuracy: 1.0000\n",
      "Epoch 00838: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0241 - accuracy: 1.0000 - val_loss: 0.0301 - val_accuracy: 1.0000\n",
      "Epoch 839/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0237 - accuracy: 1.0000\n",
      "Epoch 00839: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0237 - accuracy: 1.0000 - val_loss: 0.0338 - val_accuracy: 0.9959\n",
      "Epoch 840/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0235 - accuracy: 1.0000\n",
      "Epoch 00840: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0235 - accuracy: 1.0000 - val_loss: 0.0354 - val_accuracy: 0.9959\n",
      "Epoch 841/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0229 - accuracy: 1.0000\n",
      "Epoch 00841: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 5s 220ms/step - loss: 0.0229 - accuracy: 1.0000 - val_loss: 0.0324 - val_accuracy: 0.9959\n",
      "Epoch 842/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0237 - accuracy: 1.0000\n",
      "Epoch 00842: loss did not improve from 0.02272\n",
      "23/23 [==============================] - 4s 192ms/step - loss: 0.0237 - accuracy: 1.0000 - val_loss: 0.0315 - val_accuracy: 1.0000\n",
      "Epoch 843/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0227 - accuracy: 1.0000\n",
      "Epoch 00843: loss improved from 0.02272 to 0.02272, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 4s 186ms/step - loss: 0.0227 - accuracy: 1.0000 - val_loss: 0.0357 - val_accuracy: 0.9959\n",
      "Epoch 844/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0224 - accuracy: 1.0000\n",
      "Epoch 00844: loss improved from 0.02272 to 0.02238, saving model to Emotion_detection.h5\n",
      "23/23 [==============================] - 5s 223ms/step - loss: 0.0224 - accuracy: 1.0000 - val_loss: 0.0282 - val_accuracy: 1.0000\n",
      "Epoch 845/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0267 - accuracy: 0.9986\n",
      "Epoch 00845: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 5s 218ms/step - loss: 0.0267 - accuracy: 0.9986 - val_loss: 0.0713 - val_accuracy: 0.9797\n",
      "Epoch 846/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0862 - accuracy: 0.9864\n",
      "Epoch 00846: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0862 - accuracy: 0.9864 - val_loss: 0.0961 - val_accuracy: 0.9756\n",
      "Epoch 847/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0449 - accuracy: 0.9959\n",
      "Epoch 00847: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0449 - accuracy: 0.9959 - val_loss: 0.0753 - val_accuracy: 0.9797\n",
      "Epoch 848/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0745 - accuracy: 0.9878\n",
      "Epoch 00848: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0745 - accuracy: 0.9878 - val_loss: 0.1589 - val_accuracy: 0.9593\n",
      "Epoch 849/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0681 - accuracy: 0.9891\n",
      "Epoch 00849: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0681 - accuracy: 0.9891 - val_loss: 0.1256 - val_accuracy: 0.9593\n",
      "Epoch 850/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0433 - accuracy: 0.9946\n",
      "Epoch 00850: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0433 - accuracy: 0.9946 - val_loss: 0.0540 - val_accuracy: 0.9959\n",
      "Epoch 851/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0505 - accuracy: 0.9946\n",
      "Epoch 00851: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 184ms/step - loss: 0.0505 - accuracy: 0.9946 - val_loss: 0.0532 - val_accuracy: 0.9878\n",
      "Epoch 852/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0345 - accuracy: 0.9986\n",
      "Epoch 00852: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0345 - accuracy: 0.9986 - val_loss: 0.1142 - val_accuracy: 0.9756\n",
      "Epoch 853/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0412 - accuracy: 0.9959\n",
      "Epoch 00853: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0412 - accuracy: 0.9959 - val_loss: 0.0698 - val_accuracy: 0.9837\n",
      "Epoch 854/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0343 - accuracy: 0.9986\n",
      "Epoch 00854: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0343 - accuracy: 0.9986 - val_loss: 0.0884 - val_accuracy: 0.9756\n",
      "Epoch 855/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0327 - accuracy: 0.9986\n",
      "Epoch 00855: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0327 - accuracy: 0.9986 - val_loss: 0.0578 - val_accuracy: 0.9878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 856/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0310 - accuracy: 1.0000\n",
      "Epoch 00856: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0310 - accuracy: 1.0000 - val_loss: 0.0820 - val_accuracy: 0.9878\n",
      "Epoch 857/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0300 - accuracy: 1.0000\n",
      "Epoch 00857: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 179ms/step - loss: 0.0300 - accuracy: 1.0000 - val_loss: 0.0942 - val_accuracy: 0.9878\n",
      "Epoch 858/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0327 - accuracy: 0.9973\n",
      "Epoch 00858: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0327 - accuracy: 0.9973 - val_loss: 0.0892 - val_accuracy: 0.9837\n",
      "Epoch 859/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0341 - accuracy: 1.0000\n",
      "Epoch 00859: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0341 - accuracy: 1.0000 - val_loss: 0.0772 - val_accuracy: 0.9878\n",
      "Epoch 860/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0333 - accuracy: 0.9973\n",
      "Epoch 00860: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0333 - accuracy: 0.9973 - val_loss: 0.0696 - val_accuracy: 0.9756\n",
      "Epoch 861/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0301 - accuracy: 1.0000\n",
      "Epoch 00861: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0301 - accuracy: 1.0000 - val_loss: 0.0500 - val_accuracy: 0.9919\n",
      "Epoch 862/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0302 - accuracy: 1.0000\n",
      "Epoch 00862: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0302 - accuracy: 1.0000 - val_loss: 0.0479 - val_accuracy: 0.9919\n",
      "Epoch 863/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0288 - accuracy: 1.0000\n",
      "Epoch 00863: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0288 - accuracy: 1.0000 - val_loss: 0.0508 - val_accuracy: 0.9837\n",
      "Epoch 864/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0318 - accuracy: 0.9986\n",
      "Epoch 00864: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0318 - accuracy: 0.9986 - val_loss: 0.0650 - val_accuracy: 0.9797\n",
      "Epoch 865/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0434 - accuracy: 0.9959\n",
      "Epoch 00865: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0434 - accuracy: 0.9959 - val_loss: 0.0601 - val_accuracy: 0.9837\n",
      "Epoch 866/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0326 - accuracy: 1.0000\n",
      "Epoch 00866: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0326 - accuracy: 1.0000 - val_loss: 0.0407 - val_accuracy: 1.0000\n",
      "Epoch 867/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0382 - accuracy: 0.9959\n",
      "Epoch 00867: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0382 - accuracy: 0.9959 - val_loss: 0.0632 - val_accuracy: 0.9837\n",
      "Epoch 868/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0323 - accuracy: 0.9986\n",
      "Epoch 00868: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 5s 219ms/step - loss: 0.0323 - accuracy: 0.9986 - val_loss: 0.0820 - val_accuracy: 0.9837\n",
      "Epoch 869/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0407 - accuracy: 0.9959\n",
      "Epoch 00869: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 192ms/step - loss: 0.0407 - accuracy: 0.9959 - val_loss: 0.0749 - val_accuracy: 0.9837\n",
      "Epoch 870/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0375 - accuracy: 0.9973\n",
      "Epoch 00870: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0375 - accuracy: 0.9973 - val_loss: 0.0529 - val_accuracy: 0.9878\n",
      "Epoch 871/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0310 - accuracy: 1.0000\n",
      "Epoch 00871: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0310 - accuracy: 1.0000 - val_loss: 0.0395 - val_accuracy: 0.9959\n",
      "Epoch 872/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0310 - accuracy: 0.9986\n",
      "Epoch 00872: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0310 - accuracy: 0.9986 - val_loss: 0.0487 - val_accuracy: 0.9919\n",
      "Epoch 873/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0300 - accuracy: 1.0000\n",
      "Epoch 00873: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0300 - accuracy: 1.0000 - val_loss: 0.0353 - val_accuracy: 1.0000\n",
      "Epoch 874/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0312 - accuracy: 0.9986\n",
      "Epoch 00874: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0312 - accuracy: 0.9986 - val_loss: 0.0436 - val_accuracy: 0.9919\n",
      "Epoch 875/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0334 - accuracy: 0.9973\n",
      "Epoch 00875: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0334 - accuracy: 0.9973 - val_loss: 0.0462 - val_accuracy: 0.9919\n",
      "Epoch 876/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0341 - accuracy: 0.9973\n",
      "Epoch 00876: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0341 - accuracy: 0.9973 - val_loss: 0.0345 - val_accuracy: 1.0000\n",
      "Epoch 877/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0295 - accuracy: 1.0000\n",
      "Epoch 00877: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0295 - accuracy: 1.0000 - val_loss: 0.0359 - val_accuracy: 0.9959\n",
      "Epoch 878/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0286 - accuracy: 1.0000\n",
      "Epoch 00878: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0286 - accuracy: 1.0000 - val_loss: 0.0442 - val_accuracy: 0.9919\n",
      "Epoch 879/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0279 - accuracy: 1.0000\n",
      "Epoch 00879: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0279 - accuracy: 1.0000 - val_loss: 0.0433 - val_accuracy: 0.9959\n",
      "Epoch 880/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0286 - accuracy: 1.0000\n",
      "Epoch 00880: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0286 - accuracy: 1.0000 - val_loss: 0.0437 - val_accuracy: 0.9959\n",
      "Epoch 881/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0282 - accuracy: 1.0000\n",
      "Epoch 00881: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0282 - accuracy: 1.0000 - val_loss: 0.0810 - val_accuracy: 0.9878\n",
      "Epoch 882/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0272 - accuracy: 1.0000\n",
      "Epoch 00882: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0272 - accuracy: 1.0000 - val_loss: 0.0648 - val_accuracy: 0.9878\n",
      "Epoch 883/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0269 - accuracy: 1.0000\n",
      "Epoch 00883: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0269 - accuracy: 1.0000 - val_loss: 0.0419 - val_accuracy: 0.9959\n",
      "Epoch 884/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0263 - accuracy: 1.0000\n",
      "Epoch 00884: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0263 - accuracy: 1.0000 - val_loss: 0.0383 - val_accuracy: 0.9959\n",
      "Epoch 885/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0268 - accuracy: 1.0000\n",
      "Epoch 00885: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0268 - accuracy: 1.0000 - val_loss: 0.0443 - val_accuracy: 0.9959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 886/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0275 - accuracy: 1.0000\n",
      "Epoch 00886: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0275 - accuracy: 1.0000 - val_loss: 0.0327 - val_accuracy: 0.9959\n",
      "Epoch 887/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0259 - accuracy: 1.0000\n",
      "Epoch 00887: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0259 - accuracy: 1.0000 - val_loss: 0.0334 - val_accuracy: 0.9959\n",
      "Epoch 888/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0260 - accuracy: 1.0000\n",
      "Epoch 00888: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0260 - accuracy: 1.0000 - val_loss: 0.0406 - val_accuracy: 0.9959\n",
      "Epoch 889/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0251 - accuracy: 1.0000\n",
      "Epoch 00889: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0251 - accuracy: 1.0000 - val_loss: 0.0355 - val_accuracy: 0.9959\n",
      "Epoch 890/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0266 - accuracy: 1.0000\n",
      "Epoch 00890: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0266 - accuracy: 1.0000 - val_loss: 0.0434 - val_accuracy: 0.9878\n",
      "Epoch 891/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0254 - accuracy: 1.0000\n",
      "Epoch 00891: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0254 - accuracy: 1.0000 - val_loss: 0.0533 - val_accuracy: 0.9878\n",
      "Epoch 892/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0260 - accuracy: 1.0000\n",
      "Epoch 00892: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0260 - accuracy: 1.0000 - val_loss: 0.0367 - val_accuracy: 0.9959\n",
      "Epoch 893/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0257 - accuracy: 1.0000\n",
      "Epoch 00893: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0257 - accuracy: 1.0000 - val_loss: 0.0336 - val_accuracy: 0.9959\n",
      "Epoch 894/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0241 - accuracy: 1.0000\n",
      "Epoch 00894: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0241 - accuracy: 1.0000 - val_loss: 0.0404 - val_accuracy: 0.9919\n",
      "Epoch 895/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0239 - accuracy: 1.0000\n",
      "Epoch 00895: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 5s 198ms/step - loss: 0.0239 - accuracy: 1.0000 - val_loss: 0.0351 - val_accuracy: 0.9959\n",
      "Epoch 896/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0237 - accuracy: 1.0000\n",
      "Epoch 00896: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 5s 211ms/step - loss: 0.0237 - accuracy: 1.0000 - val_loss: 0.0344 - val_accuracy: 0.9959\n",
      "Epoch 897/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0231 - accuracy: 1.0000\n",
      "Epoch 00897: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0231 - accuracy: 1.0000 - val_loss: 0.0341 - val_accuracy: 0.9959\n",
      "Epoch 898/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0235 - accuracy: 1.0000\n",
      "Epoch 00898: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0235 - accuracy: 1.0000 - val_loss: 0.0337 - val_accuracy: 0.9959\n",
      "Epoch 899/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0245 - accuracy: 1.0000\n",
      "Epoch 00899: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0245 - accuracy: 1.0000 - val_loss: 0.0577 - val_accuracy: 0.9919\n",
      "Epoch 900/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0242 - accuracy: 1.0000\n",
      "Epoch 00900: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0242 - accuracy: 1.0000 - val_loss: 0.0494 - val_accuracy: 0.9837\n",
      "Epoch 901/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0229 - accuracy: 1.0000\n",
      "Epoch 00901: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0229 - accuracy: 1.0000 - val_loss: 0.0617 - val_accuracy: 0.9878\n",
      "Epoch 902/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0228 - accuracy: 1.0000\n",
      "Epoch 00902: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0228 - accuracy: 1.0000 - val_loss: 0.0485 - val_accuracy: 0.9919\n",
      "Epoch 903/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0261 - accuracy: 0.9986\n",
      "Epoch 00903: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 194ms/step - loss: 0.0261 - accuracy: 0.9986 - val_loss: 0.0787 - val_accuracy: 0.9919\n",
      "Epoch 904/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0343 - accuracy: 0.9959\n",
      "Epoch 00904: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 193ms/step - loss: 0.0343 - accuracy: 0.9959 - val_loss: 0.0493 - val_accuracy: 0.9837\n",
      "Epoch 905/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0331 - accuracy: 0.9959\n",
      "Epoch 00905: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 5s 202ms/step - loss: 0.0331 - accuracy: 0.9959 - val_loss: 0.0566 - val_accuracy: 0.9959\n",
      "Epoch 906/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0473 - accuracy: 0.9878\n",
      "Epoch 00906: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 193ms/step - loss: 0.0473 - accuracy: 0.9878 - val_loss: 0.0850 - val_accuracy: 0.9756\n",
      "Epoch 907/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0589 - accuracy: 0.9905\n",
      "Epoch 00907: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 187ms/step - loss: 0.0589 - accuracy: 0.9905 - val_loss: 0.0686 - val_accuracy: 0.9919\n",
      "Epoch 908/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0356 - accuracy: 0.9973\n",
      "Epoch 00908: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 188ms/step - loss: 0.0356 - accuracy: 0.9973 - val_loss: 0.0827 - val_accuracy: 0.9837\n",
      "Epoch 909/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0670 - accuracy: 0.9891\n",
      "Epoch 00909: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 187ms/step - loss: 0.0670 - accuracy: 0.9891 - val_loss: 0.2099 - val_accuracy: 0.9390\n",
      "Epoch 910/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0857 - accuracy: 0.9782\n",
      "Epoch 00910: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 185ms/step - loss: 0.0857 - accuracy: 0.9782 - val_loss: 0.1267 - val_accuracy: 0.9675\n",
      "Epoch 911/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0598 - accuracy: 0.9918\n",
      "Epoch 00911: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 184ms/step - loss: 0.0598 - accuracy: 0.9918 - val_loss: 0.0815 - val_accuracy: 0.9837\n",
      "Epoch 912/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0483 - accuracy: 0.9932\n",
      "Epoch 00912: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 186ms/step - loss: 0.0483 - accuracy: 0.9932 - val_loss: 0.1144 - val_accuracy: 0.9756\n",
      "Epoch 913/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0661 - accuracy: 0.9864\n",
      "Epoch 00913: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 186ms/step - loss: 0.0661 - accuracy: 0.9864 - val_loss: 0.0812 - val_accuracy: 0.9919\n",
      "Epoch 914/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0821 - accuracy: 0.9905\n",
      "Epoch 00914: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 187ms/step - loss: 0.0821 - accuracy: 0.9905 - val_loss: 0.1479 - val_accuracy: 0.9512\n",
      "Epoch 915/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0473 - accuracy: 0.9959\n",
      "Epoch 00915: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 184ms/step - loss: 0.0473 - accuracy: 0.9959 - val_loss: 0.0599 - val_accuracy: 0.9878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 916/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0377 - accuracy: 1.0000\n",
      "Epoch 00916: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 179ms/step - loss: 0.0377 - accuracy: 1.0000 - val_loss: 0.0503 - val_accuracy: 0.9959\n",
      "Epoch 917/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0402 - accuracy: 0.9986\n",
      "Epoch 00917: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0402 - accuracy: 0.9986 - val_loss: 0.0504 - val_accuracy: 0.9919\n",
      "Epoch 918/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0385 - accuracy: 0.9973\n",
      "Epoch 00918: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 5s 223ms/step - loss: 0.0385 - accuracy: 0.9973 - val_loss: 0.0444 - val_accuracy: 0.9959\n",
      "Epoch 919/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0345 - accuracy: 1.0000\n",
      "Epoch 00919: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0345 - accuracy: 1.0000 - val_loss: 0.0442 - val_accuracy: 0.9959\n",
      "Epoch 920/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0347 - accuracy: 1.0000\n",
      "Epoch 00920: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0347 - accuracy: 1.0000 - val_loss: 0.0363 - val_accuracy: 1.0000\n",
      "Epoch 921/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0347 - accuracy: 0.9986\n",
      "Epoch 00921: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0347 - accuracy: 0.9986 - val_loss: 0.0497 - val_accuracy: 0.9878\n",
      "Epoch 922/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0335 - accuracy: 1.0000\n",
      "Epoch 00922: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 5s 218ms/step - loss: 0.0335 - accuracy: 1.0000 - val_loss: 0.0405 - val_accuracy: 1.0000\n",
      "Epoch 923/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0348 - accuracy: 0.9986\n",
      "Epoch 00923: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 191ms/step - loss: 0.0348 - accuracy: 0.9986 - val_loss: 0.0522 - val_accuracy: 0.9959\n",
      "Epoch 924/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0349 - accuracy: 0.9986\n",
      "Epoch 00924: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0349 - accuracy: 0.9986 - val_loss: 0.0404 - val_accuracy: 1.0000\n",
      "Epoch 925/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0313 - accuracy: 1.0000\n",
      "Epoch 00925: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0313 - accuracy: 1.0000 - val_loss: 0.0412 - val_accuracy: 0.9959\n",
      "Epoch 926/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0316 - accuracy: 1.0000\n",
      "Epoch 00926: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0316 - accuracy: 1.0000 - val_loss: 0.0379 - val_accuracy: 1.0000\n",
      "Epoch 927/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0316 - accuracy: 1.0000\n",
      "Epoch 00927: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0316 - accuracy: 1.0000 - val_loss: 0.0429 - val_accuracy: 0.9959\n",
      "Epoch 928/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0409 - accuracy: 0.9973\n",
      "Epoch 00928: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0409 - accuracy: 0.9973 - val_loss: 0.1003 - val_accuracy: 0.9675\n",
      "Epoch 929/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0545 - accuracy: 0.9891\n",
      "Epoch 00929: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0545 - accuracy: 0.9891 - val_loss: 0.1169 - val_accuracy: 0.9756\n",
      "Epoch 930/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0385 - accuracy: 0.9973\n",
      "Epoch 00930: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0385 - accuracy: 0.9973 - val_loss: 0.1123 - val_accuracy: 0.9756\n",
      "Epoch 931/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0360 - accuracy: 0.9986\n",
      "Epoch 00931: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0360 - accuracy: 0.9986 - val_loss: 0.1054 - val_accuracy: 0.9675\n",
      "Epoch 932/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0341 - accuracy: 1.0000\n",
      "Epoch 00932: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0341 - accuracy: 1.0000 - val_loss: 0.0970 - val_accuracy: 0.9715\n",
      "Epoch 933/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0338 - accuracy: 0.9986\n",
      "Epoch 00933: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0338 - accuracy: 0.9986 - val_loss: 0.1173 - val_accuracy: 0.9756\n",
      "Epoch 934/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0337 - accuracy: 1.0000\n",
      "Epoch 00934: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0337 - accuracy: 1.0000 - val_loss: 0.0844 - val_accuracy: 0.9837\n",
      "Epoch 935/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0358 - accuracy: 0.9973\n",
      "Epoch 00935: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0358 - accuracy: 0.9973 - val_loss: 0.0775 - val_accuracy: 0.9797\n",
      "Epoch 936/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0346 - accuracy: 0.9986\n",
      "Epoch 00936: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0346 - accuracy: 0.9986 - val_loss: 0.0539 - val_accuracy: 0.9959\n",
      "Epoch 937/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0412 - accuracy: 0.9959\n",
      "Epoch 00937: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0412 - accuracy: 0.9959 - val_loss: 0.0642 - val_accuracy: 0.9797\n",
      "Epoch 938/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0323 - accuracy: 1.0000\n",
      "Epoch 00938: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0323 - accuracy: 1.0000 - val_loss: 0.0539 - val_accuracy: 0.9919\n",
      "Epoch 939/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0356 - accuracy: 0.9986\n",
      "Epoch 00939: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0356 - accuracy: 0.9986 - val_loss: 0.1505 - val_accuracy: 0.9715\n",
      "Epoch 940/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0348 - accuracy: 0.9986\n",
      "Epoch 00940: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0348 - accuracy: 0.9986 - val_loss: 0.0640 - val_accuracy: 0.9878\n",
      "Epoch 941/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0313 - accuracy: 1.0000\n",
      "Epoch 00941: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0313 - accuracy: 1.0000 - val_loss: 0.0685 - val_accuracy: 0.9837\n",
      "Epoch 942/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0310 - accuracy: 1.0000\n",
      "Epoch 00942: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0310 - accuracy: 1.0000 - val_loss: 0.0596 - val_accuracy: 0.9878\n",
      "Epoch 943/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0299 - accuracy: 1.0000\n",
      "Epoch 00943: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.0299 - accuracy: 1.0000 - val_loss: 0.0580 - val_accuracy: 0.9878\n",
      "Epoch 944/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0297 - accuracy: 1.0000\n",
      "Epoch 00944: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0297 - accuracy: 1.0000 - val_loss: 0.0552 - val_accuracy: 0.9878\n",
      "Epoch 945/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0290 - accuracy: 1.0000\n",
      "Epoch 00945: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0290 - accuracy: 1.0000 - val_loss: 0.0536 - val_accuracy: 0.9878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 946/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0293 - accuracy: 1.0000\n",
      "Epoch 00946: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0293 - accuracy: 1.0000 - val_loss: 0.0452 - val_accuracy: 0.9919\n",
      "Epoch 947/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0283 - accuracy: 1.0000\n",
      "Epoch 00947: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0283 - accuracy: 1.0000 - val_loss: 0.0423 - val_accuracy: 0.9919\n",
      "Epoch 948/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0281 - accuracy: 1.0000\n",
      "Epoch 00948: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 179ms/step - loss: 0.0281 - accuracy: 1.0000 - val_loss: 0.0420 - val_accuracy: 0.9919\n",
      "Epoch 949/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0279 - accuracy: 1.0000\n",
      "Epoch 00949: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 5s 204ms/step - loss: 0.0279 - accuracy: 1.0000 - val_loss: 0.0400 - val_accuracy: 0.9959\n",
      "Epoch 950/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0275 - accuracy: 1.0000\n",
      "Epoch 00950: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.0275 - accuracy: 1.0000 - val_loss: 0.0393 - val_accuracy: 0.9959\n",
      "Epoch 951/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0272 - accuracy: 1.0000\n",
      "Epoch 00951: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0272 - accuracy: 1.0000 - val_loss: 0.0413 - val_accuracy: 0.9959\n",
      "Epoch 952/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0268 - accuracy: 1.0000\n",
      "Epoch 00952: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0268 - accuracy: 1.0000 - val_loss: 0.0442 - val_accuracy: 0.9919\n",
      "Epoch 953/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0268 - accuracy: 1.0000\n",
      "Epoch 00953: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0268 - accuracy: 1.0000 - val_loss: 0.0451 - val_accuracy: 0.9919\n",
      "Epoch 954/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0262 - accuracy: 1.0000\n",
      "Epoch 00954: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0262 - accuracy: 1.0000 - val_loss: 0.0504 - val_accuracy: 0.9919\n",
      "Epoch 955/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0261 - accuracy: 1.0000\n",
      "Epoch 00955: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0261 - accuracy: 1.0000 - val_loss: 0.0274 - val_accuracy: 1.0000\n",
      "Epoch 956/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0258 - accuracy: 1.0000\n",
      "Epoch 00956: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0258 - accuracy: 1.0000 - val_loss: 0.0352 - val_accuracy: 0.9959\n",
      "Epoch 957/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0258 - accuracy: 1.0000\n",
      "Epoch 00957: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0258 - accuracy: 1.0000 - val_loss: 0.0344 - val_accuracy: 0.9959\n",
      "Epoch 958/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0250 - accuracy: 1.0000\n",
      "Epoch 00958: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0250 - accuracy: 1.0000 - val_loss: 0.0297 - val_accuracy: 1.0000\n",
      "Epoch 959/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0247 - accuracy: 1.0000\n",
      "Epoch 00959: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0247 - accuracy: 1.0000 - val_loss: 0.0283 - val_accuracy: 1.0000\n",
      "Epoch 960/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0246 - accuracy: 1.0000\n",
      "Epoch 00960: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0246 - accuracy: 1.0000 - val_loss: 0.0278 - val_accuracy: 1.0000\n",
      "Epoch 961/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0250 - accuracy: 1.0000\n",
      "Epoch 00961: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0250 - accuracy: 1.0000 - val_loss: 0.0360 - val_accuracy: 0.9959\n",
      "Epoch 962/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0384 - accuracy: 0.9973\n",
      "Epoch 00962: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0384 - accuracy: 0.9973 - val_loss: 0.0456 - val_accuracy: 0.9919\n",
      "Epoch 963/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0413 - accuracy: 0.9973\n",
      "Epoch 00963: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0413 - accuracy: 0.9973 - val_loss: 0.0735 - val_accuracy: 0.9919\n",
      "Epoch 964/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0363 - accuracy: 0.9973\n",
      "Epoch 00964: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0363 - accuracy: 0.9973 - val_loss: 0.0359 - val_accuracy: 0.9959\n",
      "Epoch 965/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0472 - accuracy: 0.9932\n",
      "Epoch 00965: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0472 - accuracy: 0.9932 - val_loss: 0.0764 - val_accuracy: 0.9837\n",
      "Epoch 966/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0336 - accuracy: 0.9973\n",
      "Epoch 00966: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0336 - accuracy: 0.9973 - val_loss: 0.0431 - val_accuracy: 0.9919\n",
      "Epoch 967/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0349 - accuracy: 0.9973\n",
      "Epoch 00967: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0349 - accuracy: 0.9973 - val_loss: 0.0613 - val_accuracy: 0.9878\n",
      "Epoch 968/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0336 - accuracy: 0.9973\n",
      "Epoch 00968: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0336 - accuracy: 0.9973 - val_loss: 0.0660 - val_accuracy: 0.9878\n",
      "Epoch 969/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0326 - accuracy: 0.9986\n",
      "Epoch 00969: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0326 - accuracy: 0.9986 - val_loss: 0.0629 - val_accuracy: 0.9797\n",
      "Epoch 970/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0379 - accuracy: 0.9986\n",
      "Epoch 00970: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0379 - accuracy: 0.9986 - val_loss: 0.0958 - val_accuracy: 0.9634\n",
      "Epoch 971/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0388 - accuracy: 0.9959\n",
      "Epoch 00971: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 184ms/step - loss: 0.0388 - accuracy: 0.9959 - val_loss: 0.0759 - val_accuracy: 0.9756\n",
      "Epoch 972/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0376 - accuracy: 0.9973\n",
      "Epoch 00972: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0376 - accuracy: 0.9973 - val_loss: 0.2423 - val_accuracy: 0.9553\n",
      "Epoch 973/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0356 - accuracy: 0.9986\n",
      "Epoch 00973: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0356 - accuracy: 0.9986 - val_loss: 0.0692 - val_accuracy: 0.9837\n",
      "Epoch 974/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0462 - accuracy: 0.9905\n",
      "Epoch 00974: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0462 - accuracy: 0.9905 - val_loss: 0.0654 - val_accuracy: 0.9878\n",
      "Epoch 975/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0332 - accuracy: 0.9986\n",
      "Epoch 00975: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0332 - accuracy: 0.9986 - val_loss: 0.0904 - val_accuracy: 0.9756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 976/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0334 - accuracy: 0.9959\n",
      "Epoch 00976: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 189ms/step - loss: 0.0334 - accuracy: 0.9959 - val_loss: 0.0555 - val_accuracy: 0.9878\n",
      "Epoch 977/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0325 - accuracy: 0.9986\n",
      "Epoch 00977: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 5s 225ms/step - loss: 0.0325 - accuracy: 0.9986 - val_loss: 0.0688 - val_accuracy: 0.9878\n",
      "Epoch 978/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0293 - accuracy: 1.0000\n",
      "Epoch 00978: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0293 - accuracy: 1.0000 - val_loss: 0.0570 - val_accuracy: 0.9959\n",
      "Epoch 979/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0314 - accuracy: 0.9973\n",
      "Epoch 00979: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0314 - accuracy: 0.9973 - val_loss: 0.1262 - val_accuracy: 0.9756\n",
      "Epoch 980/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0283 - accuracy: 1.0000\n",
      "Epoch 00980: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0283 - accuracy: 1.0000 - val_loss: 0.1545 - val_accuracy: 0.9675\n",
      "Epoch 981/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0284 - accuracy: 1.0000\n",
      "Epoch 00981: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0284 - accuracy: 1.0000 - val_loss: 0.0766 - val_accuracy: 0.9797\n",
      "Epoch 982/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0275 - accuracy: 1.0000\n",
      "Epoch 00982: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0275 - accuracy: 1.0000 - val_loss: 0.0911 - val_accuracy: 0.9878\n",
      "Epoch 983/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0274 - accuracy: 1.0000\n",
      "Epoch 00983: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0274 - accuracy: 1.0000 - val_loss: 0.1056 - val_accuracy: 0.9878\n",
      "Epoch 984/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0268 - accuracy: 1.0000\n",
      "Epoch 00984: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0268 - accuracy: 1.0000 - val_loss: 0.0996 - val_accuracy: 0.9878\n",
      "Epoch 985/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0271 - accuracy: 1.0000\n",
      "Epoch 00985: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0271 - accuracy: 1.0000 - val_loss: 0.0954 - val_accuracy: 0.9837\n",
      "Epoch 986/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0294 - accuracy: 0.9986\n",
      "Epoch 00986: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0294 - accuracy: 0.9986 - val_loss: 0.0720 - val_accuracy: 0.9797\n",
      "Epoch 987/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0274 - accuracy: 1.0000\n",
      "Epoch 00987: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0274 - accuracy: 1.0000 - val_loss: 0.0522 - val_accuracy: 0.9959\n",
      "Epoch 988/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0340 - accuracy: 0.9959\n",
      "Epoch 00988: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 179ms/step - loss: 0.0340 - accuracy: 0.9959 - val_loss: 0.1026 - val_accuracy: 0.9837\n",
      "Epoch 989/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0483 - accuracy: 0.9959\n",
      "Epoch 00989: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 184ms/step - loss: 0.0483 - accuracy: 0.9959 - val_loss: 0.0527 - val_accuracy: 0.9959\n",
      "Epoch 990/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0622 - accuracy: 0.9918\n",
      "Epoch 00990: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0622 - accuracy: 0.9918 - val_loss: 0.0447 - val_accuracy: 0.9959\n",
      "Epoch 991/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0374 - accuracy: 0.9973\n",
      "Epoch 00991: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0374 - accuracy: 0.9973 - val_loss: 0.0521 - val_accuracy: 0.9919\n",
      "Epoch 992/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0326 - accuracy: 1.0000\n",
      "Epoch 00992: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 0.0326 - accuracy: 1.0000 - val_loss: 0.0480 - val_accuracy: 0.9959\n",
      "Epoch 993/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0302 - accuracy: 1.0000\n",
      "Epoch 00993: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0302 - accuracy: 1.0000 - val_loss: 0.0687 - val_accuracy: 0.9837\n",
      "Epoch 994/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0289 - accuracy: 1.0000\n",
      "Epoch 00994: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0289 - accuracy: 1.0000 - val_loss: 0.0556 - val_accuracy: 0.9878\n",
      "Epoch 995/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0279 - accuracy: 1.0000\n",
      "Epoch 00995: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0279 - accuracy: 1.0000 - val_loss: 0.0519 - val_accuracy: 0.9919\n",
      "Epoch 996/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0277 - accuracy: 1.0000\n",
      "Epoch 00996: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0277 - accuracy: 1.0000 - val_loss: 0.0502 - val_accuracy: 0.9878\n",
      "Epoch 997/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0269 - accuracy: 1.0000\n",
      "Epoch 00997: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0269 - accuracy: 1.0000 - val_loss: 0.0503 - val_accuracy: 0.9878\n",
      "Epoch 998/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0267 - accuracy: 1.0000\n",
      "Epoch 00998: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 0.0267 - accuracy: 1.0000 - val_loss: 0.0501 - val_accuracy: 0.9878\n",
      "Epoch 999/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0264 - accuracy: 1.0000\n",
      "Epoch 00999: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 0.0264 - accuracy: 1.0000 - val_loss: 0.0501 - val_accuracy: 0.9878\n",
      "Epoch 1000/1000\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0270 - accuracy: 1.0000\n",
      "Epoch 01000: loss did not improve from 0.02238\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 0.0270 - accuracy: 1.0000 - val_loss: 0.0521 - val_accuracy: 0.9878\n"
     ]
    }
   ],
   "source": [
    "History=Model.fit(X_train,Y_train,batch_size=32,validation_data=(X_test,Y_test),epochs=1000,callbacks=[callback_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.0947200e-12, 7.1101509e-18, 1.2180045e-08, ..., 8.9315682e-12,\n",
       "        6.6858673e-01, 7.5042403e-12],\n",
       "       [1.2472421e-02, 2.4886131e-03, 1.3562137e-08, ..., 8.1494254e-05,\n",
       "        2.3542368e-11, 2.0215978e-07],\n",
       "       [3.4838227e-05, 7.9771173e-01, 2.2453203e-06, ..., 1.6131997e-04,\n",
       "        3.9510645e-08, 3.7093236e-05],\n",
       "       ...,\n",
       "       [1.8740183e-06, 1.4695259e-10, 8.3060586e-01, ..., 3.5764472e-08,\n",
       "        7.2698581e-06, 1.0193582e-14],\n",
       "       [2.9230476e-10, 2.8744221e-10, 3.8939896e-11, ..., 9.2868000e-01,\n",
       "        2.0526039e-10, 8.4381417e-09],\n",
       "       [2.5577883e-06, 1.5550455e-05, 1.7591176e-10, ..., 1.0161366e-05,\n",
       "        5.1488172e-08, 1.4499563e-01]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pred=Model.predict(X_test)\n",
    "Pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "i=0 \n",
    "Y_test_l=[] \n",
    "Pred_l=[] \n",
    "while(i<len(Pred)):   \n",
    "    Y_test_l.append(int(np.argmax(Y_test[i])))     \n",
    "    Pred_l.append(int(np.argmax(Pred[i])))   \n",
    "    i+=1\n",
    "report=classification_report(Y_test_l, Pred_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.98        21\n",
      "           1       1.00      1.00      1.00        14\n",
      "           2       0.96      1.00      0.98        52\n",
      "           3       0.97      1.00      0.98        31\n",
      "           4       1.00      1.00      1.00        60\n",
      "           5       1.00      0.96      0.98        48\n",
      "           6       1.00      1.00      1.00        20\n",
      "\n",
      "    accuracy                           0.99       246\n",
      "   macro avg       0.99      0.99      0.99       246\n",
      "weighted avg       0.99      0.99      0.99       246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def test_image(ind,images_f,images_f_2,Model):\n",
    "    plt.imshow(images_f[ind])\n",
    "    image_test=images_f_2[ind]\n",
    "    print(\"Label actual:  \" + Exp[labels[ind]]  )\n",
    "    pred_1=Model.predict(np.array([image_test]))\n",
    "    #print(pred_1)\n",
    "    pred_class=Exp[int(np.argmax(pred_1))]\n",
    "    print(\"Predicted Label: \"+ pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[20,  0,  0,  1,  0,  0,  0],\n",
       "       [ 0, 14,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0, 52,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0, 31,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0, 60,  0,  0],\n",
       "       [ 0,  0,  2,  0,  0, 46,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0, 20]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix \n",
    "confusion_matrix(Y_test_l, Pred_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fe9c41002b0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.heatmap(confusion_matrix(Y_test_l, Pred_l), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(History.history['loss'])\n",
    "plt.plot(History.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.subplots_adjust(top=1.00, bottom=0.0, left=0.0, right=0.95, hspace=0.25,\n",
    "                        wspace=0.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(History.history['accuracy'])\n",
    "plt.plot(History.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.subplots_adjust(top=1.00, bottom=0.0, left=0.0, right=0.95, hspace=0.25,\n",
    "                        wspace=0.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label actual:  fear\n",
      "Predicted Label: fear\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_image(72,images_f,images_f_2,Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label actual:  fear\n",
      "Predicted Label: fear\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_image(36,images_f,images_f_2,Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label actual:  contempt\n",
      "Predicted Label: contempt\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_image(122,images_f,images_f_2,Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label actual:  happy\n",
      "Predicted Label: happy\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_image(147,images_f,images_f_2,Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label actual:  surprise\n",
      "Predicted Label: surprise\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_image(369,images_f,images_f_2,Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label actual:  surprise\n",
      "Predicted Label: surprise\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD6CAYAAABnLjEDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO2de8yW5ZXurwVasLUUAUUUBCx4bLtBqdFaWmtLD45R2jQ71snEJjT+0b2TTmZ2pnaaTGaSNmn/mTrN3rUhu41MYsZ6ajXGZheth9paDiLjiOiAyElOgqJg0Qrc88f3fm7u675434fv8H4v3NcvIXzr4X6f534Oi+db17vWuiOlBGPMic+okZ6AMaY72NmNqQQ7uzGVYGc3phLs7MZUgp3dmEoYlLNHxBcj4sWIWB8RtwzVpIwxQ08M9Hv2iBgN4D8BLACwFcAKAF9LKT1/tM+ccsopady4cdm2JsePiI5jDh06lNl//vOfO45Rxz58+PAxz68pvK9Rozr/X9vk3NUcm3yOj6/2M9B985jRo0cXY3jbySefXIzh+8E2UN5XtoHyPJqeF18jdXyetzqPMWPGdDw+X48mzwezZ88e7Nu3T96gk455b/+fywCsTyltAICIuBPA9QCO6uzjxo3DDTfckG07ePBgxwPxhVI3c9++fZm9cePGYsxbb72V2e+8804xZv/+/Znd5OFSD4m6mfy5sWPHFmP4hp90UnmL+CFQ56Gci3n/+9/fdn6A/k+T56SOxXMcP358MYa3TZ48uRjD5/bmm28WY/jeqzFNXgbve9/7im387L399tvFmDPPPDOzp0yZUoyZOXNm2/kAwAc/+MHM/sAHPlCM6fTy+d73vnfUfxvMr/FnA9hyhL21tc0Y04MMu0AXETdHxMqIWHngwIHhPpwx5igMxtlfATDtCHtqa1tGSmlxSmleSmneKaecMojDGWMGw2Bi9hUAZkfETPQ5+Q0Abmz3gZRSEQNz7KRECd6mYrLdu3dn9rvvvluM4WOrWJc/10RTULHeqaee2nGcisl4m4rrTzvttMxWmsErr7zScQzH7Hv27CnGqP+gZ8yYkdkf+tCHijFvvPFGZqsYla+tErZYD1D3Q+kqzJ/+9KfMVtdVXSN+HpQ+wXH09u3bizFnnHFGx+Pz86iuPftCE7G0nwE7e0rpYET8TwD/D8BoAD9PKa0Z6P6MMcPLYN7sSCk9BOChIZqLMWYYcQadMZUwqDf7QOCYi+MdFRPt3bs3s7dt21aM4dhFxXa8TcV6vB8VN3Fsqb4fnjp1arGNY+2JEyd2HKP2zXE9fz8LlPGfipk5btyyZUsxhr8fPto25tVXX81sjuEBYN26dZmttBjepu4HX2ulxezcubPt/I4GPzMq1uY4Wn3rtGPHjsyeNm1aMYZ1hSbJOUy77+H9ZjemEuzsxlSCnd2YSrCzG1MJXRXoUkqF4MFJNSpJgJMUuKAF0Ikt6vhHokQrToZRQhsLZGefXZYEXHDBBcW2c889N7ObFIcoEY/HqGvG4mOTghp1PZoU1ChYgFL7vvDCCzObRSygFPZYaANKQY4LY4Dy+VBJV0q0Y0FMfY6LY5SIxmO44Aook5xYsFNj2H8s0Blj7OzG1IKd3ZhK6HrMzrEbx1Jbt24tPsdxkopRWQtQ8Q4nKagCDt42a9asYgzH3meddVYxhotFgLIYQhXLcDzeRIsYKgYanw9033w9miQHqWQUjuM3b95cjNm1a1dmK71EJeNwgZVKdOFtKq5nXn/99WIbx+PqOWft5bXXXsvsdkVBfrMbUwl2dmMqwc5uTCXY2Y2phK5XvXWqTlMJM6qjJ8MJIqrqjQWgCRMmFGNYbDv//POLMSwSnX766cUY7jgKlAKMEgi7KciNNHzPVOcernJTAhQLYk06EKmkFha7gDJBRh2fn9km3XZVAg+fqxJ5Oz3nTqoxxtjZjakFO7sxldDVmD0iiqQA7miqVuposgQTx2kqIYGTH1QBy+zZszOb42ygjKubdhThLjTqcyaHY1RVGMSxbpOkq0svvbQYo7rrcpckFY83KTpq8nxyAU+TJcz43NstGeU3uzGVYGc3phLs7MZUgp3dmEroelINCwxcVaTa8HJ1mOp6wgKIEiomTZqU2U3aNCuhjeejEmjUvgey3rbJUcIW37PzzjuvGMP3QyW1qGfvt7/9bWarajXupqMSb1jYU8lTnDzGlXpAKRhyopir3owxdnZjasHObkwldDVmP3z4cJEowDGQ6vLB8Y2K2zjxRsXHPEZ1s+H5qXici2VUB1rH570FFx2NGzeuGKM65XCsvWzZsmIML2OlOuAyqksRJ+NwfA6U/sGJWu2WGPcTaUwl2NmNqQQ7uzGVYGc3phK6KtAdPHiwSKJpsnwNC2mqowkLKarjDQseaj1wFklUFxpO0BhKMY7PX4mRZvCo66oSoa6++urMVqIuP9Oq4w0/Iyqphqsg1bFUpeaRtHte/GY3phLs7MZUQkdnj4ifR8SuiHjuiG0TImJpRKxr/X1au30YY0aeJjH77QD+N4B/PWLbLQAeSSn9ICJuadnf7rSjQ4cOFUk0HLuoIheO67k7h9qPWra3yfLQ3HGWlygCdPeaJvC5qeM7Gae3mDJlSmarbsNPPfVUZqvngzUlVbDChTiqS+6WLVsym5OFlP/00/HJSik9AYAVh+sBLGn9vATAwk77McaMLAN9jUxOKW1v/bwDQCljGmN6ikF/9ZZSShFx1I6QEXEzgJuBuhZAMKbXGOibfWdETAGA1t9llX2LlNLilNK8lNI8d1M1ZuQY6Jv9AQA3AfhB6+/7m3zo8OHDhQjBwoWqemsyhv8jGTt2bDGGhROVnMOVT0psYXFFtRfetGlTsY2rodRa41xl56SaHCVAcYcX1cpZdRxqAu9LLRl2zjnnZDZ3rgHKRBvVJrrJEmY8H67eU8/ie/s/6r+0iIh/A/AUgPMjYmtELEKfky+IiHUAPteyjTE9TMc3e0rpa0f5p88O8VyMMcOIv9Q1phK63l2201I5KmGmSbEMxzeq6wjH8Sr+4jFNYisVn//6178utvG5r1ixohjDSRIqqYe7p6pOOSpuHUlU/NmkgISvv+rw0gS+1+reD2Q/ADBz5szM3rBhQzGGk8lU0hdrUUrQHj9+fGbz9fDyT8YYO7sxtWBnN6YS7OzGVMKIqzicEKESXVh0UMIFd51RIh5/jsUwoBSSlODBrYNXrVpVjLn44ouLbfv378/sHTt2dNz3ypUrizFPPvlkZn/2s+W3oJdffnlmDzSppAmqguvll1/ObK4MA4Dnn38+s9W9Z8H2ggsuKMbwNpUIxcs9NUm6UnDrZqCZSMYVbCoxTInTDCfNsKjpVtLGGDu7MbVgZzemEuzsxlTCiAt03C5XCQwsLqlKMM60UtU/3BZaCSIsNqk1sjdv3pzZSiBTbYkZJSJu27Yts9esWVOM4Yw9zkQDylbawynQqeNzFhkLsUDZ4onX0FNjVCvldpVe/bAYq9Znb7JmnxIRuXWVGtPk+eR+D6r/Awt9x3Jf/WY3phLs7MZUgp3dmEroeszO8TbbKkFjIKjEG46jVYzIcZJaa/uTn/xkZqvKNKU9cKy9Z8+eYgzHZCqJg7UHVS3GMbOqFhuqnoDqnnEHldmzZxdjuDJx4sSJxRjWNdQ148+peJj1GXXPVOtm/pxKmDn33HMzW90zjq1VVWKTDkisIXFC0aA61RhjTgzs7MZUgp3dmEqwsxtTCV0V6CKiEIVYoGuSbKBEEv6cEugmTZqU2Uqg4zXcVZUViztKxLv33nuLbcuWLctslVTDrZKV2DN//vzM/shHPlKM4TZInKwDADNmzCi2DQS1jniTFmAvvvhiZt9zzz3FGL5GqlKRRbQFCxYUYy666KK28wMGLg7zualnj59ZJQayqKuq8Pha87Et0Blj7OzG1IKd3ZhK6HrM3qnFsVreh2Ogdt04+lEJIxz/cuIHUMZWqiU1r5F99913F2Mef/zxYttHP/rRzFaxLs9Rxewc63J7YaAszuBOPkOJKkziOanCE0784aWvgPLeq2O98MILmf2Tn/ykGLNo0aLMnjt3bjGmXRvmdjTRJ/h5VHoNf67Jc87dj9rpDn6zG1MJdnZjKsHObkwl2NmNqYSuCnSHDh3qKCgogY63qYQIFjOaCHSqUw0fS4lo69evz2zVJnnhwoXFtg9/+MOZ/eMf/7gYwyLNFVdc0XGOa9euLcZwhd9A1zZrAlfhAWX3Gm4bDZSJLkoMZTFSiarcKeiXv/xlMeauu+7KbF5THdBVd01gYU91vGnShYb3wwleQClQckKXalH93v6P+i/GmBMKO7sxlWBnN6YSuh6zc6cRTtxX8Q7HaWptay4sUPEXx5YqwYf1ANUVlbuF8HrpAHD99dcX2zj5Q+kBvGb75z73uWLMZZddltkqkYK7pyidY6hQXXBYm1Hddq+66qrMVskwf/jDHzJbFZnMmTMns2+44YZizG233ZbZaumtgcbsjNIDOEZvok0pLeSNN97IbH7uVbJOP36zG1MJdnZjKsHObkwldHT2iJgWEY9GxPMRsSYivtXaPiEilkbEutbfZcWGMaZnaCLQHQTwtymlVRHxQQBPR8RSAF8H8EhK6QcRcQuAWwB8u92OUkqFgMDL+ShBij+zevXqYgwnF3BXGqBMLFFJHJxoo4QcriBTCSu8PjlQCi5ciQWU1WLz5s0rxnDyxcc//vFiTJMlkYYTPtcrr7yyGMMCqUog4uuvBFzuXsPiIADMmjUrs9sJWYNl2rRpxTZO6FLVe5/5zGcyW3WqYTH2iSeeyOx2lXsd3+wppe0ppVWtn/cBWAvgbADXA1jSGrYEQJkyZozpGY7pq7eImAFgLoBlACanlLa3/mkHALmSYUTcDOBmYOD1wsaYwdPY+yLiVAD3AvjrlFL2e2zq+51I/l6UUlqcUpqXUppnZzdm5Gj0Zo+Ik9Hn6HeklO5rbd4ZEVNSStsjYgqAcm1jIqVUJOpzR5V169YVn+PEAbUkLifeqCQOHqOKEXgbzw8Arr322sxWcT0v0QuUSwBdc801HY/fJPYe6fhcwXNSiTccNyvtgTvKqKWu+Pqra8+agVr6eahQGs706dMzWz3DXMz18MMPF2P4eWRbJR3100SNDwA/A7A2pfTPR/zTAwBuav18E4D7O+3LGDNyNHmzXwngrwD8R0T0y+B/D+AHAO6KiEUANgH478MzRWPMUNDR2VNKTwIovyfo47NH2W6M6TGsmBlTCV2tehs1alQhQLFwoxR7rvRRwhpXGqkKpoF8G6C62XBih2qBrKqaWKCrHU4sUYkunHij1lVX20YSJZJxoo3qKMPJQKrikhO6+FkcVFKNMebEwM5uTCXY2Y2phK7G7EAZU3DMrrq3cEyslrvl2E4lLag4eihQHW86LXNlSlRxyPGIips5iYcLWIByeTBV9MNaFCePtXvu/GY3phLs7MZUgp3dmEqwsxtTCV1VkVTVGyeaqCoz7tjBXWnUNiVUtFsa50RHJaycKIJYr6GuKwtr3A4cKO8RC3ZA6R/bt2/P7Hb31G92YyrBzm5MJdjZjakEO7sxldB1gY5b73DLYSVKcPWPain0zDPPZLZa/4wFuhNZtOLWTO3aFZnhh9uWqzbR3DpLZXxyu6+lS5dmtgU6Y4yd3ZhasLMbUwldjdkPHz5cVLXt3r07s88666zic7ym+1NPPVWM4RbUl156aTGGE21YPwCOz9hWxXZ8bqq7j+ke3PGIY22gjMe/8IUvHPN+3anGGGNnN6YW7OzGVIKd3ZhK6KpAFxGFALZ27drMbpJI8MorrxRjOEmB13VTqGOxiHc8JNmoOfI2FkKBshKr6blyMtLxcI1GGl5DXrU6f/nllzP7rbfeKsZwuzVOnmq37rzf7MZUgp3dmEqwsxtTCV2P2Tn+3rRpU2bzEjhA2S734osvLsZw4YuKI7kFtUqq4aQE1fFmIMtIDSdKe/j973+f2Wqt7wULFmT2xz72sWKMukYPPvhgZqv10L/+9a9ntmrtXRMcs3/5y18uxixfvjyzDxw4UIzh68jr1av71U9vPbXGmGHDzm5MJdjZjakEO7sxldD1BclYONuxY0dmr169uvjMpz/96cyeP39+MWbz5s2ZrcQNFpLUmnG8jYUVoPcEOtVae9u2bZl93333FWPWr1+f2d/4xjeKMWrtvVtvvTWzJ02aVIz50pe+lNkzZ84sxtSUjMPC2TnnnFOM4UQw9exxtyWu9lTPdD+99dQaY4YNO7sxldDR2SNibEQsj4h/j4g1EfFPre0zI2JZRKyPiF9EhLsjGNPDNInZ3wFwdUppf0ScDODJiPg1gL8B8KOU0p0R8VMAiwDc1m5HKqmGk/3VutXTp0/PbNU5ttOyOECZgKASPbijS5PEm16L4YEy3vv+979fjOGOpypGVAlMP/3pTzN73759xZhXX301s1WMWvMa9qeeemqxjZ81pcXwc/3HP/4xs1VSWj8dn9LUR/8eTm79SQCuBnBPa/sSAAs77csYM3I0eiVFxOiIWA1gF4ClAF4CsDel1P9f0VYAZTN3Y0zP0MjZU0qHUkpzAEwFcBmAC5oeICJujoiVEbFS/fptjOkOxxRsppT2AngUwBUAxkdEf9A1FUDZUaLvM4tTSvNSSvN6MbY1phY6KiQRcTqAd1NKeyPiFAALAPwQfU7/VQB3ArgJwP1NDsgCHSdkPPvss8Vndu3aldlqiSjuXqN+i9i6dWtmq242PB9V0cX7Hjt2bDGmm/+xqaQWXkpIiW98L5p07gGAadOmZbYS6N58882O+zlRUc9ek2vNS5hddNFFxZjf/e53mc2+0a7qrckdmAJgSUSMRt9vAnellB6MiOcB3BkR3wPwDICfNdiXMWaE6OjsKaVnAcwV2zegL343xhwHOIg2phKiXTfKoWbMmDFp6tSp2bYzzjgjs7ds2VJ8jmPiRYsWFWMuueSSzFZFFi+99FJmn3/++cWYGTNmZDYnnihUgsSYMWM6fs6C5YmJisfZz5SGsWrVqsy+4447ijGPP/542/0899xz2L9/v6ww8tNmTCXY2Y2pBDu7MZVgZzemErqa6TB69OgikYUTMngtdqBMbPnVr35VjPn85z+f2SqxgfejKuNY8GABESjXcFeJNyqJhYXGJss2DRVqjpyAwWt9D+d8TmSaLGHWBK5oO9q+j8TLPxlj7OzG1IKd3ZhK6GrMnlLC22+/nW3j7qUqtuGOMrzMMwB897vfzexPfOITxRiOtVXRAM/n9ddfL8ZwEg13/AR0wgyP42WmgWbJOJ32C5QFRWqZa96m4kEVx5933nmZzZ2EAODss/P2BgOJWYcTFduq8+f7qD7H58ZdeoDymeWiLAC4/fbbO+6Hnxl+XtuVkfvNbkwl2NmNqQQ7uzGVYGc3phK6XvV21llnZdtYUFCCGCe6KEFKLffEXHvttW1tADj99NMzW3WBYdFKiU/ckhoohT3VKYdFxCaJNw888EAxZsWKFZm9e/fuYgwfXx2LW30DpUjFYhwAfOpTn8pstWSXukZDQZOkFvUMKcGWnz3lL3wev/nNb4ox3/zmNzNbCaZnnnlmZqvnigVc9p+NGzfiwIEDrnozpmbs7MZUgp3dmEroelINx0VcoKHiFI5jVeIAF5mopWtffPHFzL7qqquKMRMnTmw7P3V8FXuquJHnqJZDbtcdtJ+nn346s1XMzssAqW46nHiklg7iaw8ADz30UGYvW7asGMMdh1Tsf9111xXbBkIT3YnHqJhdwc9jk8Kgxx57rNjGmlKTjsTq2nNSGs/HhTDGGDu7MbVgZzemEuzsxlRC19fkYeGKhZIm7ZWVINZEqNi2bVtmb9y4sRgzefLkzFYiGic2KBFPCWJ87kpo5OO98cYbxRjuYMJLLQHA3r17M1t15Vm8eHHH/ahz4/NQQhLfj4cffrgYc8UVV2Q2JzQNlCaJSE2r8FgwVS2gOWFJLWHGz6y6rryNxWIA2LFjR2bzeVigM8bY2Y2pBTu7MZVgZzemErqeQceiFAseqi0Ti3gq+6hJBRVnH61Zs6YYc+GFF2a2EnJ4mzq2Et9YPFFiJO9LZQLyNlXxx3PkVlIAMH78+MxWWW5KIOR7pMaw2Key83jtvYEKdCy+NclyU/dV3TN+ZlQrMRbklPjGLcmVkNZkTfuBZPT14ze7MZVgZzemEuzsxlRC15NqOC5qkhTAcYmqWOJ2000qodavX19s4w4iqpUyo5Z6Up/rdO7qcyqOnDBhQmarc+V15VU8zEkbKv5T8fhpp52W2SphhyvzVMzO24Zq2SQFPzMqrlZwzK6qEleuXNlxDHc8ahKzKw2Fn48mHZr68ZvdmEqwsxtTCY2dPSJGR8QzEfFgy54ZEcsiYn1E/CIihqd7oDFmSDiWN/u3ABy5hs0PAfwopTQLwOsAFg3lxIwxQ0sjgS4ipgL4CwDfB/A30afkXA3gxtaQJQD+EcBt7faTUipEGE4sUSJRk9bNTRIr+HOqbfULL7yQ2dzeV6GqvlRCRJM14vhcVbtpFt/UufIYVYWn5s00SfxRAiVvU6IVjxkqMU4JfbxNXTMlhvK8lRi5atWqzGZRTx1PCbgs4r322mvFGE6EOpZW8E3f7LcC+DsA/VdjIoC9KaX+K7EVQNk83BjTM3R09oi4FsCulNLTncYe5fM3R8TKiFjZzQUpjDE5TX6NvxLAdRFxDYCxAMYB+BcA4yPipNbbfSqAcokLACmlxQAWA8BJJ51kbzdmhOjo7Cml7wD4DgBExFUA/ldK6S8j4m4AXwVwJ4CbANzfaV8RUcSAHN+oIheOPzmBBmhWeMExoSoy2bBhQ2bPnTu3GKPicUbFujxvPi+gjO04gQUoO7ywzgCUBRu87BZQJmTs27evGKPgOaqYnc+Nl4MCgEsuuaTR8TrB2odKNOH7oeJ61ZWIE1tUzM7rqKtkGJ6jKqjh50PpHE0Ss47GYL5n/zb6xLr16IvhfzaIfRljhpljSpdNKT0G4LHWzxsAXDb0UzLGDAfOoDOmEuzsxlRCV6veIqJI5ODqIyWacUKIEpJYFDmWDh7t9qPaK/N8lEiiBEIWH5X41qQKcMqUKZl94403FmNWr16d2TNmzCjG8Lnt3LmzGKPWp581a1Zmq+QkFpIWLlxYjBlIEo1KfOFnSCUrNVnrTd1rnqMS6PheK3GWnys1hs+tSbISC9zqHr53zKP+izHmhMLObkwl2NmNqYSuxuwHDx4slsrhggAVS3FMpuJxFesz/DkVNzUpqOH5qI64aj4c26p4mJNR1Bw3bdqU2RzDq+Or4oypU6dmtiqMUR1upk+fntlcCAKUa783WdarCSrRhK+rOg8+fpNlrYAy+UXFxKwhNVl+as+ePcUYfo7UNeP5sH7ULuHLb3ZjKsHObkwl2NmNqQQ7uzGVMOJJNSwoqESTXbt2ZbYSUljMGOjyS0264vDnlGikloTiaizVplmtyd3p+LxeOwDMnz8/s5VoxYke3KIa0B1VuMprzpw5xRg+14F2oeF71qQLTBOUgKrmyN1iVPcYFlW3bt1ajFHXluFrppJqWMDmSj31LPbjN7sxlWBnN6YS7OzGVMKIx+wcl6guqFx8oBIiuMikSRKHikebxOx8/CZdWtW+9u7dW4zhuE0l7HBHExV/Ll26NLMvvvjiYszMmTMzm5cVBrSuwHHh2rVrizF8P9SS0ayrqHvGMbu697wfdc1Yn1Cxv7rX69aty2xeHkzNUXW84TmpzkE8R1UExfvhe99Ov/Cb3ZhKsLMbUwl2dmMqwc5uTCV0VaAbNWpUx6odJVqxmNJ06R6GxQ0lyLD4pFr+skikBBnVEpvPVbU85vNXohnPUYk9y5cvz2yV6MHzUYlATUQzxVe+8pXMVsJaE2GTP6c6APF5qMQSfoZUdx3VAvrJJ5/M7CadctT1YSFNJefwfVT74WemybH78ZvdmEqwsxtTCXZ2Yyqh6zE7Fw1wvKtiMo5DVKzNxTEqHuRYXy2/xB1GVeccjseVzqCSg3ibOlfuhMJL9AJlHK3mOG3atMzesmVLMYYTRNSyWuo6crGSWsaJ42YVR/N1VAVOfG2bLI+tusBs3Lgxs1VyzLZt24ptrHWoZ4+fI9Uthrepe8ZzUkVR/DzwdXVSjTHGzm5MLdjZjakEO7sxlRBNEiSG7GARrwLYBGASgN0dhvcax+OcgeNz3p7zwJmeUir7f6PLzv7eQSNWppTmdf3Ag+B4nDNwfM7bcx4e/Gu8MZVgZzemEkbK2ReP0HEHw/E4Z+D4nLfnPAyMSMxujOk+/jXemErourNHxBcj4sWIWB8Rt3T7+E2IiJ9HxK6IeO6IbRMiYmlErGv9Xa5mMYJExLSIeDQino+INRHxrdb2np13RIyNiOUR8e+tOf9Ta/vMiFjWekZ+ERFlof0IExGjI+KZiHiwZff8nLvq7BExGsD/AfAlABcB+FpEXNTNOTTkdgBfpG23AHgkpTQbwCMtu5c4COBvU0oXAbgcwP9oXdtenvc7AK5OKf03AHMAfDEiLgfwQwA/SinNAvA6gEUjOMej8S0AR7bV7fk5d/vNfhmA9SmlDSmlPwO4E8D1XZ5DR1JKTwDgViLXA1jS+nkJgIVdnVQHUkrbU0qrWj/vQ9+DeDZ6eN6pj/7Sv5NbfxKAqwHc09reU3MGgIiYCuAvAPzflh3o8TkD3Xf2swEcWWu5tbXteGBySqm/gf0OAJNHcjLtiIgZAOYCWIYen3fr1+HVAHYBWArgJQB7U0r9tZu9+IzcCuDvAPT3qJqI3p+zBbqBkPq+wujJrzEi4lQA9wL465RSVpzfi/NOKR1KKc0BMBV9v/ldMMJTaktEXAtgV0rp6ZGey7HS1eYVAF4BcGRXhamtbccDOyNiSkppe0RMQd+bqKeIiJPR5+h3pJTua23u+XkDQEppb0Q8CuAKAOMj4qTWm7LXnpErAVwXEdcAGAtgHIB/QW/PGUD33+wrAMxuKZfvA3ADgAe6PIeB8gCAm1o/3wTg/hGcS0ErbvwZgLUppX8+4p96dt4RcXpEjG/9fAqABejTGh4F8NXWsJ6ac0rpOymlqSmlGeh7fn+bUvpL9PCc3yOl1NU/AJbmE08AAACLSURBVK4B8J/oi82+2+3jN5zjvwHYDuBd9MVfi9AXlz0CYB2AhwFMGOl50pw/ib5f0Z8FsLr155penjeAjwF4pjXn5wD8Q2v7uQCWA1gP4G4AY0Z6rkeZ/1UAHjxe5uwMOmMqwQKdMZVgZzemEuzsxlSCnd2YSrCzG1MJdnZjKsHObkwl2NmNqYT/ApMBBJNIifteAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_image(502,images_f,images_f_2,Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label actual:  disgust\n",
      "Predicted Label: disgust\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_image(800,images_f,images_f_2,Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264bit83e6fea1457f416d9db64f23f9f966e4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
